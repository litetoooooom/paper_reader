{"title": "Efficient Multi-domain Text Recognition Deep Neural Network Parameterization with Residual Adapters", "author": "Jiayou Chao, Wei Zhu", "abstract": "Recent advancements in deep neural networks have markedly enhanced the\nperformance of computer vision tasks, yet the specialized nature of these\nnetworks often necessitates extensive data and high computational power.\nAddressing these requirements, this study presents a novel neural network model\nadept at optical character recognition (OCR) across diverse domains, leveraging\nthe strengths of multi-task learning to improve efficiency and generalization.\nThe model is designed to achieve rapid adaptation to new domains, maintain a\ncompact size conducive to reduced computational resource demand, ensure high\naccuracy, retain knowledge from previous learning experiences, and allow for\ndomain-specific performance improvements without the need to retrain entirely.\nRigorous evaluation on open datasets has validated the model's ability to\nsignificantly lower the number of trainable parameters without sacrificing\nperformance, indicating its potential as a scalable and adaptable solution in\nthe field of computer vision, particularly for applications in optical text\nrecognition.", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00971v1"}
{"title": "Directional Antenna Systems for Long-Range Through-Wall Human Activity Recognition", "author": "Julian Strohmayer, Martin Kampel", "abstract": "WiFi Channel State Information (CSI)-based human activity recognition (HAR)\nenables contactless, long-range sensing in spatially constrained environments\nwhile preserving visual privacy. However, despite the presence of numerous\nWiFi-enabled devices around us, few expose CSI to users, resulting in a lack of\nsensing hardware options. Variants of the Espressif ESP32 have emerged as\npotential low-cost and easy-to-deploy solutions for WiFi CSI-based HAR. In this\nwork, four ESP32-S3-based 2.4GHz directional antenna systems are evaluated for\ntheir ability to facilitate long-range through-wall HAR. Two promising systems\nare proposed, one of which combines the ESP32-S3 with a directional biquad\nantenna. This combination represents, to the best of our knowledge, the first\ndemonstration of such a system in WiFi-based HAR. The second system relies on\nthe built-in printed inverted-F antenna (PIFA) of the ESP32-S3 and achieves\ndirectionality through a plane reflector. In a comprehensive evaluation of\nline-of-sight (LOS) and non-line-of-sight (NLOS) HAR performance, both systems\nare deployed in an office environment spanning a distance of 18 meters across\nfive rooms. In this experimental setup, the Wallhack1.8k dataset, comprising\n1806 CSI amplitude spectrograms of human activities, is collected and made\npublicly available. Based on Wallhack1.8k, we train activity recognition models\nusing the EfficientNetV2 architecture to assess system performance in LOS and\nNLOS scenarios. For the core NLOS activity recognition problem, the biquad\nantenna and PIFA-based systems achieve accuracies of 92.0$\\pm$3.5 and\n86.8$\\pm$4.7, respectively, demonstrating the feasibility of long-range\nthrough-wall HAR with the proposed systems.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI", "cs.LG"], "links": "http://arxiv.org/abs/2401.01388v1"}
{"title": "Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human Activity Recognition", "author": "Julian Strohmayer, Martin Kampel", "abstract": "The recognition of human activities based on WiFi Channel State Information\n(CSI) enables contactless and visual privacy-preserving sensing in indoor\nenvironments. However, poor model generalization, due to varying environmental\nconditions and sensing hardware, is a well-known problem in this space. To\naddress this issue, in this work, data augmentation techniques commonly used in\nimage-based learning are applied to WiFi CSI to investigate their effects on\nmodel generalization performance in cross-scenario and cross-system settings.\nIn particular, we focus on the generalization between line-of-sight (LOS) and\nnon-line-of-sight (NLOS) through-wall scenarios, as well as on the\ngeneralization between different antenna systems, which remains under-explored.\nWe collect and make publicly available a dataset of CSI amplitude spectrograms\nof human activities. Utilizing this data, an ablation study is conducted in\nwhich activity recognition models based on the EfficientNetV2 architecture are\ntrained, allowing us to assess the effects of each augmentation on model\ngeneralization performance. The gathered results show that specific\ncombinations of simple data augmentation techniques applied to CSI amplitude\ndata can significantly improve cross-scenario and cross-system generalization.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI", "cs.LG"], "links": "http://arxiv.org/abs/2401.00964v1"}
{"title": "DiffAugment: Diffusion based Long-Tailed Visual Relationship Recognition", "author": "Parul Gupta, Tuan Nguyen, Abhinav Dhall, Munawar Hayat, Trung Le, Thanh-Toan Do", "abstract": "The task of Visual Relationship Recognition (VRR) aims to identify\nrelationships between two interacting objects in an image and is particularly\nchallenging due to the widely-spread and highly imbalanced distribution of\n<subject, relation, object> triplets. To overcome the resultant performance\nbias in existing VRR approaches, we introduce DiffAugment -- a method which\nfirst augments the tail classes in the linguistic space by making use of\nWordNet and then utilizes the generative prowess of Diffusion Models to expand\nthe visual space for minority classes. We propose a novel hardness-aware\ncomponent in diffusion which is based upon the hardness of each <S,R,O> triplet\nand demonstrate the effectiveness of hardness-aware diffusion in generating\nvisual embeddings for the tail classes. We also propose a novel subject and\nobject based seeding strategy for diffusion sampling which improves the\ndiscriminative capability of the generated visual embeddings. Extensive\nexperimentation on the GQA-LT dataset shows favorable gains in the\nsubject/object and relation average per-class accuracy using Diffusion\naugmented samples.", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01387v1"}
{"title": "Tissue Artifact Segmentation and Severity Analysis for Automated Diagnosis Using Whole Slide Images", "author": "Galib Muhammad Shahriar Himel", "abstract": "Traditionally, pathological analysis and diagnosis are performed by manually\neyeballing glass slide specimens under a microscope by an expert. The whole\nslide image is the digital specimen produced from the glass slide. Whole slide\nimage enabled specimens to be observed on a computer screen and led to\ncomputational pathology where computer vision and artificial intelligence are\nutilized for automated analysis and diagnosis. With the current computational\nadvancement, the entire whole slide image can be analyzed autonomously without\nhuman supervision. However, the analysis could fail or lead to wrong diagnosis\nif the whole slide image is affected by tissue artifacts such as tissue fold or\nair bubbles depending on the severity. Existing artifact detection methods rely\non experts for severity assessment to eliminate artifact affected regions from\nthe analysis. This process is time consuming, exhausting and undermines the\ngoal of automated analysis or removal of artifacts without evaluating their\nseverity, which could result in the loss of diagnostically important data.\nTherefore, it is necessary to detect artifacts and then assess their severity\nautomatically. In this paper, we propose a system that incorporates severity\nevaluation with artifact detection utilizing convolutional neural networks. The\nproposed system uses DoubleUNet to segment artifacts and an ensemble network of\nsix fine tuned convolutional neural network models to determine severity. This\nmethod outperformed current state of the art in accuracy by 9 percent for\nartifact segmentation and achieved a strong correlation of 97 percent with the\nevaluation of pathologists for severity assessment. The robustness of the\nsystem was demonstrated using our proposed heterogeneous dataset and practical\nusability was ensured by integrating it with an automated analysis system.", "published": "2024-01-01", "categories": ["eess.IV", "cs.CV", "cs.LG"], "links": "http://arxiv.org/abs/2401.01386v1"}
{"title": "Boundary Attention: Learning to Find Faint Boundaries at Any Resolution", "author": "Mia Gaia Polansky, Charles Herrmann, Junhwa Hur, Deqing Sun, Dor Verbin, Todd Zickler", "abstract": "We present a differentiable model that explicitly models boundaries --\nincluding contours, corners and junctions -- using a new mechanism that we call\nboundary attention. We show that our model provides accurate results even when\nthe boundary signal is very weak or is swamped by noise. Compared to previous\nclassical methods for finding faint boundaries, our model has the advantages of\nbeing differentiable; being scalable to larger images; and automatically\nadapting to an appropriate level of geometric detail in each part of an image.\nCompared to previous deep methods for finding boundaries via end-to-end\ntraining, it has the advantages of providing sub-pixel precision, being more\nresilient to noise, and being able to process any image at its native\nresolution and aspect ratio.", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00935v1"}
{"title": "Refining Pre-Trained Motion Models", "author": "Xinglong Sun, Adam W. Harley, Leonidas J. Guibas", "abstract": "Given the difficulty of manually annotating motion in video, the current best\nmotion estimation methods are trained with synthetic data, and therefore\nstruggle somewhat due to a train/test gap. Self-supervised methods hold the\npromise of training directly on real video, but typically perform worse. These\ninclude methods trained with warp error (i.e., color constancy) combined with\nsmoothness terms, and methods that encourage cycle-consistency in the estimates\n(i.e., tracking backwards should yield the opposite trajectory as tracking\nforwards). In this work, we take on the challenge of improving state-of-the-art\nsupervised models with self-supervised training. We find that when the\ninitialization is supervised weights, most existing self-supervision techniques\nactually make performance worse instead of better, which suggests that the\nbenefit of seeing the new data is overshadowed by the noise in the training\nsignal. Focusing on obtaining a ``clean'' training signal from real-world\nunlabelled video, we propose to separate label-making and training into two\ndistinct stages. In the first stage, we use the pre-trained model to estimate\nmotion in a video, and then select the subset of motion estimates which we can\nverify with cycle-consistency. This produces a sparse but accurate\npseudo-labelling of the video. In the second stage, we fine-tune the model to\nreproduce these outputs, while also applying augmentations on the input. We\ncomplement this boot-strapping method with simple techniques that densify and\nre-balance the pseudo-labels, ensuring that we do not merely train on ``easy''\ntracks. We show that our method yields reliable gains over fully-supervised\nmethods in real videos, for both short-term (flow-based) and long-range\n(multi-frame) pixel tracking.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.00850v1"}
{"title": "COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training", "author": "Alex Jinpeng Wang, Linjie Li, Kevin Qinghong Lin, Jianfeng Wang, Kevin Lin, Zhengyuan Yang, Lijuan Wang, Mike Zheng Shou", "abstract": "In the evolution of Vision-Language Pre-training, shifting from short-text\ncomprehension to encompassing extended textual contexts is pivotal. Recent\nautoregressive vision-language models like \\cite{flamingo, palme}, leveraging\nthe long-context capability of Large Language Models, have excelled in few-shot\ntext generation tasks but face challenges in alignment tasks. Addressing this\ngap, we introduce the contrastive loss into text generation models, presenting\nthe COntrastive-Streamlined MultimOdal framework (\\ModelName), strategically\npartitioning the language model into dedicated unimodal text processing and\nadept multimodal data handling components. \\ModelName, our unified framework,\nmerges unimodal and multimodal elements, enhancing model performance for tasks\ninvolving textual and visual data while notably reducing learnable parameters.\nHowever, these models demand extensive long-text datasets, yet the availability\nof high-quality long-text video datasets remains limited. To bridge this gap,\nthis work introduces \\VideoDatasetName, an inaugural interleaved video-text\ndataset featuring comprehensive captions, marking a significant step forward.\nDemonstrating its impact, we illustrate how \\VideoDatasetName{} enhances model\nperformance in image-text tasks. With 34% learnable parameters and utilizing\n72\\% of the available data, our model demonstrates significant superiority over\nOpenFlamingo~\\cite{openflamingo}. For instance, in the 4-shot flickr captioning\ntask, performance notably improves from 57.2% to 65.\\%. The contributions of\n\\ModelName{} and \\VideoDatasetName{} are underscored by notable performance\ngains across 14 diverse downstream datasets encompassing both image-text and\nvideo-text tasks.", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00849v1"}
{"title": "Mocap Everyone Everywhere: Lightweight Motion Capture With Smartwatches and a Head-Mounted Camera", "author": "Jiye Lee, Hanbyul Joo", "abstract": "We present a lightweight and affordable motion capture method based on two\nsmartwatches and a head-mounted camera. In contrast to the existing approaches\nthat use six or more expert-level IMU devices, our approach is much more\ncost-effective and convenient. Our method can make wearable motion capture\naccessible to everyone everywhere, enabling 3D full-body motion capture in\ndiverse environments. As a key idea to overcome the extreme sparsity and\nambiguities of sensor inputs, we integrate 6D head poses obtained from the\nhead-mounted cameras for motion estimation. To enable capture in expansive\nindoor and outdoor scenes, we propose an algorithm to track and update floor\nlevel changes to define head poses, coupled with a multi-stage\nTransformer-based regression module. We also introduce novel strategies\nleveraging visual cues of egocentric images to further enhance the motion\ncapture quality while reducing ambiguities. We demonstrate the performance of\nour method on various challenging scenarios, including complex outdoor\nenvironments and everyday motions including object interactions and social\ninteractions among multiple individuals.", "published": "2024-01-01", "categories": ["cs.CV", "cs.GR"], "links": "http://arxiv.org/abs/2401.00847v1"}
{"title": "Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP", "author": "Ruinan Jin, Chun-Yin Huang, Chenyu You, Xiaoxiao Li", "abstract": "In recent years, foundation models (FMs) have solidified their role as\ncornerstone advancements in the deep learning domain. By extracting intricate\npatterns from vast datasets, these models consistently achieve state-of-the-art\nresults across a spectrum of downstream tasks, all without necessitating\nextensive computational resources. Notably, MedCLIP, a vision-language\ncontrastive learning-based medical FM, has been designed using unpaired\nimage-text training. While the medical domain has often adopted unpaired\ntraining to amplify data, the exploration of potential security concerns linked\nto this approach hasn't kept pace with its practical usage. Notably, the\naugmentation capabilities inherent in unpaired training also indicate that\nminor label discrepancies can result in significant model deviations. In this\nstudy, we frame this label discrepancy as a backdoor attack problem. We further\nanalyze its impact on medical FMs throughout the FM supply chain. Our\nevaluation primarily revolves around MedCLIP, emblematic of medical FM\nemploying the unpaired strategy. We begin with an exploration of\nvulnerabilities in MedCLIP stemming from unpaired image-text matching, termed\nBadMatch. BadMatch is achieved using a modest set of wrongly labeled data.\nSubsequently, we disrupt MedCLIP's contrastive learning through\nBadDist-assisted BadMatch by introducing a Bad-Distance between the embeddings\nof clean and poisoned data. Additionally, combined with BadMatch and BadDist,\nthe attacking pipeline consistently fends off backdoor assaults across diverse\nmodel designs, datasets, and triggers. Also, our findings reveal that current\ndefense strategies are insufficient in detecting these latent threats in\nmedical FMs' supply chains.", "published": "2024-01-01", "categories": ["cs.CV", "cs.LG"], "links": "http://arxiv.org/abs/2401.01911v1"}
{"title": "Deblurring 3D Gaussian Splatting", "author": "Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park", "abstract": "Recent studies in Radiance Fields have paved the robust way for novel view\nsynthesis with their photorealistic rendering quality. Nevertheless, they\nusually employ neural networks and volumetric rendering, which are costly to\ntrain and impede their broad use in various real-time applications due to the\nlengthy rendering time. Lately 3D Gaussians splatting-based approach has been\nproposed to model the 3D scene, and it achieves remarkable visual quality while\nrendering the images in real-time. However, it suffers from severe degradation\nin the rendering quality if the training images are blurry. Blurriness commonly\noccurs due to the lens defocusing, object motion, and camera shake, and it\ninevitably intervenes in clean image acquisition. Several previous studies have\nattempted to render clean and sharp images from blurry input images using\nneural fields. The majority of those works, however, are designed only for\nvolumetric rendering-based neural radiance fields and are not straightforwardly\napplicable to rasterization-based 3D Gaussian splatting methods. Thus, we\npropose a novel real-time deblurring framework, deblurring 3D Gaussian\nSplatting, using a small Multi-Layer Perceptron (MLP) that manipulates the\ncovariance of each 3D Gaussian to model the scene blurriness. While deblurring\n3D Gaussian Splatting can still enjoy real-time rendering, it can reconstruct\nfine and sharp details from blurry images. A variety of experiments have been\nconducted on the benchmark, and the results have revealed the effectiveness of\nour approach for deblurring. Qualitative results are available at\nhttps://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00834v1"}
{"title": "Rethinking RAFT for Efficient Optical Flow", "author": "Navid Eslami, Farnoosh Arefi, Amir M. Mansourian, Shohreh Kasaei", "abstract": "Despite significant progress in deep learning-based optical flow methods,\naccurately estimating large displacements and repetitive patterns remains a\nchallenge. The limitations of local features and similarity search patterns\nused in these algorithms contribute to this issue. Additionally, some existing\nmethods suffer from slow runtime and excessive graphic memory consumption. To\naddress these problems, this paper proposes a novel approach based on the RAFT\nframework. The proposed Attention-based Feature Localization (AFL) approach\nincorporates the attention mechanism to handle global feature extraction and\naddress repetitive patterns. It introduces an operator for matching pixels with\ncorresponding counterparts in the second frame and assigning accurate flow\nvalues. Furthermore, an Amorphous Lookup Operator (ALO) is proposed to enhance\nconvergence speed and improve RAFTs ability to handle large displacements by\nreducing data redundancy in its search operator and expanding the search space\nfor similarity extraction. The proposed method, Efficient RAFT\n(Ef-RAFT),achieves significant improvements of 10% on the Sintel dataset and 5%\non the KITTI dataset over RAFT. Remarkably, these enhancements are attained\nwith a modest 33% reduction in speed and a mere 13% increase in memory usage.\nThe code is available at: https://github.com/n3slami/Ef-RAFT", "published": "2024-01-01", "categories": ["cs.CV", "ACM-class: F.2.2, I.2.7"], "links": "http://arxiv.org/abs/2401.00833v1"}
{"title": "GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable Simulation, Demonstration, and Imitation", "author": "Zifan Wang, Junyu Chen, Ziqing Chen, Pengwei Xie, Rui Chen, Li Yi", "abstract": "This paper presents GenH2R, a framework for learning generalizable\nvision-based human-to-robot (H2R) handover skills. The goal is to equip robots\nwith the ability to reliably receive objects with unseen geometry handed over\nby humans in various complex trajectories. We acquire such generalizability by\nlearning H2R handover at scale with a comprehensive solution including\nprocedural simulation assets creation, automated demonstration generation, and\neffective imitation learning. We leverage large-scale 3D model repositories,\ndexterous grasp generation methods, and curve-based 3D animation to create an\nH2R handover simulation environment named \\simabbns, surpassing the number of\nscenes in existing simulators by three orders of magnitude. We further\nintroduce a distillation-friendly demonstration generation method that\nautomatically generates a million high-quality demonstrations suitable for\nlearning. Finally, we present a 4D imitation learning method augmented by a\nfuture forecasting objective to distill demonstrations into a visuo-motor\nhandover policy. Experimental evaluations in both simulators and the real world\ndemonstrate significant improvements (at least +10\\% success rate) over\nbaselines in all cases. The project page is https://GenH2R.github.io/.", "published": "2024-01-01", "categories": ["cs.RO", "cs.CV"], "links": "http://arxiv.org/abs/2401.00929v1"}
{"title": "Sharp-NeRF: Grid-based Fast Deblurring Neural Radiance Fields Using Sharpness Prior", "author": "Byeonghyeon Lee, Howoong Lee, Usman Ali, Eunbyung Park", "abstract": "Neural Radiance Fields (NeRF) have shown remarkable performance in neural\nrendering-based novel view synthesis. However, NeRF suffers from severe visual\nquality degradation when the input images have been captured under imperfect\nconditions, such as poor illumination, defocus blurring, and lens aberrations.\nEspecially, defocus blur is quite common in the images when they are normally\ncaptured using cameras. Although few recent studies have proposed to render\nsharp images of considerably high-quality, yet they still face many key\nchallenges. In particular, those methods have employed a Multi-Layer Perceptron\n(MLP) based NeRF, which requires tremendous computational time. To overcome\nthese shortcomings, this paper proposes a novel technique Sharp-NeRF -- a\ngrid-based NeRF that renders clean and sharp images from the input blurry\nimages within half an hour of training. To do so, we used several grid-based\nkernels to accurately model the sharpness/blurriness of the scene. The\nsharpness level of the pixels is computed to learn the spatially varying blur\nkernels. We have conducted experiments on the benchmarks consisting of blurry\nimages and have evaluated full-reference and non-reference metrics. The\nqualitative and quantitative results have revealed that our approach renders\nthe sharp novel views with vivid colors and fine details, and it has\nconsiderably faster training time than the previous works. Our project page is\navailable at https://benhenryl.github.io/SharpNeRF/", "published": "2024-01-01", "categories": ["cs.CV", "cs.GR", "eess.IV"], "links": "http://arxiv.org/abs/2401.00825v1"}
{"title": "GLIMPSE: Generalized Local Imaging with MLPs", "author": "AmirEhsan Khorashadizadeh, Valentin Debarnot, Tianlin Liu, Ivan DokmaniÄ‡", "abstract": "Deep learning is the current de facto state of the art in tomographic\nimaging. A common approach is to feed the result of a simple inversion, for\nexample the backprojection, to a convolutional neural network (CNN) which then\ncomputes the reconstruction. Despite strong results on 'in-distribution' test\ndata similar to the training data, backprojection from sparse-view data\ndelocalizes singularities, so these approaches require a large receptive field\nto perform well. As a consequence, they overfit to certain global structures\nwhich leads to poor generalization on out-of-distribution (OOD) samples.\nMoreover, their memory complexity and training time scale unfavorably with\nimage resolution, making them impractical for application at realistic clinical\nresolutions, especially in 3D: a standard U-Net requires a substantial 140GB of\nmemory and 2600 seconds per epoch on a research-grade GPU when training on\n1024x1024 images. In this paper, we introduce GLIMPSE, a local processing\nneural network for computed tomography which reconstructs a pixel value by\nfeeding only the measurements associated with the neighborhood of the pixel to\na simple MLP. While achieving comparable or better performance with successful\nCNNs like the U-Net on in-distribution test data, GLIMPSE significantly\noutperforms them on OOD samples while maintaining a memory footprint almost\nindependent of image resolution; 5GB memory suffices to train on 1024x1024\nimages. Further, we built GLIMPSE to be fully differentiable, which enables\nfeats such as recovery of accurate projection angles if they are out of\ncalibration.", "published": "2024-01-01", "categories": ["cs.CV", "cs.LG", "eess.IV"], "links": "http://arxiv.org/abs/2401.00816v1"}
{"title": "Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases", "author": "Yifei Chen, Chenyan Zhang, Ben Chen, Yiyu Huang, Yifei Sun, Changmiao Wang, Xianjun Fu, Yuxing Dai, Feiwei Qin, Yong Peng, Yu Gao", "abstract": "In standard hospital blood tests, the traditional process requires doctors to\nmanually isolate leukocytes from microscopic images of patients' blood using\nmicroscopes. These isolated leukocytes are then categorized via automatic\nleukocyte classifiers to determine the proportion and volume of different types\nof leukocytes present in the blood samples, aiding disease diagnosis. This\nmethodology is not only time-consuming and labor-intensive, but it also has a\nhigh propensity for errors due to factors such as image quality and\nenvironmental conditions, which could potentially lead to incorrect subsequent\nclassifications and misdiagnosis. To address these issues, this paper proposes\nan innovative method of leukocyte detection: the Multi-level Feature Fusion and\nDeformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte\nscale disparity, we designed the High-level Screening-feature Fusion Pyramid\n(HS-FPN), enabling multi-level fusion. This model uses high-level features as\nweights to filter low-level feature information via a channel attention module\nand then merges the screened information with the high-level features, thus\nenhancing the model's feature expression capability. Further, we address the\nissue of leukocyte feature scarcity by incorporating a multi-scale deformable\nself-attention module in the encoder and using the self-attention and\ncross-deformable attention mechanisms in the decoder, which aids in the\nextraction of the global features of the leukocyte feature maps. The\neffectiveness, superiority, and generalizability of the proposed MFDS-DETR\nmethod are confirmed through comparisons with other cutting-edge leukocyte\ndetection models using the private WBCDD, public LISC and BCCD datasets. Our\nsource code and private WBCCD dataset are available at\nhttps://github.com/JustlfC03/MFDS-DETR.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.00926v2"}
{"title": "Retrieval-Augmented Egocentric Video Captioning", "author": "Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie", "abstract": "Understanding human actions from videos of first-person view poses\nsignificant challenges. Most prior approaches explore representation learning\non egocentric videos only, while overlooking the potential benefit of\nexploiting existing large-scale third-person videos. In this paper, (1) we\ndevelop EgoInstructor, a retrieval-augmented multimodal captioning model that\nautomatically retrieves semantically relevant third-person instructional videos\nto enhance the video captioning of egocentric videos. (2) For training the\ncross-view retrieval module, we devise an automatic pipeline to discover\nego-exo video pairs from distinct large-scale egocentric and exocentric\ndatasets. (3) We train the cross-view retrieval module with a novel EgoExoNCE\nloss that pulls egocentric and exocentric video features closer by aligning\nthem to shared text features that describe similar actions. (4) Through\nextensive experiments, our cross-view retrieval module demonstrates superior\nperformance across seven benchmarks. Regarding egocentric video captioning,\nEgoInstructor exhibits significant improvements by leveraging third-person\nvideos as references.", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00789v2"}
{"title": "Bracketing is All You Need: Unifying Image Restoration and Enhancement Tasks with Multi-Exposure Images", "author": "Zhilu Zhang, Shuohao Zhang, Renlong Wu, Zifei Yan, Wangmeng Zuo", "abstract": "It is challenging but highly desired to acquire high-quality photos with\nclear content in low-light environments. Although multi-image processing\nmethods (using burst, dual-exposure, or multi-exposure images) have made\nsignificant progress in addressing this issue, they typically focus exclusively\non specific restoration or enhancement tasks, being insufficient in exploiting\nmulti-image. Motivated by that multi-exposure images are complementary in\ndenoising, deblurring, high dynamic range imaging, and super-resolution, we\npropose to utilize bracketing photography to unify restoration and enhancement\ntasks in this work. Due to the difficulty in collecting real-world pairs, we\nsuggest a solution that first pre-trains the model with synthetic paired data\nand then adapts it to real-world unlabeled images. In particular, a temporally\nmodulated recurrent network (TMRNet) and self-supervised adaptation method are\nproposed. Moreover, we construct a data simulation pipeline to synthesize pairs\nand collect real-world images from 200 nighttime scenarios. Experiments on both\ndatasets show that our method performs favorably against the state-of-the-art\nmulti-image processing ones. The dataset, code, and pre-trained models are\navailable at https://github.com/cszhilu1998/BracketIRE.", "published": "2024-01-01", "categories": ["cs.CV", "eess.IV"], "links": "http://arxiv.org/abs/2401.00766v1"}
{"title": "New Job, New Gender? Measuring the Social Bias in Image Generation Models", "author": "Wenxuan Wang, Haonan Bai, Jen-tse Huang, Yuxuan Wan, Youliang Yuan, Haoyi Qiu, Nanyun Peng, Michael R. Lyu", "abstract": "Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel metamorphic testing\nframework that can accurately, automatically and comprehensively trigger social\nbias in image generation models. BiasPainter uses a diverse range of seed\nimages of individuals and prompts the image generation models to edit these\nimages using gender, race, and age-neutral queries. These queries span 62\nprofessions, 39 activities, 57 types of objects, and 70 personality traits. The\nframework then compares the edited images to the original seed images, focusing\non any changes related to gender, race, and age. BiasPainter adopts a testing\noracle that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. To evaluate the\neffectiveness of BiasPainter, we use BiasPainter to test five widely-used\ncommercial image generation software and models, such as stable diffusion and\nMidjourney. Experimental results show that 100\\% of the generated test cases\ncan successfully trigger social bias in image generation models.", "published": "2024-01-01", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "links": "http://arxiv.org/abs/2401.00763v1"}
{"title": "Beyond Subspace Isolation: Many-to-Many Transformer for Light Field Image Super-resolution", "author": "Zeke Zexi Hu, Xiaoming Chen, Vera Yuk Ying Chung, Yiran Shen", "abstract": "The effective extraction of spatial-angular features plays a crucial role in\nlight field image super-resolution (LFSR) tasks, and the introduction of\nconvolution and Transformers leads to significant improvement in this area.\nNevertheless, due to the large 4D data volume of light field images, many\nexisting methods opted to decompose the data into a number of lower-dimensional\nsubspaces and perform Transformers in each sub-space individually. As a side\neffect, these methods inadvertently restrict the self-attention mechanisms to a\nOne-to-One scheme accessing only a limited subset of LF data, explicitly\npreventing comprehensive optimization on all spatial and angular cues. In this\npaper, we identify this limitation as subspace isolation and introduce a novel\nMany-to-Many Transformer (M2MT) to address it. M2MT aggregates angular\ninformation in the spatial subspace before performing the self-attention\nmechanism. It enables complete access to all information across all\nsub-aperture images (SAIs) in a light field image. Consequently, M2MT is\nenabled to comprehensively capture long-range correlation dependencies. With\nM2MT as the pivotal component, we develop a simple yet effective M2MT network\nfor LFSR. Our experimental results demonstrate that M2MT achieves\nstate-of-the-art performance across various public datasets. We further conduct\nin-depth analysis using local attribution maps (LAM) to obtain visual\ninterpretability, and the results validate that M2MT is empowered with a truly\nnon-local context in both spatial and angular subspaces to mitigate subspace\nisolation and acquire effective spatial-angular representation.", "published": "2024-01-01", "categories": ["eess.IV", "cs.CV"], "links": "http://arxiv.org/abs/2401.00740v1"}
{"title": "DiffMorph: Text-less Image Morphing with Diffusion Models", "author": "Shounak Chatterjee", "abstract": "Text-conditioned image generation models are a prevalent use of AI image\nsynthesis, yet intuitively controlling output guided by an artist remains\nchallenging. Current methods require multiple images and textual prompts for\neach object to specify them as concepts to generate a single customized image.\n  On the other hand, our work, \\verb|DiffMorph|, introduces a novel approach\nthat synthesizes images that mix concepts without the use of textual prompts.\nOur work integrates a sketch-to-image module to incorporate user sketches as\ninput. \\verb|DiffMorph| takes an initial image with conditioning artist-drawn\nsketches to generate a morphed image.\n  We employ a pre-trained text-to-image diffusion model and fine-tune it to\nreconstruct each image faithfully. We seamlessly merge images and concepts from\nsketches into a cohesive composition. The image generation capability of our\nwork is demonstrated through our results and a comparison of these with\nprompt-based image generation.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.00739v1"}
{"title": "Diffusion Models, Image Super-Resolution And Everything: A Survey", "author": "Brian B. Moser, Arundhati S. Shanbhag, Federico Raue, Stanislav Frolov, Sebastian Palacio, Andreas Dengel", "abstract": "Diffusion Models (DMs) represent a significant advancement in image\nSuper-Resolution (SR), aligning technical image quality more closely with human\npreferences and expanding SR applications. DMs address critical limitations of\nprevious methods, enhancing overall realism and details in SR images. However,\nDMs suffer from color-shifting issues, and their high computational costs call\nfor efficient sampling alternatives, underscoring the challenge of balancing\ncomputational efficiency and image quality. This survey gives an overview of\nDMs applied to image SR and offers a detailed analysis that underscores the\nunique characteristics and methodologies within this domain, distinct from\nbroader existing reviews in the field. It presents a unified view of DM\nfundamentals and explores research directions, including alternative input\ndomains, conditioning strategies, guidance, corruption spaces, and zero-shot\nmethods. This survey provides insights into the evolution of image SR with DMs,\naddressing current trends, challenges, and future directions in this rapidly\nevolving field.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI", "cs.GL", "cs.LG", "cs.MM"], "links": "http://arxiv.org/abs/2401.00736v1"}
{"title": "Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence", "author": "Ruizhuo Xu, Linzhi Huang, Mei Wang, Jiani Hu, Weihong Deng", "abstract": "Self-supervised pre-training paradigms have been extensively explored in the\nfield of skeleton-based action recognition. In particular, methods based on\nmasked prediction have pushed the performance of pre-training to a new height.\nHowever, these methods take low-level features, such as raw joint coordinates\nor temporal motion, as prediction targets for the masked regions, which is\nsuboptimal. In this paper, we show that using high-level contextualized\nfeatures as prediction targets can achieve superior performance. Specifically,\nwe propose Skeleton2vec, a simple and efficient self-supervised 3D action\nrepresentation learning framework, which utilizes a transformer-based teacher\nencoder taking unmasked training samples as input to create latent\ncontextualized representations as prediction targets. Benefiting from the\nself-attention mechanism, the latent representations generated by the teacher\nencoder can incorporate the global context of the entire training samples,\nleading to a richer training task. Additionally, considering the high temporal\ncorrelations in skeleton sequences, we propose a motion-aware tube masking\nstrategy which divides the skeleton sequence into several tubes and performs\npersistent masking within each tube based on motion priors, thus forcing the\nmodel to build long-range spatio-temporal connections and focus on\naction-semantic richer regions. Extensive experiments on NTU-60, NTU-120, and\nPKU-MMD datasets demonstrate that our proposed Skeleton2vec outperforms\nprevious methods and achieves state-of-the-art results.", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00921v1"}
{"title": "NightRain: Nighttime Video Deraining via Adaptive-Rain-Removal and Adaptive-Correction", "author": "Beibei Lin, Yeying Jin, Wending Yan, Wei Ye, Yuan Yuan, Shunli Zhang, Robby Tan", "abstract": "Existing deep-learning-based methods for nighttime video deraining rely on\nsynthetic data due to the absence of real-world paired data. However, the\nintricacies of the real world, particularly with the presence of light effects\nand low-light regions affected by noise, create significant domain gaps,\nhampering synthetic-trained models in removing rain streaks properly and\nleading to over-saturation and color shifts. Motivated by this, we introduce\nNightRain, a novel nighttime video deraining method with adaptive-rain-removal\nand adaptive-correction. Our adaptive-rain-removal uses unlabeled rain videos\nto enable our model to derain real-world rain videos, particularly in regions\naffected by complex light effects. The idea is to allow our model to obtain\nrain-free regions based on the confidence scores. Once rain-free regions and\nthe corresponding regions from our input are obtained, we can have region-based\npaired real data. These paired data are used to train our model using a\nteacher-student framework, allowing the model to iteratively learn from less\nchallenging regions to more challenging regions. Our adaptive-correction aims\nto rectify errors in our model's predictions, such as over-saturation and color\nshifts. The idea is to learn from clear night input training videos based on\nthe differences or distance between those input videos and their corresponding\npredictions. Our model learns from these differences, compelling our model to\ncorrect the errors. From extensive experiments, our method demonstrates\nstate-of-the-art performance. It achieves a PSNR of 26.73dB, surpassing\nexisting nighttime video deraining methods by a substantial margin of 13.7%.", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00729v1"}
{"title": "MultiFusionNet: Multilayer Multimodal Fusion of Deep Neural Networks for Chest X-Ray Image Classification", "author": "Saurabh Agarwal, K. V. Arya, Yogesh Kumar Meena", "abstract": "Chest X-ray imaging is a critical diagnostic tool for identifying pulmonary\ndiseases. However, manual interpretation of these images is time-consuming and\nerror-prone. Automated systems utilizing convolutional neural networks (CNNs)\nhave shown promise in improving the accuracy and efficiency of chest X-ray\nimage classification. While previous work has mainly focused on using feature\nmaps from the final convolution layer, there is a need to explore the benefits\nof leveraging additional layers for improved disease classification. Extracting\nrobust features from limited medical image datasets remains a critical\nchallenge. In this paper, we propose a novel deep learning-based multilayer\nmultimodal fusion model that emphasizes extracting features from different\nlayers and fusing them. Our disease detection model considers the\ndiscriminatory information captured by each layer. Furthermore, we propose the\nfusion of different-sized feature maps (FDSFM) module to effectively merge\nfeature maps from diverse layers. The proposed model achieves a significantly\nhigher accuracy of 97.21% and 99.60% for both three-class and two-class\nclassifications, respectively. The proposed multilayer multimodal fusion model,\nalong with the FDSFM module, holds promise for accurate disease classification\nand can also be extended to other disease classifications in chest X-ray\nimages.", "published": "2024-01-01", "categories": ["eess.IV", "cs.CV", "cs.LG"], "links": "http://arxiv.org/abs/2401.00728v1"}
{"title": "BRAU-Net++: U-Shaped Hybrid CNN-Transformer Network for Medical Image Segmentation", "author": "Libin Lan, Pengzhou Cai, Lu Jiang, Xiaojuan Liu, Yongmei Li, Yudong Zhang", "abstract": "Accurate medical image segmentation is essential for clinical quantification,\ndisease diagnosis, treatment planning and many other applications. Both\nconvolution-based and transformer-based u-shaped architectures have made\nsignificant success in various medical image segmentation tasks. The former can\nefficiently learn local information of images while requiring much more\nimage-specific inductive biases inherent to convolution operation. The latter\ncan effectively capture long-range dependency at different feature scales using\nself-attention, whereas it typically encounters the challenges of quadratic\ncompute and memory requirements with sequence length increasing. To address\nthis problem, through integrating the merits of these two paradigms in a\nwell-designed u-shaped architecture, we propose a hybrid yet effective\nCNN-Transformer network, named BRAU-Net++, for an accurate medical image\nsegmentation task. Specifically, BRAU-Net++ uses bi-level routing attention as\nthe core building block to design our u-shaped encoder-decoder structure, in\nwhich both encoder and decoder are hierarchically constructed, so as to learn\nglobal semantic information while reducing computational complexity.\nFurthermore, this network restructures skip connection by incorporating\nchannel-spatial attention which adopts convolution operations, aiming to\nminimize local spatial information loss and amplify global\ndimension-interaction of multi-scale features. Extensive experiments on three\npublic benchmark datasets demonstrate that our proposed approach surpasses\nother state-of-the-art methods including its baseline: BRAU-Net under almost\nall evaluation metrics. We achieve the average Dice-Similarity Coefficient\n(DSC) of 82.47, 90.10, and 92.94 on Synapse multi-organ segmentation, ISIC-2018\nChallenge, and CVC-ClinicDB, as well as the mIoU of 84.01 and 88.17 on\nISIC-2018 Challenge and CVC-ClinicDB, respectively.", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00722v1"}
{"title": "Depth Map Denoising Network and Lightweight Fusion Network for Enhanced 3D Face Recognition", "author": "Ruizhuo Xu, Ke Wang, Chao Deng, Mei Wang, Xi Chen, Wenhui Huang, Junlan Feng, Weihong Deng", "abstract": "With the increasing availability of consumer depth sensors, 3D face\nrecognition (FR) has attracted more and more attention. However, the data\nacquired by these sensors are often coarse and noisy, making them impractical\nto use directly. In this paper, we introduce an innovative Depth map denoising\nnetwork (DMDNet) based on the Denoising Implicit Image Function (DIIF) to\nreduce noise and enhance the quality of facial depth images for low-quality 3D\nFR. After generating clean depth faces using DMDNet, we further design a\npowerful recognition network called Lightweight Depth and Normal Fusion network\n(LDNFNet), which incorporates a multi-branch fusion block to learn unique and\ncomplementary features between different modalities such as depth and normal\nimages. Comprehensive experiments conducted on four distinct low-quality\ndatabases demonstrate the effectiveness and robustness of our proposed methods.\nFurthermore, when combining DMDNet and LDNFNet, we achieve state-of-the-art\nresults on the Lock3DFace database.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.00719v1"}
{"title": "Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data", "author": "Michalis Pistos, Islem Rekik", "abstract": "The understanding of the convoluted evolution of infant brain networks during\nthe first postnatal year is pivotal for identifying the dynamics of early brain\nconnectivity development. Existing deep learning solutions suffer from three\nmajor limitations. First, they cannot generalize to multi-trajectory prediction\ntasks, where each graph trajectory corresponds to a particular imaging modality\nor connectivity type (e.g., T1-w MRI). Second, existing models require\nextensive training datasets to achieve satisfactory performance which are often\nchallenging to obtain. Third, they do not efficiently utilize incomplete time\nseries data. To address these limitations, we introduce FedGmTE-Net++, a\nfederated graph-based multi-trajectory evolution network. Using the power of\nfederation, we aggregate local learnings among diverse hospitals with limited\ndatasets. As a result, we enhance the performance of each hospital's local\ngenerative model, while preserving data privacy. The three key innovations of\nFedGmTE-Net++ are: (i) presenting the first federated learning framework\nspecifically designed for brain multi-trajectory evolution prediction in a\ndata-scarce environment, (ii) incorporating an auxiliary regularizer in the\nlocal objective function to exploit all the longitudinal brain connectivity\nwithin the evolution trajectory and maximize data utilization, (iii)\nintroducing a two-step imputation process, comprising a preliminary KNN-based\nprecompletion followed by an imputation refinement step that employs regressors\nto improve similarity scores and refine imputations. Our comprehensive\nexperimental results showed the outperformance of FedGmTE-Net++ in brain\nmulti-trajectory prediction from a single baseline graph in comparison with\nbenchmark methods.", "published": "2024-01-01", "categories": ["q-bio.NC", "cs.AI", "cs.CV", "cs.LG"], "links": "http://arxiv.org/abs/2401.01383v1"}
{"title": "Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven Body Controllable Attribute", "author": "Chaoqun Gong, Yuqin Dai, Ronghui Li, Achun Bao, Jun Li, Jian Yang, Yachao Zhang, Xiu Li", "abstract": "Generating 3D human models directly from text helps reduce the cost and time\nof character modeling. However, achieving multi-attribute controllable and\nrealistic 3D human avatar generation is still challenging due to feature\ncoupling and the scarcity of realistic 3D human avatar datasets. To address\nthese issues, we propose Text2Avatar, which can generate realistic-style 3D\navatars based on the coupled text prompts. Text2Avatar leverages a discrete\ncodebook as an intermediate feature to establish a connection between text and\navatars, enabling the disentanglement of features. Furthermore, to alleviate\nthe scarcity of realistic style 3D human avatar data, we utilize a pre-trained\nunconditional 3D human avatar generation model to obtain a large amount of 3D\navatar pseudo data, which allows Text2Avatar to achieve realistic style\ngeneration. Experimental results demonstrate that our method can generate\nrealistic 3D avatars from coupled textual data, which is challenging for other\nexisting methods in this field.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.00711v1"}
{"title": "Exploring Multi-Modal Control in Music-Driven Dance Generation", "author": "Ronghui Li, Yuqin Dai, Yachao Zhang, Jun Li, Jian Yang, Jie Guo, Xiu Li", "abstract": "Existing music-driven 3D dance generation methods mainly concentrate on\nhigh-quality dance generation, but lack sufficient control during the\ngeneration process. To address these issues, we propose a unified framework\ncapable of generating high-quality dance movements and supporting multi-modal\ncontrol, including genre control, semantic control, and spatial control. First,\nwe decouple the dance generation network from the dance control network,\nthereby avoiding the degradation in dance quality when adding additional\ncontrol information. Second, we design specific control strategies for\ndifferent control information and integrate them into a unified framework.\nExperimental results show that the proposed dance generation framework\noutperforms state-of-the-art methods in terms of motion quality and\ncontrollability.", "published": "2024-01-01", "categories": ["cs.SD", "cs.CV", "eess.AS"], "links": "http://arxiv.org/abs/2401.01382v1"}
{"title": "Revisiting Nonlocal Self-Similarity from Continuous Representation", "author": "Yisi Luo, Xile Zhao, Deyu Meng", "abstract": "Nonlocal self-similarity (NSS) is an important prior that has been\nsuccessfully applied in multi-dimensional data processing tasks, e.g., image\nand video recovery. However, existing NSS-based methods are solely suitable for\nmeshgrid data such as images and videos, but are not suitable for emerging\noff-meshgrid data, e.g., point cloud and climate data. In this work, we revisit\nthe NSS from the continuous representation perspective and propose a novel\nContinuous Representation-based NonLocal method (termed as CRNL), which has two\ninnovative features as compared with classical nonlocal methods. First, based\non the continuous representation, our CRNL unifies the measure of\nself-similarity for on-meshgrid and off-meshgrid data and thus is naturally\nsuitable for both of them. Second, the nonlocal continuous groups can be more\ncompactly and efficiently represented by the coupled low-rank function\nfactorization, which simultaneously exploits the similarity within each group\nand across different groups, while classical nonlocal methods neglect the\nsimilarity across groups. This elaborately designed coupled mechanism allows\nour method to enjoy favorable performance over conventional NSS methods in\nterms of both effectiveness and efficiency. Extensive multi-dimensional data\nprocessing experiments on-meshgrid (e.g., image inpainting and image denoising)\nand off-meshgrid (e.g., climate data prediction and point cloud recovery)\nvalidate the versatility, effectiveness, and efficiency of our CRNL as compared\nwith state-of-the-art methods.", "published": "2024-01-01", "categories": ["cs.CV", "eess.IV"], "links": "http://arxiv.org/abs/2401.00708v1"}
{"title": "Towards Efficient and Effective Text-to-Video Retrieval with Coarse-to-Fine Visual Representation Learning", "author": "Kaibin Tian, Yanhua Cheng, Yi Liu, Xinglin Hou, Quan Chen, Han Li", "abstract": "In recent years, text-to-video retrieval methods based on CLIP have\nexperienced rapid development. The primary direction of evolution is to exploit\nthe much wider gamut of visual and textual cues to achieve alignment.\nConcretely, those methods with impressive performance often design a heavy\nfusion block for sentence (words)-video (frames) interaction, regardless of the\nprohibitive computation complexity. Nevertheless, these approaches are not\noptimal in terms of feature utilization and retrieval efficiency. To address\nthis issue, we adopt multi-granularity visual feature learning, ensuring the\nmodel's comprehensiveness in capturing visual content features spanning from\nabstract to detailed levels during the training phase. To better leverage the\nmulti-granularity features, we devise a two-stage retrieval architecture in the\nretrieval phase. This solution ingeniously balances the coarse and fine\ngranularity of retrieval content. Moreover, it also strikes a harmonious\nequilibrium between retrieval effectiveness and efficiency. Specifically, in\ntraining phase, we design a parameter-free text-gated interaction block (TIB)\nfor fine-grained video representation learning and embed an extra Pearson\nConstraint to optimize cross-modal representation learning. In retrieval phase,\nwe use coarse-grained video representations for fast recall of top-k\ncandidates, which are then reranked by fine-grained video representations.\nExtensive experiments on four benchmarks demonstrate the efficiency and\neffectiveness. Notably, our method achieves comparable performance with the\ncurrent state-of-the-art methods while being nearly 50 times faster.", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00701v1"}
{"title": "An attempt to generate new bridge types from latent space of generative adversarial network", "author": "Hongjun Zhang", "abstract": "Try to generate new bridge types using generative artificial intelligence\ntechnology. Symmetric structured image dataset of three-span beam bridge, arch\nbridge, cable-stayed bridge and suspension bridge are used . Based on Python\nprogramming language, TensorFlow and Keras deep learning platform framework ,\nas well as Wasserstein loss function and Lipschitz constraints, generative\nadversarial network is constructed and trained. From the obtained low\ndimensional bridge-type latent space sampling, new bridge types with asymmetric\nstructures can be generated. Generative adversarial network can create new\nbridge types by organically combining different structural components on the\nbasis of human original bridge types. It has a certain degree of human original\nability. Generative artificial intelligence technology can open up imagination\nspace and inspire humanity.", "published": "2024-01-01", "categories": ["cs.LG", "cs.AI", "cs.CV"], "links": "http://arxiv.org/abs/2401.00700v1"}
{"title": "Credible Teacher for Semi-Supervised Object Detection in Open Scene", "author": "Jingyu Zhuang, Kuo Wang, Liang Lin, Guanbin Li", "abstract": "Semi-Supervised Object Detection (SSOD) has achieved resounding success by\nleveraging unlabeled data to improve detection performance. However, in Open\nScene Semi-Supervised Object Detection (O-SSOD), unlabeled data may contains\nunknown objects not observed in the labeled data, which will increase\nuncertainty in the model's predictions for known objects. It is detrimental to\nthe current methods that mainly rely on self-training, as more uncertainty\nleads to the lower localization and classification precision of pseudo labels.\nTo this end, we propose Credible Teacher, an end-to-end framework. Credible\nTeacher adopts an interactive teaching mechanism using flexible labels to\nprevent uncertain pseudo labels from misleading the model and gradually reduces\nits uncertainty through the guidance of other credible pseudo labels. Empirical\nresults have demonstrated our method effectively restrains the adverse effect\ncaused by O-SSOD and significantly outperforms existing counterparts.", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00695v2"}
{"title": "Self-supervised learning for skin cancer diagnosis with limited training data", "author": "Hamish Haggerty, Rohitash Chandra", "abstract": "Cancer diagnosis is a well-studied problem in machine learning since early\ndetection of cancer is often the determining factor in prognosis. Supervised\ndeep learning achieves excellent results in cancer image classification,\nusually through transfer learning. However, these models require large amounts\nof labelled data and for several types of cancer, large labelled datasets do\nnot exist. In this paper, we demonstrate that a model pre-trained using a\nself-supervised learning algorithm known as Barlow Twins can outperform the\nconventional supervised transfer learning pipeline. We juxtapose two base\nmodels: i) pretrained in a supervised fashion on ImageNet; ii) pretrained in a\nself-supervised fashion on ImageNet. Both are subsequently fine tuned on a\nsmall labelled skin lesion dataset and evaluated on a large test set. We\nachieve a mean test accuracy of 70\\% for self-supervised transfer in comparison\nto 66\\% for supervised transfer. Interestingly, boosting performance further is\npossible by self-supervised pretraining a second time (on unlabelled skin\nlesion images) before subsequent fine tuning. This hints at an alternative path\nto collecting more labelled data in settings where this is challenging - namely\njust collecting more unlabelled images. Our framework is applicable to cancer\nimage classification models in the low-labelled data regime.", "published": "2024-01-01", "categories": ["eess.IV", "cs.CV", "cs.LG"], "links": "http://arxiv.org/abs/2401.00692v1"}
{"title": "1st Place Solution for 5th LSVOS Challenge: Referring Video Object Segmentation", "author": "Zhuoyan Luo, Yicheng Xiao, Yong Liu, Yitong Wang, Yansong Tang, Xiu Li, Yujiu Yang", "abstract": "The recent transformer-based models have dominated the Referring Video Object\nSegmentation (RVOS) task due to the superior performance. Most prior works\nadopt unified DETR framework to generate segmentation masks in\nquery-to-instance manner. In this work, we integrate strengths of that leading\nRVOS models to build up an effective paradigm. We first obtain binary mask\nsequences from the RVOS models. To improve the consistency and quality of\nmasks, we propose Two-Stage Multi-Model Fusion strategy. Each stage rationally\nensembles RVOS models based on framework design as well as training strategy,\nand leverages different video object segmentation (VOS) models to enhance mask\ncoherence by object propagation mechanism. Our method achieves 75.7% J&F on\nRef-Youtube-VOS validation set and 70% J&F on test set, which ranks 1st place\non 5th Large-scale Video Object Segmentation Challenge (ICCV 2023) track 3.\nCode is available at https://github.com/RobertLuo1/iccv2023_RVOS_Challenge.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.00663v1"}
{"title": "Optimizing ADMM and Over-Relaxed ADMM Parameters for Linear Quadratic Problems", "author": "Jintao Song, Wenqi Lu, Yunwen Lei, Yuchao Tang, Zhenkuan Pan, Jinming Duan", "abstract": "The Alternating Direction Method of Multipliers (ADMM) has gained significant\nattention across a broad spectrum of machine learning applications.\nIncorporating the over-relaxation technique shows potential for enhancing the\nconvergence rate of ADMM. However, determining optimal algorithmic parameters,\nincluding both the associated penalty and relaxation parameters, often relies\non empirical approaches tailored to specific problem domains and contextual\nscenarios. Incorrect parameter selection can significantly hinder ADMM's\nconvergence rate. To address this challenge, in this paper we first propose a\ngeneral approach to optimize the value of penalty parameter, followed by a\nnovel closed-form formula to compute the optimal relaxation parameter in the\ncontext of linear quadratic problems (LQPs). We then experimentally validate\nour parameter selection methods through random instantiations and diverse\nimaging applications, encompassing diffeomorphic image registration, image\ndeblurring, and MRI reconstruction.", "published": "2024-01-01", "categories": ["math.OC", "cs.CV", "math.SP"], "links": "http://arxiv.org/abs/2401.00657v1"}
{"title": "PROMPT-IML: Image Manipulation Localization with Pre-trained Foundation Models Through Prompt Tuning", "author": "Xuntao Liu, Yuzhou Yang, Qichao Ying, Zhenxing Qian, Xinpeng Zhang, Sheng Li", "abstract": "Deceptive images can be shared in seconds with social networking services,\nposing substantial risks. Tampering traces, such as boundary artifacts and\nhigh-frequency information, have been significantly emphasized by massive\nnetworks in the Image Manipulation Localization (IML) field. However, they are\nprone to image post-processing operations, which limit the generalization and\nrobustness of existing methods. We present a novel Prompt-IML framework. We\nobserve that humans tend to discern the authenticity of an image based on both\nsemantic and high-frequency information, inspired by which, the proposed\nframework leverages rich semantic knowledge from pre-trained visual foundation\nmodels to assist IML. We are the first to design a framework that utilizes\nvisual foundation models specially for the IML task. Moreover, we design a\nFeature Alignment and Fusion module to align and fuse features of semantic\nfeatures with high-frequency features, which aims at locating tampered regions\nfrom multiple perspectives. Experimental results demonstrate that our model can\nachieve better performance on eight typical fake image datasets and outstanding\nrobustness.", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00653v1"}
{"title": "From Covert Hiding to Visual Editing: Robust Generative Video Steganography", "author": "Xueying Mao, Xiaoxiao Hu, Wanli Peng, Zhenliang Gan, Qichao Ying, Zhenxing Qian, Sheng Li, Xinpeng Zhang", "abstract": "Traditional video steganography methods are based on modifying the covert\nspace for embedding, whereas we propose an innovative approach that embeds\nsecret message within semantic feature for steganography during the video\nediting process. Although existing traditional video steganography methods\ndisplay a certain level of security and embedding capacity, they lack adequate\nrobustness against common distortions in online social networks (OSNs). In this\npaper, we introduce an end-to-end robust generative video steganography network\n(RoGVS), which achieves visual editing by modifying semantic feature of videos\nto embed secret message. We employ face-swapping scenario to showcase the\nvisual editing effects. We first design a secret message embedding module to\nadaptively hide secret message into the semantic feature of videos. Extensive\nexperiments display that the proposed RoGVS method applied to facial video\ndatasets demonstrate its superiority over existing video and image\nsteganography techniques in terms of both robustness and capacity.", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00652v1"}
{"title": "Geometry Depth Consistency in RGBD Relative Pose Estimation", "author": "Sourav Kumar, Chiang-Heng Chien, Benjamin Kimia", "abstract": "Relative pose estimation for RGBD cameras is crucial in a number of\napplications. Previous approaches either rely on the RGB aspect of the images\nto estimate pose thus not fully making use of depth in the estimation process\nor estimate pose from the 3D cloud of points that each image produces, thus not\nmaking full use of RGB information. This paper shows that if one pair of\ncorrespondences is hypothesized from the RGB-based ranked-ordered\ncorrespondence list, then the space of remaining correspondences is restricted\nto corresponding pairs of curves nested around the hypothesized correspondence,\nimplicitly capturing depth consistency. This simple Geometric Depth Constraint\n(GDC) significantly reduces potential matches. In effect this becomes a filter\non possible correspondences that helps reduce the number of outliers and thus\nexpedites RANSAC significantly. As such, the same budget of time allows for\nmore RANSAC iterations and therefore additional robustness and a significant\nspeedup. In addition, the paper proposed a Nested RANSAC approach that also\nspeeds up the process, as shown through experiments on TUM, ICL-NUIM, and RGBD\nScenes v2 datasets.", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00639v1"}
{"title": "ScatterFormer: Efficient Voxel Transformer with Scattered Linear Attention", "author": "Chenhang He, Ruihuang Li, Guowen Zhang, Lei Zhang", "abstract": "Window-based transformers have demonstrated strong ability in large-scale\npoint cloud understanding by capturing context-aware representations with\naffordable attention computation in a more localized manner. However, because\nof the sparse nature of point clouds, the number of voxels per window varies\nsignificantly. Current methods partition the voxels in each window into\nmultiple subsets of equal size, which cost expensive overhead in sorting and\npadding the voxels, making them run slower than sparse convolution based\nmethods. In this paper, we present ScatterFormer, which, for the first time to\nour best knowledge, could directly perform attention on voxel sets with\nvariable length. The key of ScatterFormer lies in the innovative Scatter Linear\nAttention (SLA) module, which leverages the linear attention mechanism to\nprocess in parallel all voxels scattered in different windows. Harnessing the\nhierarchical computation units of the GPU and matrix blocking algorithm, we\nreduce the latency of the proposed SLA module to less than 1 ms on moderate\nGPUs. Besides, we develop a cross-window interaction module to simultaneously\nenhance the local representation and allow the information flow across windows,\neliminating the need for window shifting. Our proposed ScatterFormer\ndemonstrates 73 mAP (L2) on the large-scale Waymo Open Dataset and 70.5 NDS on\nthe NuScenes dataset, running at an outstanding detection rate of 28 FPS. Code\nis available at https://github.com/skyhehe123/ScatterFormer", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00912v1"}
{"title": "Towards Improved Proxy-based Deep Metric Learning via Data-Augmented Domain Adaptation", "author": "Li Ren, Chen Chen, Liqiang Wang, Kien Hua", "abstract": "Deep Metric Learning (DML) plays an important role in modern computer vision\nresearch, where we learn a distance metric for a set of image representations.\nRecent DML techniques utilize the proxy to interact with the corresponding\nimage samples in the embedding space. However, existing proxy-based DML methods\nfocus on learning individual proxy-to-sample distance while the overall\ndistribution of samples and proxies lacks attention. In this paper, we present\na novel proxy-based DML framework that focuses on aligning the sample and proxy\ndistributions to improve the efficiency of proxy-based DML losses.\nSpecifically, we propose the Data-Augmented Domain Adaptation (DADA) method to\nadapt the domain gap between the group of samples and proxies. To the best of\nour knowledge, we are the first to leverage domain adaptation to boost the\nperformance of proxy-based DML. We show that our method can be easily plugged\ninto existing proxy-based DML losses. Our experiments on benchmarks, including\nthe popular CUB-200-2011, CARS196, Stanford Online Products, and In-Shop\nClothes Retrieval, show that our learning algorithm significantly improves the\nexisting proxy losses and achieves superior results compared to the existing\nmethods.", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00617v1"}
{"title": "GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for One-shot Generalizable Neural Radiance Fields", "author": "Xiao Pan, Zongxin Yang, Shuai Bai, Yi Yang", "abstract": "In this paper, we focus on the One-shot Novel View Synthesis (O-NVS) task\nwhich targets synthesizing photo-realistic novel views given only one reference\nimage per scene. Previous One-shot Generalizable Neural Radiance Fields\n(OG-NeRF) methods solve this task in an inference-time finetuning-free manner,\nyet suffer the blurry issue due to the encoder-only architecture that highly\nrelies on the limited reference image. On the other hand, recent\ndiffusion-based image-to-3d methods show vivid plausible results via distilling\npre-trained 2D diffusion models into a 3D representation, yet require tedious\nper-scene optimization. Targeting these issues, we propose the GD$^2$-NeRF, a\nGenerative Detail compensation framework via GAN and Diffusion that is both\ninference-time finetuning-free and with vivid plausible details. In detail,\nfollowing a coarse-to-fine strategy, GD$^2$-NeRF is mainly composed of a\nOne-stage Parallel Pipeline (OPP) and a 3D-consistent Detail Enhancer\n(Diff3DE). At the coarse stage, OPP first efficiently inserts the GAN model\ninto the existing OG-NeRF pipeline for primarily relieving the blurry issue\nwith in-distribution priors captured from the training dataset, achieving a\ngood balance between sharpness (LPIPS, FID) and fidelity (PSNR, SSIM). Then, at\nthe fine stage, Diff3DE further leverages the pre-trained image diffusion\nmodels to complement rich out-distribution details while maintaining decent 3D\nconsistency. Extensive experiments on both the synthetic and real-world\ndatasets show that GD$^2$-NeRF noticeably improves the details while without\nper-scene finetuning.", "published": "2024-01-01", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00616v2"}
