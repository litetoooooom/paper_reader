{"title": "GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised Learning", "author": "Aarash Feizi, Randall Balestriero, Adriana Romero-Soriano, Reihaneh Rabbany", "abstract": "We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a\ngeneral method to inject a priori knowledge into Self-Supervised Learning (SSL)\npositive samples selection. Current SSL methods leverage Data-Augmentations\n(DA) for generating positive samples and incorporate prior knowledge - an\nincorrect, or too weak DA will drastically reduce the quality of the learned\nrepresentation. GPS-SSL proposes instead to design a metric space where\nEuclidean distances become a meaningful proxy for semantic relationship. In\nthat space, it is now possible to generate positive samples from nearest\nneighbor sampling. Any prior knowledge can now be embedded into that metric\nspace independently from the employed DA. From its simplicity, GPS-SSL is\napplicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is\nin reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches\n85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We\ntherefore move a step forward towards the goal of making SSL less reliant on\nDA. We also show that even when using strong DAs, GPS-SSL outperforms the\nbaselines on under-studied domains. We evaluate GPS-SSL along with multiple\nbaseline SSL methods on numerous downstream datasets from different domains\nwhen the models use strong or minimal data augmentations. We hope that GPS-SSL\nwill open new avenues in studying how to inject a priori knowledge into SSL in\na principled manner.", "published": "2024-01-03", "categories": ["cs.CV", "cs.AI", "cs.LG"], "links": "http://arxiv.org/abs/2401.01990v1"}
{"title": "AUPIMO: Redefining Visual Anomaly Detection Benchmarks with High Speed and Low Tolerance", "author": "Joao P. C. Bertoldo, Dick Ameln, Ashwin Vaidya, Samet Akçay", "abstract": "Recent advances in visual anomaly detection research have seen AUROC and\nAUPRO scores on public benchmark datasets such as MVTec and VisA converge\ntowards perfect recall, giving the impression that these benchmarks are\nnear-solved. However, high AUROC and AUPRO scores do not always reflect\nqualitative performance, which limits the validity of these metrics in\nreal-world applications. We argue that the artificial ceiling imposed by the\nlack of an adequate evaluation metric restrains progression of the field, and\nit is crucial that we revisit the evaluation metrics used to rate our\nalgorithms. In response, we introduce Per-IMage Overlap (PIMO), a novel metric\nthat addresses the shortcomings of AUROC and AUPRO. PIMO retains the\nrecall-based nature of the existing metrics but introduces two distinctions:\nthe assignment of curves (and respective area under the curve) is per-image,\nand its X-axis relies solely on normal images. Measuring recall per image\nsimplifies instance score indexing and is more robust to noisy annotations. As\nwe show, it also accelerates computation and enables the usage of statistical\ntests to compare models. By imposing low tolerance for false positives on\nnormal images, PIMO provides an enhanced model validation procedure and\nhighlights performance variations across datasets. Our experiments demonstrate\nthat PIMO offers practical advantages and nuanced performance insights that\nredefine anomaly detection benchmarks -- notably challenging the perception\nthat MVTec AD and VisA datasets have been solved by contemporary models.\nAvailable on GitHub: https://github.com/jpcbertoldo/aupimo.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01984v1"}
{"title": "Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers", "author": "Aleksandar Stanić, Sergi Caelles, Michael Tschannen", "abstract": "Visual reasoning is dominated by end-to-end neural networks scaled to\nbillions of model parameters and training examples. However, even the largest\nmodels struggle with compositional reasoning, generalization, fine-grained\nspatial and temporal reasoning, and counting. Visual reasoning with large\nlanguage models (LLMs) as controllers can, in principle, address these\nlimitations by decomposing the task and solving subtasks by orchestrating a set\nof (visual) tools. Recently, these models achieved great performance on tasks\nsuch as compositional visual question answering, visual grounding, and video\ntemporal reasoning. Nevertheless, in their current form, these models heavily\nrely on human engineering of in-context examples in the prompt, which are often\ndataset- and task-specific and require significant labor by highly skilled\nprogrammers. In this work, we present a framework that mitigates these issues\nby introducing spatially and temporally abstract routines and by leveraging a\nsmall number of labeled examples to automatically generate in-context examples,\nthereby avoiding human-created in-context examples. On a number of visual\nreasoning tasks, we show that our framework leads to consistent gains in\nperformance, makes LLMs as controllers setup more robust, and removes the need\nfor human engineering of in-context examples.", "published": "2024-01-03", "categories": ["cs.CV", "cs.AI", "cs.LG"], "links": "http://arxiv.org/abs/2401.01974v1"}
{"title": "FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding", "author": "Xingxing Zuo, Pouya Samangouei, Yunwen Zhou, Yan Di, Mingyang Li", "abstract": "Precisely perceiving the geometric and semantic properties of real-world 3D\nobjects is crucial for the continued evolution of augmented reality and robotic\napplications. To this end, we present \\algfull{} (\\algname{}), which\nincorporates vision-language embeddings of foundation models into 3D Gaussian\nSplatting (GS). The key contribution of this work is an efficient method to\nreconstruct and represent 3D vision-language models. This is achieved by\ndistilling feature maps generated from image-based foundation models into those\nrendered from our 3D model. To ensure high-quality rendering and fast training,\nwe introduce a novel scene representation by integrating strengths from both GS\nand multi-resolution hash encodings (MHE). Our effective training procedure\nalso introduces a pixel alignment loss that makes the rendered feature distance\nof same semantic entities close, following the pixel-level semantic boundaries.\nOur results demonstrate remarkable multi-view semantic consistency,\nfacilitating diverse downstream tasks, beating state-of-the-art methods by\n$\\mathbf{10.2}$ percent on open-vocabulary language-based object detection,\ndespite that we are $\\mathbf{851\\times}$ faster for inference. This research\nexplores the intersection of vision, language, and 3D scene representation,\npaving the way for enhanced scene understanding in uncontrolled real-world\nenvironments. We plan to release the code upon paper acceptance.", "published": "2024-01-03", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.01970v1"}
{"title": "Instruct-Imagen: Image Generation with Multi-modal Instruction", "author": "Hexiang Hu, Kelvin C. K. Chan, Yu-Chuan Su, Wenhu Chen, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue Ben, Boqing Gong, William Cohen, Ming-Wei Chang, Xuhui Jia", "abstract": "This paper presents instruct-imagen, a model that tackles heterogeneous image\ngeneration tasks and generalizes across unseen tasks. We introduce *multi-modal\ninstruction* for image generation, a task representation articulating a range\nof generation intents with precision. It uses natural language to amalgamate\ndisparate modalities (e.g., text, edge, style, subject, etc.), such that\nabundant generation intents can be standardized in a uniform format.\n  We then build instruct-imagen by fine-tuning a pre-trained text-to-image\ndiffusion model with a two-stage framework. First, we adapt the model using the\nretrieval-augmented training, to enhance model's capabilities to ground its\ngeneration on external multimodal context. Subsequently, we fine-tune the\nadapted model on diverse image generation tasks that requires vision-language\nunderstanding (e.g., subject-driven generation, etc.), each paired with a\nmulti-modal instruction encapsulating the task's essence. Human evaluation on\nvarious image generation datasets reveals that instruct-imagen matches or\nsurpasses prior task-specific models in-domain and demonstrates promising\ngeneralization to unseen and more complex tasks.", "published": "2024-01-03", "categories": ["cs.CV", "cs.AI", "cs.CL"], "links": "http://arxiv.org/abs/2401.01952v1"}
{"title": "Can We Generate Realistic Hands Only Using Convolution?", "author": "Mehran Hosseini, Peyman Hosseini", "abstract": "The enduring inability of image generative models to recreate intricate\ngeometric features, such as those present in human hands and fingers has been\nan ongoing problem in image generation for nearly a decade. While strides have\nbeen made by increasing model sizes and diversifying training datasets, this\nissue remains prevalent across all models, from denoising diffusion models to\nGenerative Adversarial Networks (GAN), pointing to a fundamental shortcoming in\nthe underlying architectures. In this paper, we demonstrate how this problem\ncan be mitigated by augmenting convolution layers geometric capabilities\nthrough providing them with a single input channel incorporating the relative\n$n$-dimensional Cartesian coordinate system. We show that this drastically\nimproves quality of hand and face images generated by GANs and Variational\nAutoEncoders (VAE).", "published": "2024-01-03", "categories": ["cs.CV", "cs.AI", "cs.LG", "51", "I.2.10; I.4.0; I.4.10"], "links": "http://arxiv.org/abs/2401.01951v1"}
{"title": "LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry", "author": "Weirong Chen, Le Chen, Rui Wang, Marc Pollefeys", "abstract": "Visual odometry estimates the motion of a moving camera based on visual\ninput. Existing methods, mostly focusing on two-view point tracking, often\nignore the rich temporal context in the image sequence, thereby overlooking the\nglobal motion patterns and providing no assessment of the full trajectory\nreliability. These shortcomings hinder performance in scenarios with occlusion,\ndynamic objects, and low-texture areas. To address these challenges, we present\nthe Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively\ncombines visual, inter-track, and temporal cues with mindfully selected anchors\nfor dynamic track estimation. Moreover, LEAP's temporal probabilistic\nformulation integrates distribution updates into a learnable iterative\nrefinement module to reason about point-wise uncertainty. Based on these\ntraits, we develop LEAP-VO, a robust visual odometry system adept at handling\nocclusions and dynamic scenes. Our mindful integration showcases a novel\npractice by employing long-term point tracking as the front-end. Extensive\nexperiments demonstrate that the proposed pipeline significantly outperforms\nexisting baselines across various visual odometry benchmarks.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01887v1"}
{"title": "From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations", "author": "Evonne Ng, Javier Romero, Timur Bagautdinov, Shaojie Bai, Trevor Darrell, Angjoo Kanazawa, Alexander Richard", "abstract": "We present a framework for generating full-bodied photorealistic avatars that\ngesture according to the conversational dynamics of a dyadic interaction. Given\nspeech audio, we output multiple possibilities of gestural motion for an\nindividual, including face, body, and hands. The key behind our method is in\ncombining the benefits of sample diversity from vector quantization with the\nhigh-frequency details obtained through diffusion to generate more dynamic,\nexpressive motion. We visualize the generated motion using highly\nphotorealistic avatars that can express crucial nuances in gestures (e.g.\nsneers and smirks). To facilitate this line of research, we introduce a\nfirst-of-its-kind multi-view conversational dataset that allows for\nphotorealistic reconstruction. Experiments show our model generates appropriate\nand diverse gestures, outperforming both diffusion- and VQ-only methods.\nFurthermore, our perceptual evaluation highlights the importance of\nphotorealism (vs. meshes) in accurately assessing subtle motion details in\nconversational gestures. Code and dataset available online.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01885v1"}
{"title": "Step length measurement in the wild using FMCW radar", "author": "Parthipan Siva, Alexander Wong, Patricia Hewston, George Ioannidis, Dr. Jonathan Adachi, Dr. Alexander Rabinovich, Andrea Lee, Alexandra Papaioannou", "abstract": "With an aging population, numerous assistive and monitoring technologies are\nunder development to enable older adults to age in place. To facilitate aging\nin place predicting risk factors such as falls, and hospitalization and\nproviding early interventions are important. Much of the work on ambient\nmonitoring for risk prediction has centered on gait speed analysis, utilizing\nprivacy-preserving sensors like radar. Despite compelling evidence that\nmonitoring step length, in addition to gait speed, is crucial for predicting\nrisk, radar-based methods have not explored step length measurement in the\nhome. Furthermore, laboratory experiments on step length measurement using\nradars are limited to proof of concept studies with few healthy subjects. To\naddress this gap, a radar-based step length measurement system for the home is\nproposed based on detection and tracking using radar point cloud, followed by\nDoppler speed profiling of the torso to obtain step lengths in the home. The\nproposed method was evaluated in a clinical environment, involving 35 frail\nolder adults, to establish its validity. Additionally, the method was assessed\nin people's homes, with 21 frail older adults who had participated in the\nclinical assessment. The proposed radar-based step length measurement method\nwas compared to the gold standard Zeno Walkway Gait Analysis System, revealing\na 4.5cm/8.3% error in a clinical setting. Furthermore, it exhibited excellent\nreliability (ICC(2,k)=0.91, 95% CI 0.82 to 0.96) in uncontrolled home settings.\nThe method also proved accurate in uncontrolled home settings, as indicated by\na strong agreement (ICC(3,k)=0.81 (95% CI 0.53 to 0.92)) between home\nmeasurements and in-clinic assessments.", "published": "2024-01-03", "categories": ["cs.CV", "cs.AI", "I.5.4; C.3; J.7"], "links": "http://arxiv.org/abs/2401.01868v1"}
{"title": "A Vision Check-up for Language Models", "author": "Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, Antonio Torralba", "abstract": "What does learning to model relationships between strings teach large\nlanguage models (LLMs) about the visual world? We systematically evaluate LLMs'\nabilities to generate and recognize an assortment of visual concepts of\nincreasing complexity and then demonstrate how a preliminary visual\nrepresentation learning system can be trained using models of text. As language\nmodels lack the ability to consume or output visual information as pixels, we\nuse code to represent images in our study. Although LLM-generated images do not\nlook like natural images, results on image generation and the ability of models\nto correct these generated images indicate that precise modeling of strings can\nteach language models about numerous aspects of the visual world. Furthermore,\nexperiments on self-supervised visual representation learning, utilizing images\ngenerated with text models, highlight the potential to train vision models\ncapable of making semantic assessments of natural images using just LLMs.", "published": "2024-01-03", "categories": ["cs.CV", "cs.CL", "cs.LG"], "links": "http://arxiv.org/abs/2401.01862v1"}
{"title": "Synthetic dataset of ID and Travel Document", "author": "Carlos Boned, Maxime Talarmain, Nabil Ghanmi, Guillaume Chiron, Sanket Biswas, Ahmad Montaser Awal, Oriol Ramos Terrades", "abstract": "This paper presents a new synthetic dataset of ID and travel documents,\ncalled SIDTD. The SIDTD dataset is created to help training and evaluating\nforged ID documents detection systems. Such a dataset has become a necessity as\nID documents contain personal information and a public dataset of real\ndocuments can not be released. Moreover, forged documents are scarce, compared\nto legit ones, and the way they are generated varies from one fraudster to\nanother resulting in a class of high intra-variability. In this paper we\ntrained state-of-the-art models on this dataset and we compare them to the\nperformance achieved in larger, but private, datasets. The creation of this\ndataset will help to document image analysis community to progress in the task\nof ID document verification.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01858v1"}
{"title": "Frequency Domain Modality-invariant Feature Learning for Visible-infrared Person Re-Identification", "author": "Yulin Li, Tianzhu Zhang, Yongdong Zhang", "abstract": "Visible-infrared person re-identification (VI-ReID) is challenging due to the\nsignificant cross-modality discrepancies between visible and infrared images.\nWhile existing methods have focused on designing complex network architectures\nor using metric learning constraints to learn modality-invariant features, they\noften overlook which specific component of the image causes the modality\ndiscrepancy problem. In this paper, we first reveal that the difference in the\namplitude component of visible and infrared images is the primary factor that\ncauses the modality discrepancy and further propose a novel Frequency Domain\nmodality-invariant feature learning framework (FDMNet) to reduce modality\ndiscrepancy from the frequency domain perspective. Our framework introduces two\nnovel modules, namely the Instance-Adaptive Amplitude Filter (IAF) module and\nthe Phrase-Preserving Normalization (PPNorm) module, to enhance the\nmodality-invariant amplitude component and suppress the modality-specific\ncomponent at both the image- and feature-levels. Extensive experimental results\non two standard benchmarks, SYSU-MM01 and RegDB, demonstrate the superior\nperformance of our FDMNet against state-of-the-art methods.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01839v2"}
{"title": "Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions", "author": "David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, Doyen Sahoo", "abstract": "Most existing video diffusion models (VDMs) are limited to mere text\nconditions. Thereby, they are usually lacking in control over visual appearance\nand geometry structure of the generated videos. This work presents Moonshot, a\nnew video generation model that conditions simultaneously on multimodal inputs\nof image and text. The model builts upon a core module, called multimodal video\nblock (MVB), which consists of conventional spatialtemporal layers for\nrepresenting video features, and a decoupled cross-attention layer to address\nimage and text inputs for appearance conditioning. In addition, we carefully\ndesign the model architecture such that it can optionally integrate with\npre-trained image ControlNet modules for geometry visual conditions, without\nneeding of extra training overhead as opposed to prior methods. Experiments\nshow that with versatile multimodal conditioning mechanisms, Moonshot\ndemonstrates significant improvement on visual quality and temporal consistency\ncompared to existing models. In addition, the model can be easily repurposed\nfor a variety of generative applications, such as personalized video\ngeneration, image animation and video editing, unveiling its potential to serve\nas a fundamental architecture for controllable video generation. Models will be\nmade public on https://github.com/salesforce/LAVIS.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01827v1"}
{"title": "HawkRover: An Autonomous mmWave Vehicular Communication Testbed with Multi-sensor Fusion and Deep Learning", "author": "Ethan Zhu, Haijian Sun, Mingyue Ji", "abstract": "Connected and automated vehicles (CAVs) have become a transformative\ntechnology that can change our daily life. Currently, millimeter-wave (mmWave)\nbands are identified as the promising CAV connectivity solution. While it can\nprovide high data rate, their realization faces many challenges such as high\nattenuation during mmWave signal propagation and mobility management. Existing\nsolution has to initiate pilot signal to measure channel information, then\napply signal processing to calculate the best narrow beam towards the receiver\nend to guarantee sufficient signal power. This process takes significant\noverhead and time, hence not suitable for vehicles. In this study, we propose\nan autonomous and low-cost testbed to collect extensive co-located mmWave\nsignal and other sensors data such as LiDAR (Light Detection and Ranging),\ncameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave\nvehicular communications. Intuitively, these sensors can build a 3D map around\nthe vehicle and signal propagation path can be estimated, eliminating iterative\nthe process via pilot signals. This multimodal data fusion, together with AI,\nis expected to bring significant advances in ``connected'' research.", "published": "2024-01-03", "categories": ["cs.IT", "cs.CV", "math.IT"], "links": "http://arxiv.org/abs/2401.01822v2"}
{"title": "Detours for Navigating Instructional Videos", "author": "Kumar Ashutosh, Zihui Xue, Tushar Nagarajan, Kristen Grauman", "abstract": "We introduce the video detours problem for navigating instructional videos.\nGiven a source video and a natural language query asking to alter the how-to\nvideo's current path of execution in a certain way, the goal is to find a\nrelated ''detour video'' that satisfies the requested alteration. To address\nthis challenge, we propose VidDetours, a novel video-language approach that\nlearns to retrieve the targeted temporal segments from a large repository of\nhow-to's using video-and-text conditioned queries. Furthermore, we devise a\nlanguage-based pipeline that exploits how-to video narration text to create\nweakly supervised training data. We demonstrate our idea applied to the domain\nof how-to cooking videos, where a user can detour from their current recipe to\nfind steps with alternate ingredients, tools, and techniques. Validating on a\nground truth annotated dataset of 16K samples, we show our model's significant\nimprovements over best available methods for video retrieval and question\nanswering, with recall rates exceeding the state of the art by 35%.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01823v1"}
{"title": "aMUSEd: An Open MUSE Reproduction", "author": "Suraj Patil, William Berman, Robin Rombach, Patrick von Platen", "abstract": "We present aMUSEd, an open-source, lightweight masked image model (MIM) for\ntext-to-image generation based on MUSE. With 10 percent of MUSE's parameters,\naMUSEd is focused on fast image generation. We believe MIM is under-explored\ncompared to latent diffusion, the prevailing approach for text-to-image\ngeneration. Compared to latent diffusion, MIM requires fewer inference steps\nand is more interpretable. Additionally, MIM can be fine-tuned to learn\nadditional styles with only a single image. We hope to encourage further\nexploration of MIM by demonstrating its effectiveness on large-scale\ntext-to-image generation and releasing reproducible training code. We also\nrelease checkpoints for two models which directly produce images at 256x256 and\n512x512 resolutions.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01808v1"}
{"title": "Unsupervised Object-Centric Learning from Multiple Unspecified Viewpoints", "author": "Jinyang Yuan, Tonglin Chen, Zhimeng Shen, Bin Li, Xiangyang Xue", "abstract": "Visual scenes are extremely diverse, not only because there are infinite\npossible combinations of objects and backgrounds but also because the\nobservations of the same scene may vary greatly with the change of viewpoints.\nWhen observing a multi-object visual scene from multiple viewpoints, humans can\nperceive the scene compositionally from each viewpoint while achieving the\nso-called ``object constancy'' across different viewpoints, even though the\nexact viewpoints are untold. This ability is essential for humans to identify\nthe same object while moving and to learn from vision efficiently. It is\nintriguing to design models that have a similar ability. In this paper, we\nconsider a novel problem of learning compositional scene representations from\nmultiple unspecified (i.e., unknown and unrelated) viewpoints without using any\nsupervision and propose a deep generative model which separates latent\nrepresentations into a viewpoint-independent part and a viewpoint-dependent\npart to solve this problem. During the inference, latent representations are\nrandomly initialized and iteratively updated by integrating the information in\ndifferent viewpoints with neural networks. Experiments on several specifically\ndesigned synthetic datasets have shown that the proposed method can effectively\nlearn from multiple unspecified viewpoints.", "published": "2024-01-03", "categories": ["cs.CV", "cs.LG"], "links": "http://arxiv.org/abs/2401.01922v1"}
{"title": "VGA: Vision and Graph Fused Attention Network for Rumor Detection", "author": "Lin Bai, Caiyan Jia, Ziying Song, Chaoqun Cui", "abstract": "With the development of social media, rumors have been spread broadly on\nsocial media platforms, causing great harm to society. Beside textual\ninformation, many rumors also use manipulated images or conceal textual\ninformation within images to deceive people and avoid being detected, making\nmultimodal rumor detection be a critical problem. The majority of multimodal\nrumor detection methods mainly concentrate on extracting features of source\nclaims and their corresponding images, while ignoring the comments of rumors\nand their propagation structures. These comments and structures imply the\nwisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these\nmethods usually only extract visual features in a basic manner, seldom consider\ntampering or textual information in images. Therefore, in this study, we\npropose a novel Vision and Graph Fused Attention Network (VGA) for rumor\ndetection to utilize propagation structures among posts so as to obtain the\ncrowd opinions and further explore visual tampering features, as well as the\ntextual information hidden in images. We conduct extensive experiments on three\ndatasets, demonstrating that VGA can effectively detect multimodal rumors and\noutperform state-of-the-art methods significantly.", "published": "2024-01-03", "categories": ["cs.SI", "cs.CL", "cs.CV", "cs.MM"], "links": "http://arxiv.org/abs/2401.01759v1"}
{"title": "FullLoRA-AT: Efficiently Boosting the Robustness of Pretrained Vision Transformers", "author": "Zheng Yuan, Jie Zhang, Shiguang Shan", "abstract": "In recent years, the Vision Transformer (ViT) model has gradually become\nmainstream in various computer vision tasks, and the robustness of the model\nhas received increasing attention. However, existing large models tend to\nprioritize performance during training, potentially neglecting the robustness,\nwhich may lead to serious security concerns. In this paper, we establish a new\nchallenge: exploring how to use a small number of additional parameters for\nadversarial finetuning to quickly and effectively enhance the adversarial\nrobustness of a standardly trained model. To address this challenge, we develop\nthe novel LNLoRA module, incorporating a learnable layer normalization before\nthe conventional LoRA module, which helps mitigate magnitude differences in\nparameters between the adversarial and standard training paradigms.\n  Furthermore, we propose the FullLoRA-AT framework by integrating the\nlearnable LNLoRA modules into all key components of ViT-based models while\nkeeping the pretrained model frozen, which can significantly improve the model\nrobustness via adversarial finetuning in a parameter-efficient manner.\n  Extensive experiments on CIFAR-10, CIFAR-100, and Imagenette demonstrate the\nsuperiority of our proposed FullLoRA-AT framework. It achieves comparable\nrobustness with full finetuning while only requiring about 5% of the learnable\nparameters. This also effectively addresses concerns regarding extra model\nstorage space and enormous training time caused by adversarial finetuning.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01752v1"}
{"title": "Towards Robust Semantic Segmentation against Patch-based Attack via Attention Refinement", "author": "Zheng Yuan, Jie Zhang, Yude Wang, Shiguang Shan, Xilin Chen", "abstract": "The attention mechanism has been proven effective on various visual tasks in\nrecent years. In the semantic segmentation task, the attention mechanism is\napplied in various methods, including the case of both Convolution Neural\nNetworks (CNN) and Vision Transformer (ViT) as backbones. However, we observe\nthat the attention mechanism is vulnerable to patch-based adversarial attacks.\nThrough the analysis of the effective receptive field, we attribute it to the\nfact that the wide receptive field brought by global attention may lead to the\nspread of the adversarial patch. To address this issue, in this paper, we\npropose a Robust Attention Mechanism (RAM) to improve the robustness of the\nsemantic segmentation model, which can notably relieve the vulnerability\nagainst patch-based attacks. Compared to the vallina attention mechanism, RAM\nintroduces two novel modules called Max Attention Suppression and Random\nAttention Dropout, both of which aim to refine the attention matrix and limit\nthe influence of a single adversarial patch on the semantic segmentation\nresults of other positions. Extensive experiments demonstrate the effectiveness\nof our RAM to improve the robustness of semantic segmentation models against\nvarious patch-based attack methods under different attack settings.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01750v1"}
{"title": "Few-shot Image Generation via Information Transfer from the Built Geodesic Surface", "author": "Yuexing Han, Liheng Ruan, Bing Wang", "abstract": "Images generated by most of generative models trained with limited data often\nexhibit deficiencies in either fidelity, diversity, or both. One effective\nsolution to address the limitation is few-shot generative model adaption.\nHowever, the type of approaches typically rely on a large-scale pre-trained\nmodel, serving as a source domain, to facilitate information transfer to the\ntarget domain. In this paper, we propose a method called Information Transfer\nfrom the Built Geodesic Surface (ITBGS), which contains two module: Feature\nAugmentation on Geodesic Surface (FAGS); Interpolation and Regularization\n(I\\&R). With the FAGS module, a pseudo-source domain is created by projecting\nimage features from the training dataset into the Pre-Shape Space, subsequently\ngenerating new features on the Geodesic surface. Thus, no pre-trained models is\nneeded for the adaption process during the training of generative models with\nFAGS. I\\&R module are introduced for supervising the interpolated images and\nregularizing their relative distances, respectively, to further enhance the\nquality of generated images. Through qualitative and quantitative experiments,\nwe demonstrate that the proposed method consistently achieves optimal or\ncomparable results across a diverse range of semantically distinct datasets,\neven in extremely few-shot scenarios.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01749v1"}
{"title": "Few-shot Adaptation of Multi-modal Foundation Models: A Survey", "author": "Fan Liu, Tianshu Zhang, Wenwen Dai, Wenwen Cai, Xiaocong Zhou, Delong Chen", "abstract": "Multi-modal (vision-language) models, such as CLIP, are replacing traditional\nsupervised pre-training models (e.g., ImageNet-based pre-training) as the new\ngeneration of visual foundation models. These models with robust and aligned\nsemantic representations learned from billions of internet image-text pairs and\ncan be applied to various downstream tasks in a zero-shot manner. However, in\nsome fine-grained domains like medical imaging and remote sensing, the\nperformance of multi-modal foundation models often leaves much to be desired.\nConsequently, many researchers have begun to explore few-shot adaptation\nmethods for these models, gradually deriving three main technical approaches:\n1) prompt-based methods, 2) adapter-based methods, and 3) external\nknowledge-based methods. Nevertheless, this rapidly developing field has\nproduced numerous results without a comprehensive survey to systematically\norganize the research progress. Therefore, in this survey, we introduce and\nanalyze the research advancements in few-shot adaptation methods for\nmulti-modal models, summarizing commonly used datasets and experimental setups,\nand comparing the results of different methods. In addition, due to the lack of\nreliable theoretical support for existing methods, we derive the few-shot\nadaptation generalization error bound for multi-modal models. The theorem\nreveals that the generalization error of multi-modal foundation models is\nconstrained by three factors: domain gap, model capacity, and sample size.\nBased on this, we propose three possible solutions from the following aspects:\n1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive\nknowledge utilization.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01736v2"}
{"title": "Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data", "author": "Thomas Lips, Victor-Louis De Gusseme, Francis wyffels", "abstract": "Assistive robots should be able to wash, fold or iron clothes. However, due\nto the variety, deformability and self-occlusions of clothes, creating\ngeneral-purpose robot systems for cloth manipulation is challenging. Synthetic\ndata is a promising direction to improve generalization, though its usability\nis often limited by the sim-to-real gap. To advance the use of synthetic data\nfor cloth manipulation and to enable tasks such as robotic folding, we present\na synthetic data pipeline to train keypoint detectors for almost flattened\ncloth items. To test its performance, we have also collected a real-world\ndataset. We train detectors for both T-shirts, towels and shorts and obtain an\naverage precision of 64.3%. Fine-tuning on real-world data improves performance\nto 74.2%. Additional insight is provided by discussing various failure modes of\nthe keypoint detectors and by comparing different approaches to obtain cloth\nmeshes and materials. We also quantify the remaining sim-to-real gap and argue\nthat further improvements to the fidelity of cloth assets will be required to\nfurther reduce this gap. The code, dataset and trained models are available\nonline.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01734v1"}
{"title": "STAF: 3D Human Mesh Recovery from Video with Spatio-Temporal Alignment Fusion", "author": "Wei Yao, Hongwen Zhang, Yunlian Sun, Jinhui Tang", "abstract": "The recovery of 3D human mesh from monocular images has significantly been\ndeveloped in recent years. However, existing models usually ignore spatial and\ntemporal information, which might lead to mesh and image misalignment and\ntemporal discontinuity. For this reason, we propose a novel Spatio-Temporal\nAlignment Fusion (STAF) model. As a video-based model, it leverages coherence\nclues from human motion by an attention-based Temporal Coherence Fusion Module\n(TCFM). As for spatial mesh-alignment evidence, we extract fine-grained local\ninformation through predicted mesh projection on the feature maps. Based on the\nspatial features, we further introduce a multi-stage adjacent Spatial Alignment\nFusion Module (SAFM) to enhance the feature representation of the target frame.\nIn addition to the above, we propose an Average Pooling Module (APM) to allow\nthe model to focus on the entire input sequence rather than just the target\nframe. This method can remarkably improve the smoothness of recovery results\nfrom video. Extensive experiments on 3DPW, MPII3D, and H36M demonstrate the\nsuperiority of STAF. We achieve a state-of-the-art trade-off between precision\nand smoothness. Our code and more video results are on the project page\nhttps://yw0208.github.io/staf/", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01730v1"}
{"title": "Lightweight Adaptive Feature De-drifting for Compressed Image Classification", "author": "Long Peng, Yang Cao, Yuejin Sun, Yang Wang", "abstract": "JPEG is a widely used compression scheme to efficiently reduce the volume of\ntransmitted images. The artifacts appear among blocks due to the information\nloss, which not only affects the quality of images but also harms the\nsubsequent high-level tasks in terms of feature drifting. High-level vision\nmodels trained on high-quality images will suffer performance degradation when\ndealing with compressed images, especially on mobile devices. Numerous\nlearning-based JPEG artifact removal methods have been proposed to handle\nvisual artifacts. However, it is not an ideal choice to use these JPEG artifact\nremoval methods as a pre-processing for compressed image classification for the\nfollowing reasons: 1. These methods are designed for human vision rather than\nhigh-level vision models; 2. These methods are not efficient enough to serve as\npre-processing on resource-constrained devices. To address these issues, this\npaper proposes a novel lightweight AFD module to boost the performance of\npre-trained image classification models when facing compressed images. First, a\nFDE-Net is devised to generate the spatial-wise FDM in the DCT domain. Next,\nthe estimated FDM is transmitted to the FE-Net to generate the mapping\nrelationship between degraded features and corresponding high-quality features.\nA simple but effective RepConv block equipped with structural\nre-parameterization is utilized in FE-Net, which enriches feature\nrepresentation in the training phase while maintaining efficiency in the\ndeployment phase. After training on limited compressed images, the AFD-Module\ncan serve as a \"plug-and-play\" model for pre-trained classification models to\nimprove their performance on compressed images. Experiments demonstrate that\nour proposed AFD module can comprehensively improve the accuracy of the\npre-trained classification models and significantly outperform the existing\nmethods.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01724v1"}
{"title": "Local Adaptive Clustering Based Image Matching for Automatic Visual Identification", "author": "Zhizhen Wang", "abstract": "Monitoring cameras are extensively utilized in industrial production to\nmonitor equipment running. With advancements in computer vision, device\nrecognition using image features is viable. This paper presents a\nvision-assisted identification system that implements real-time automatic\nequipment labeling through image matching in surveillance videos. The system\ndeploys the ORB algorithm to extract image features and the GMS algorithm to\nremove incorrect matching points. According to the principles of clustering and\ntemplate locality, a method known as Local Adaptive Clustering (LAC) has been\nestablished to enhance label positioning. This method segments matching\ntemplates using the cluster center, which improves the efficiency and stability\nof labels. The experimental results demonstrate that LAC effectively curtails\nthe label drift.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01720v1"}
{"title": "Fact-checking based fake news detection: a review", "author": "Yuzhou Yang, Yangming Zhou, Qichao Ying, Zhenxing Qian, Dan Zeng, Liang Liu", "abstract": "This paper reviews and summarizes the research results on fact-based fake\nnews from the perspectives of tasks and problems, algorithm strategies, and\ndatasets. First, the paper systematically explains the task definition and core\nproblems of fact-based fake news detection. Second, the paper summarizes the\nexisting detection methods based on the algorithm principles. Third, the paper\nanalyzes the classic and newly proposed datasets in the field, and summarizes\nthe experimental results on each dataset. Finally, the paper summarizes the\nadvantages and disadvantages of existing methods, proposes several challenges\nthat methods in this field may face, and looks forward to the next stage of\nresearch. It is hoped that this paper will provide reference for subsequent\nwork in the field.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01717v1"}
{"title": "WordArt Designer API: User-Driven Artistic Typography Synthesis with Large Language Models on ModelScope", "author": "Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Yusen Hu, Xianhui Lin, Xiaoyang Kang, Zengke Jin, Bin Luo, Yifeng Geng, Xuansong Xie, Jingren Zhou", "abstract": "This paper introduces the WordArt Designer API, a novel framework for\nuser-driven artistic typography synthesis utilizing Large Language Models\n(LLMs) on ModelScope. We address the challenge of simplifying artistic\ntypography for non-professionals by offering a dynamic, adaptive, and\ncomputationally efficient alternative to traditional rigid templates. Our\napproach leverages the power of LLMs to understand and interpret user input,\nfacilitating a more intuitive design process. We demonstrate through various\ncase studies how users can articulate their aesthetic preferences and\nfunctional requirements, which the system then translates into unique and\ncreative typographic designs. Our evaluations indicate significant improvements\nin user satisfaction, design flexibility, and creative expression over existing\nsystems. The WordArt Designer API not only democratizes the art of typography\nbut also opens up new possibilities for personalized digital communication and\ndesign.", "published": "2024-01-03", "categories": ["cs.CV", "cs.CL", "cs.MM"], "links": "http://arxiv.org/abs/2401.01699v1"}
{"title": "AID-DTI: Accelerating High-fidelity Diffusion Tensor Imaging with Detail-Preserving Model-based Deep Learning", "author": "Wenxin Fan, Jian Cheng, Cheng Li, Xinrui Ma, Jing Yang, Juan Zou, Ruoyou Wu, Qiegen Liu, Shanshan Wang", "abstract": "Deep learning has shown great potential in accelerating diffusion tensor\nimaging (DTI). Nevertheless, existing methods tend to suffer from Rician noise\nand detail loss in reconstructing the DTI-derived parametric maps especially\nwhen sparsely sampled q-space data are used. This paper proposes a novel\nmethod, AID-DTI (Accelerating hIgh fiDelity Diffusion Tensor Imaging), to\nfacilitate fast and accurate DTI with only six measurements. AID-DTI is\nequipped with a newly designed Singular Value Decomposition (SVD)-based\nregularizer, which can effectively capture fine details while suppressing noise\nduring network training. Experimental results on Human Connectome Project (HCP)\ndata consistently demonstrate that the proposed method estimates DTI parameter\nmaps with fine-grained details and outperforms three state-of-the-art methods\nboth quantitatively and qualitatively.", "published": "2024-01-03", "categories": ["cs.CV", "eess.IV"], "links": "http://arxiv.org/abs/2401.01693v1"}
{"title": "ODTrack: Online Dense Temporal Token Learning for Visual Tracking", "author": "Yaozong Zheng, Bineng Zhong, Qihua Liang, Zhiyi Mo, Shengping Zhang, Xianxian Li", "abstract": "Online contextual reasoning and association across consecutive video frames\nare critical to perceive instances in visual tracking. However, most current\ntop-performing trackers persistently lean on sparse temporal relationships\nbetween reference and search frames via an offline mode. Consequently, they can\nonly interact independently within each image-pair and establish limited\ntemporal correlations. To alleviate the above problem, we propose a simple,\nflexible and effective video-level tracking pipeline, named \\textbf{ODTrack},\nwhich densely associates the contextual relationships of video frames in an\nonline token propagation manner. ODTrack receives video frames of arbitrary\nlength to capture the spatio-temporal trajectory relationships of an instance,\nand compresses the discrimination features (localization information) of a\ntarget into a token sequence to achieve frame-to-frame association. This new\nsolution brings the following benefits: 1) the purified token sequences can\nserve as prompts for the inference in the next video frame, whereby past\ninformation is leveraged to guide future inference; 2) the complex online\nupdate strategies are effectively avoided by the iterative propagation of token\nsequences, and thus we can achieve more efficient model representation and\ncomputation. ODTrack achieves a new \\textit{SOTA} performance on seven\nbenchmarks, while running at real-time speed. Code and models are available at\n\\url{https://github.com/GXNU-ZhongLab/ODTrack}.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01686v1"}
{"title": "Modality Exchange Network for Retinogeniculate Visual Pathway Segmentation", "author": "Hua Han, Cheng Li, Lei Xie, Yuanjing Feng, Alou Diakite, Shanshan Wang", "abstract": "Accurate segmentation of the retinogeniculate visual pathway (RGVP) aids in\nthe diagnosis and treatment of visual disorders by identifying disruptions or\nabnormalities within the pathway. However, the complex anatomical structure and\nconnectivity of RGVP make it challenging to achieve accurate segmentation. In\nthis study, we propose a novel Modality Exchange Network (ME-Net) that\neffectively utilizes multi-modal magnetic resonance (MR) imaging information to\nenhance RGVP segmentation. Our ME-Net has two main contributions. Firstly, we\nintroduce an effective multi-modal soft-exchange technique. Specifically, we\ndesign a channel and spatially mixed attention module to exchange modality\ninformation between T1-weighted and fractional anisotropy MR images. Secondly,\nwe propose a cross-fusion module that further enhances the fusion of\ninformation between the two modalities. Experimental results demonstrate that\nour method outperforms existing state-of-the-art approaches in terms of RGVP\nsegmentation performance.", "published": "2024-01-03", "categories": ["eess.IV", "cs.CV"], "links": "http://arxiv.org/abs/2401.01685v1"}
{"title": "Performance Evaluation of GPS Trajectory Rasterization Methods", "author": "Necip Enes Gengec, Ergin Tari", "abstract": "The availability of the Global Positioning System (GPS) trajectory data is\nincreasing along with the availability of different GPS receivers and with the\nincreasing use of various mobility services. GPS trajectory is an important\ndata source which is used in traffic density detection, transport mode\ndetection, mapping data inferences with the use of different methods such as\nimage processing and machine learning methods. While the data size increases,\nefficient representation of this type of data is becoming difficult to be used\nin these methods. A common approach is the representation of GPS trajectory\ninformation such as average speed, bearing, etc. in raster image form and\napplying analysis methods. In this study, we evaluate GPS trajectory data\nrasterization using the spatial join functions of QGIS, PostGIS+QGIS, and our\niterative spatial structured grid aggregation implementation coded in the\nPython programming language. Our implementation is also parallelizable, and\nthis parallelization is also included as the fourth method. According to the\nresults of experiment carried out with an example GPS trajectory dataset, QGIS\nmethod and PostGIS+QGIS method showed relatively low performance with respect\nto our method using the metric of total processing time. PostGIS+QGIS method\nachieved the best results for spatial join though its total performance\ndecreased quickly while test area size increases. On the other hand, both of\nour methods' performances decrease directly proportional to GPS point. And our\nmethods' performance can be increased proportional to the increase with the\nnumber of processor cores and/or with multiple computing clusters.", "published": "2024-01-03", "categories": ["eess.SP", "cs.CV"], "links": "http://arxiv.org/abs/2401.01676v1"}
{"title": "Transformer RGBT Tracking with Spatio-Temporal Multimodal Tokens", "author": "Dengdi Sun, Yajie Pan, Andong Lu, Chenglong Li, Bin Luo", "abstract": "Many RGBT tracking researches primarily focus on modal fusion design, while\noverlooking the effective handling of target appearance changes. While some\napproaches have introduced historical frames or fuse and replace initial\ntemplates to incorporate temporal information, they have the risk of disrupting\nthe original target appearance and accumulating errors over time. To alleviate\nthese limitations, we propose a novel Transformer RGBT tracking approach, which\nmixes spatio-temporal multimodal tokens from the static multimodal templates\nand multimodal search regions in Transformer to handle target appearance\nchanges, for robust RGBT tracking. We introduce independent dynamic template\ntokens to interact with the search region, embedding temporal information to\naddress appearance changes, while also retaining the involvement of the initial\nstatic template tokens in the joint feature extraction process to ensure the\npreservation of the original reliable target appearance information that\nprevent deviations from the target appearance caused by traditional temporal\nupdates. We also use attention mechanisms to enhance the target features of\nmultimodal template tokens by incorporating supplementary modal cues, and make\nthe multimodal search region tokens interact with multimodal dynamic template\ntokens via attention mechanisms, which facilitates the conveyance of\nmultimodal-enhanced target change information. Our module is inserted into the\ntransformer backbone network and inherits joint feature extraction,\nsearch-template matching, and cross-modal interaction. Extensive experiments on\nthree RGBT benchmark datasets show that the proposed approach maintains\ncompetitive performance compared to other state-of-the-art tracking algorithms\nwhile running at 39.1 FPS.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01674v1"}
{"title": "Simultaneous q-Space Sampling Optimization and Reconstruction for Fast and High-fidelity Diffusion Magnetic Resonance Imaging", "author": "Jing Yang, Jian Cheng, Cheng Li, Wenxin Fan, Juan Zou, Ruoyou Wu, Shanshan Wang", "abstract": "Diffusion Magnetic Resonance Imaging (dMRI) plays a crucial role in the\nnoninvasive investigation of tissue microstructural properties and structural\nconnectivity in the \\textit{in vivo} human brain. However, to effectively\ncapture the intricate characteristics of water diffusion at various directions\nand scales, it is important to employ comprehensive q-space sampling.\nUnfortunately, this requirement leads to long scan times, limiting the clinical\napplicability of dMRI. To address this challenge, we propose SSOR, a\nSimultaneous q-Space sampling Optimization and Reconstruction framework. We\njointly optimize a subset of q-space samples using a continuous representation\nof spherical harmonic functions and a reconstruction network. Additionally, we\nintegrate the unique properties of diffusion magnetic resonance imaging (dMRI)\nin both the q-space and image domains by applying $l1$-norm and total-variation\nregularization. The experiments conducted on HCP data demonstrate that SSOR has\npromising strengths both quantitatively and qualitatively and exhibits\nrobustness to noise.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01662v1"}
{"title": "DiffYOLO: Object Detection for Anti-Noise via YOLO and Diffusion Models", "author": "Yichen Liu, Huajian Zhang, Daqing Gao", "abstract": "Object detection models represented by YOLO series have been widely used and\nhave achieved great results on the high quality datasets, but not all the\nworking conditions are ideal. To settle down the problem of locating targets on\nlow quality datasets, the existing methods either train a new object detection\nnetwork, or need a large collection of low-quality datasets to train. However,\nwe propose a framework in this paper and apply it on the YOLO models called\nDiffYOLO. Specifically, we extract feature maps from the denoising diffusion\nprobabilistic models to enhance the well-trained models, which allows us\nfine-tune YOLO on high-quality datasets and test on low-quality datasets. The\nresults proved this framework can not only prove the performance on noisy\ndatasets, but also prove the detection results on high-quality test datasets.\nWe will supplement more experiments later (with various datasets and network\narchitectures).", "published": "2024-01-03", "categories": ["cs.CV", "68T45", "I.2.10"], "links": "http://arxiv.org/abs/2401.01659v1"}
{"title": "AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI", "author": "Fanda Fan, Chunjie Luo, Jianfeng Zhan, Wanling Gao", "abstract": "The burgeoning field of Artificial Intelligence Generated Content (AIGC) is\nwitnessing rapid advancements, particularly in video generation. This paper\nintroduces AIGCBench, a pioneering comprehensive and scalable benchmark\ndesigned to evaluate a variety of video generation tasks, with a primary focus\non Image-to-Video (I2V) generation. AIGCBench tackles the limitations of\nexisting benchmarks, which suffer from a lack of diverse datasets, by including\na varied and open-domain image-text dataset that evaluates different\nstate-of-the-art algorithms under equivalent conditions. We employ a novel text\ncombiner and GPT-4 to create rich text prompts, which are then used to generate\nimages via advanced Text-to-Image models. To establish a unified evaluation\nframework for video generation tasks, our benchmark includes 11 metrics\nspanning four dimensions to assess algorithm performance. These dimensions are\ncontrol-video alignment, motion effects, temporal consistency, and video\nquality. These metrics are both reference video-dependent and video-free,\nensuring a comprehensive evaluation strategy. The evaluation standard proposed\ncorrelates well with human judgment, providing insights into the strengths and\nweaknesses of current I2V algorithms. The findings from our extensive\nexperiments aim to stimulate further research and development in the I2V field.\nAIGCBench represents a significant step toward creating standardized benchmarks\nfor the broader AIGC landscape, proposing an adaptable and equitable framework\nfor future assessments of video generation tasks.", "published": "2024-01-03", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.01651v1"}
{"title": "De-Confusing Pseudo-Labels in Source-Free Domain Adaptation", "author": "Idit Diamant, Idan Achituve, Arnon Netzer", "abstract": "Source-free domain adaptation (SFDA) aims to transfer knowledge learned from\na source domain to an unlabeled target domain, where the source data is\nunavailable during adaptation. Existing approaches for SFDA focus on\nself-training usually including well-established entropy minimization and\npseudo-labeling techniques. Recent work suggested a co-learning strategy to\nimprove the quality of the generated target pseudo-labels using robust\npretrained networks such as Swin-B. However, since the generated pseudo-labels\ndepend on the source model, they may be noisy due to domain shift. In this\npaper, we view SFDA from the perspective of label noise learning and learn to\nde-confuse the pseudo-labels. More specifically, we learn a noise transition\nmatrix of the pseudo-labels to capture the label corruption of each class and\nlearn the underlying true label distribution. Estimating the noise transition\nmatrix enables a better true class-posterior estimation results with better\nprediction accuracy. We demonstrate the effectiveness of our approach applied\nwith several SFDA methods: SHOT, SHOT++, and AaD. We obtain state-of-the-art\nresults on three domain adaptation datasets: VisDA, DomainNet, and OfficeHome.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01650v1"}
{"title": "SIGNeRF: Scene Integrated Generation for Neural Radiance Fields", "author": "Jan-Niklas Dihlmann, Andreas Engelhardt, Hendrik Lensch", "abstract": "Advances in image diffusion models have recently led to notable improvements\nin the generation of high-quality images. In combination with Neural Radiance\nFields (NeRFs), they enabled new opportunities in 3D generation. However, most\ngenerative 3D approaches are object-centric and applying them to editing\nexisting photorealistic scenes is not trivial. We propose SIGNeRF, a novel\napproach for fast and controllable NeRF scene editing and scene-integrated\nobject generation. A new generative update strategy ensures 3D consistency\nacross the edited images, without requiring iterative optimization. We find\nthat depth-conditioned diffusion models inherently possess the capability to\ngenerate 3D consistent views by requesting a grid of images instead of single\nviews. Based on these insights, we introduce a multi-view reference sheet of\nmodified images. Our method updates an image collection consistently based on\nthe reference sheet and refines the original NeRF with the newly generated\nimage set in one go. By exploiting the depth conditioning mechanism of the\nimage diffusion model, we gain fine control over the spatial location of the\nedit and enforce shape guidance by a selected region or an external mesh.", "published": "2024-01-03", "categories": ["cs.CV", "cs.GR"], "links": "http://arxiv.org/abs/2401.01647v1"}
{"title": "Prototypical Information Bottlenecking and Disentangling for Multimodal Cancer Survival Prediction", "author": "Yilan Zhang, Yingxue Xu, Jianqi Chen, Fengying Xie, Hao Chen", "abstract": "Multimodal learning significantly benefits cancer survival prediction,\nespecially the integration of pathological images and genomic data. Despite\nadvantages of multimodal learning for cancer survival prediction, massive\nredundancy in multimodal data prevents it from extracting discriminative and\ncompact information: (1) An extensive amount of intra-modal task-unrelated\ninformation blurs discriminability, especially for gigapixel whole slide images\n(WSIs) with many patches in pathology and thousands of pathways in genomic\ndata, leading to an ``intra-modal redundancy\" issue. (2) Duplicated information\namong modalities dominates the representation of multimodal data, which makes\nmodality-specific information prone to being ignored, resulting in an\n``inter-modal redundancy\" issue. To address these, we propose a new framework,\nPrototypical Information Bottlenecking and Disentangling (PIBD), consisting of\nPrototypical Information Bottleneck (PIB) module for intra-modal redundancy and\nPrototypical Information Disentanglement (PID) module for inter-modal\nredundancy. Specifically, a variant of information bottleneck, PIB, is proposed\nto model prototypes approximating a bunch of instances for different risk\nlevels, which can be used for selection of discriminative instances within\nmodality. PID module decouples entangled multimodal data into compact distinct\ncomponents: modality-common and modality-specific knowledge, under the guidance\nof the joint prototypical distribution. Extensive experiments on five cancer\nbenchmark datasets demonstrated our superiority over other methods.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01646v1"}
{"title": "S3Net: Innovating Stereo Matching and Semantic Segmentation with a Single-Branch Semantic Stereo Network in Satellite Epipolar Imagery", "author": "Qingyuan Yang, Guanzhou Chen, Xiaoliang Tan, Tong Wang, Jiaqi Wang, Xiaodong Zhang", "abstract": "Stereo matching and semantic segmentation are significant tasks in binocular\nsatellite 3D reconstruction. However, previous studies primarily view these as\nindependent parallel tasks, lacking an integrated multitask learning framework.\nThis work introduces a solution, the Single-branch Semantic Stereo Network\n(S3Net), which innovatively combines semantic segmentation and stereo matching\nusing Self-Fuse and Mutual-Fuse modules. Unlike preceding methods that utilize\nsemantic or disparity information independently, our method dentifies and\nleverages the intrinsic link between these two tasks, leading to a more\naccurate understanding of semantic information and disparity estimation.\nComparative testing on the US3D dataset proves the effectiveness of our S3Net.\nOur model improves the mIoU in semantic segmentation from 61.38 to 67.39, and\nreduces the D1-Error and average endpoint error (EPE) in disparity estimation\nfrom 10.051 to 9.579 and 1.439 to 1.403 respectively, surpassing existing\ncompetitive methods. Our codes are available at:https://github.com/CVEO/S3Net.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01643v1"}
{"title": "BLADE: Box-Level Supervised Amodal Segmentation through Directed Expansion", "author": "Zhaochen Liu, Zhixuan Li, Tingting Jiang", "abstract": "Perceiving the complete shape of occluded objects is essential for human and\nmachine intelligence. While the amodal segmentation task is to predict the\ncomplete mask of partially occluded objects, it is time-consuming and\nlabor-intensive to annotate the pixel-level ground truth amodal masks.\nBox-level supervised amodal segmentation addresses this challenge by relying\nsolely on ground truth bounding boxes and instance classes as supervision,\nthereby alleviating the need for exhaustive pixel-level annotations.\nNevertheless, current box-level methodologies encounter limitations in\ngenerating low-resolution masks and imprecise boundaries, failing to meet the\ndemands of practical real-world applications. We present a novel solution to\ntackle this problem by introducing a directed expansion approach from visible\nmasks to corresponding amodal masks. Our approach involves a hybrid end-to-end\nnetwork based on the overlapping region - the area where different instances\nintersect. Diverse segmentation strategies are applied for overlapping regions\nand non-overlapping regions according to distinct characteristics. To guide the\nexpansion of visible masks, we introduce an elaborately-designed connectivity\nloss for overlapping regions, which leverages correlations with visible masks\nand facilitates accurate amodal segmentation. Experiments are conducted on\nseveral challenging datasets and the results show that our proposed method can\noutperform existing state-of-the-art methods with large margins.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01642v2"}
{"title": "Context-Aware Interaction Network for RGB-T Semantic Segmentation", "author": "Ying Lv, Zhi Liu, Gongyang Li", "abstract": "RGB-T semantic segmentation is a key technique for autonomous driving scenes\nunderstanding. For the existing RGB-T semantic segmentation methods, however,\nthe effective exploration of the complementary relationship between different\nmodalities is not implemented in the information interaction between multiple\nlevels. To address such an issue, the Context-Aware Interaction Network\n(CAINet) is proposed for RGB-T semantic segmentation, which constructs\ninteraction space to exploit auxiliary tasks and global context for explicitly\nguided learning. Specifically, we propose a Context-Aware Complementary\nReasoning (CACR) module aimed at establishing the complementary relationship\nbetween multimodal features with the long-term context in both spatial and\nchannel dimensions. Further, considering the importance of global contextual\nand detailed information, we propose the Global Context Modeling (GCM) module\nand Detail Aggregation (DA) module, and we introduce specific auxiliary\nsupervision to explicitly guide the context interaction and refine the\nsegmentation map. Extensive experiments on two benchmark datasets of MFNet and\nPST900 demonstrate that the proposed CAINet achieves state-of-the-art\nperformance. The code is available at https://github.com/YingLv1106/CAINet.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01624v1"}
{"title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded", "author": "Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su", "abstract": "The recent development on large multimodal models (LMMs), especially\nGPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries\nof multimodal models beyond traditional tasks like image captioning and visual\nquestion answering. In this work, we explore the potential of LMMs like GPT-4V\nas a generalist web agent that can follow natural language instructions to\ncomplete tasks on any given website. We propose SEEACT, a generalist web agent\nthat harnesses the power of LMMs for integrated visual understanding and acting\non the web. We evaluate on the recent MIND2WEB benchmark. In addition to\nstandard offline evaluation on cached websites, we enable a new online\nevaluation setting by developing a tool that allows running web agents on live\nwebsites. We show that GPT-4V presents a great potential for web agents - it\ncan successfully complete 50% of the tasks on live websites if we manually\nground its textual plans into actions on the websites. This substantially\noutperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)\nspecifically fine-tuned for web agents. However, grounding still remains a\nmajor challenge. Existing LMM grounding strategies like set-of-mark prompting\nturns out not effective for web agents, and the best grounding strategy we\ndevelop in this paper leverages both the HTML text and visuals. Yet, there is\nstill a substantial gap with oracle grounding, leaving ample room for further\nimprovement.", "published": "2024-01-03", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV"], "links": "http://arxiv.org/abs/2401.01614v1"}
{"title": "Distilling Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection", "author": "Haowen Zheng, Dong Cao, Jintao Xu, Rui Ai, Weihao Gu, Yang Yang, Yanyan Liang", "abstract": "Striking a balance between precision and efficiency presents a prominent\nchallenge in the bird's-eye-view (BEV) 3D object detection. Although previous\ncamera-based BEV methods achieved remarkable performance by incorporating\nlong-term temporal information, most of them still face the problem of low\nefficiency. One potential solution is knowledge distillation. Existing\ndistillation methods only focus on reconstructing spatial features, while\noverlooking temporal knowledge. To this end, we propose TempDistiller, a\nTemporal knowledge Distiller, to acquire long-term memory from a teacher\ndetector when provided with a limited number of frames. Specifically, a\nreconstruction target is formulated by integrating long-term temporal knowledge\nthrough self-attention operation applied to feature teachers. Subsequently,\nnovel features are generated for masked student features via a generator.\nUltimately, we utilize this reconstruction target to reconstruct the student\nfeatures. In addition, we also explore temporal relational knowledge when\ninputting full frames for the student model. We verify the effectiveness of the\nproposed method on the nuScenes benchmark. The experimental results show our\nmethod obtain an enhancement of +1.6 mAP and +1.1 NDS compared to the baseline,\na speed improvement of approximately 6 FPS after compressing temporal\nknowledge, and the most accurate velocity estimation.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01918v1"}
{"title": "Learning Prompt with Distribution-Based Feature Replay for Few-Shot Class-Incremental Learning", "author": "Zitong Huang, Ze Chen, Zhixing Chen, Erjin Zhou, Xinxing Xu, Rick Siow Mong Goh, Yong Liu, Chunmei Feng, Wangmeng Zuo", "abstract": "Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new\nclasses based on very limited training data without forgetting the old ones\nencountered. Existing studies solely relied on pure visual networks, while in\nthis paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP)\nand propose a simple yet effective framework, named Learning Prompt with\nDistribution-based Feature Replay (LP-DiF). We observe that simply using CLIP\nfor zero-shot evaluation can substantially outperform the most influential\nmethods. Then, prompt tuning technique is involved to further improve its\nadaptation ability, allowing the model to continually capture specific\nknowledge from each session. To prevent the learnable prompt from forgetting\nold knowledge in the new session, we propose a pseudo-feature replay approach.\nSpecifically, we preserve the old knowledge of each class by maintaining a\nfeature-level Gaussian distribution with a diagonal covariance matrix, which is\nestimated by the image features of training images and synthesized features\ngenerated from a VAE. When progressing to a new session, pseudo-features are\nsampled from old-class distributions combined with training images of the\ncurrent session to optimize the prompt, thus enabling the model to learn new\nknowledge while retaining old knowledge. Experiments on three prevalent\nbenchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging\nbenchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the\nsuperiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL. Code is\npublicly available at https://github.com/1170300714/LP-DiF.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01598v1"}
{"title": "MLIP: Medical Language-Image Pre-training with Masked Local Representation Learning", "author": "Jiarun Liu, Hong-Yu Zhou, Cheng Li, Weijian Huang, Hao Yang, Yong Liang, Shanshan Wang", "abstract": "Existing contrastive language-image pre-training aims to learn a joint\nrepresentation by matching abundant image-text pairs. However, the number of\nimage-text pairs in medical datasets is usually orders of magnitude smaller\nthan that in natural datasets. Besides, medical image-text pairs often involve\nnumerous complex fine-grained correspondences. This paper aims to enhance the\ndata efficiency by introducing multiple-to-multiple local relationship modeling\nto capture denser supervisions. More specifically, we propose a Medical\nLanguage-Image Pre-training (MLIP) framework, which exploits the limited\nimage-text medical data more efficiently through patch-sentence matching.\nFurthermore, we introduce a masked contrastive learning strategy with semantic\nintegrity estimation to reduce redundancy in images while preserving the\nunderlying semantics. Our evaluation results show that MLIP outperforms\nprevious work in zero/few-shot classification and few-shot segmentation tasks\nby a large margin.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01591v1"}
{"title": "Real-Time Human Fall Detection using a Lightweight Pose Estimation Technique", "author": "Ekram Alam, Abu Sufian, Paramartha Dutta, Marco Leo", "abstract": "The elderly population is increasing rapidly around the world. There are no\nenough caretakers for them. Use of AI-based in-home medical care systems is\ngaining momentum due to this. Human fall detection is one of the most important\ntasks of medical care system for the aged people. Human fall is a common\nproblem among elderly people. Detection of a fall and providing medical help as\nearly as possible is very important to reduce any further complexity. The\nchances of death and other medical complications can be reduced by detecting\nand providing medical help as early as possible after the fall. There are many\nstate-of-the-art fall detection techniques available these days, but the\nmajority of them need very high computing power. In this paper, we proposed a\nlightweight and fast human fall detection system using pose estimation. We used\n`Movenet' for human joins key-points extraction. Our proposed method can work\nin real-time on any low-computing device with any basic camera. All computation\ncan be processed locally, so there is no problem of privacy of the subject. We\nused two datasets `GMDCSA' and `URFD' for the experiment. We got the\nsensitivity value of 0.9375 and 0.9167 for the dataset `GMDCSA' and `URFD'\nrespectively. The source code and the dataset GMDCSA of our work are available\nonline to access.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01587v1"}
{"title": "Enhancing the medical foundation model with multi-scale and cross-modality feature learning", "author": "Weijian Huang, Cheng Li, Hong-Yu Zhou, Jiarun Liu, Hao Yang, Yong Liang, Shanshan Wang", "abstract": "The development of multi-modal medical foundation models has attracted\nsignificant attention in the field of medicine and healthcare due to their\npromising prospects in various clinical applications. One area of focus in this\nresearch direction is the extractions of features at different scales. While\nprevious studies have explored feature learning at individual scales,\ninvestigation on integrating the diverse scales and modalities of information\nis lacking, which may hinder the potential for mutual reinforcement among these\nfeatures. This paper aims to bridge this gap by proposing a method that\neffectively exploits multi-scale and cross-modality information to enhance the\nperformance of medical foundation models. The proposed method simultaneously\nexploit features at the local, instance, modality and global aspects,\nfacilitating comprehensive representation learning within the models. We\nevaluate the effectiveness of the proposed method on six open-source datasets\nacross different clinical tasks, demonstrating its ability to enhance the\nperformance of medical foundation models.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01583v1"}
{"title": "Context-Guided Spatio-Temporal Video Grounding", "author": "Xin Gu, Heng Fan, Yan Huang, Tiejian Luo, Libo Zhang", "abstract": "Spatio-temporal video grounding (or STVG) task aims at locating a\nspatio-temporal tube for a specific instance given a text query. Despite\nadvancements, current methods easily suffer the distractors or heavy object\nappearance variations in videos due to insufficient object information from the\ntext, leading to degradation. Addressing this, we propose a novel framework,\ncontext-guided STVG (CG-STVG), which mines discriminative instance context for\nobject in videos and applies it as a supplementary guidance for target\nlocalization. The key of CG-STVG lies in two specially designed modules,\nincluding instance context generation (ICG), which focuses on discovering\nvisual context information (in both appearance and motion) of the instance, and\ninstance context refinement (ICR), which aims to improve the instance context\nfrom ICG by eliminating irrelevant or even harmful information from the\ncontext. During grounding, ICG, together with ICR, are deployed at each\ndecoding stage of a Transformer architecture for instance context learning.\nParticularly, instance context learned from one decoding stage is fed to the\nnext stage, and leveraged as a guidance containing rich and discriminative\nobject feature to enhance the target-awareness in decoding feature, which\nconversely benefits generating better new instance context for improving\nlocalization finally. Compared to existing methods, CG-STVG enjoys object\ninformation in text query and guidance from mined instance visual context for\nmore accurate target localization. In our experiments on three benchmarks,\nincluding HCSTVG-v1/-v2 and VidSTG, CG-STVG sets new state-of-the-arts in\nm_tIoU and m_vIoU on all of them, showing its efficacy. The code will be\nreleased at https://github.com/HengLan/CGSTVG.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01578v1"}
{"title": "Test-Time Personalization with Meta Prompt for Gaze Estimation", "author": "Huan Liu, Julia Qi, Zhenhao Li, Mohammad Hassanpour, Yang Wang, Konstantinos Plataniotis, Yuanhao Yu", "abstract": "Despite the recent remarkable achievement in gaze estimation, efficient and\naccurate personalization of gaze estimation without labels is a practical\nproblem but rarely touched on in the literature. To achieve efficient\npersonalization, we take inspiration from the recent advances in Natural\nLanguage Processing (NLP) by updating a negligible number of parameters,\n\"prompts\", at the test time. Specifically, the prompt is additionally attached\nwithout perturbing original network and can contain less than 1% of a\nResNet-18's parameters. Our experiments show high efficiency of the prompt\ntuning approach. The proposed one can be 10 times faster in terms of adaptation\nspeed than the methods compared. However, it is non-trivial to update the\nprompt for personalized gaze estimation without labels. At the test time, it is\nessential to ensure that the minimizing of particular unsupervised loss leads\nto the goals of minimizing gaze estimation error. To address this difficulty,\nwe propose to meta-learn the prompt to ensure that its updates align with the\ngoal. Our experiments show that the meta-learned prompt can be effectively\nadapted even with a simple symmetry loss. In addition, we experiment on four\ncross-dataset validations to show the remarkable advantages of the proposed\nmethod.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01577v1"}
{"title": "Enhancing Generalization of Invisible Facial Privacy Cloak via Gradient Accumulation", "author": "Xuannan Liu, Yaoyao Zhong, Weihong Deng, Hongzhi Shi, Xingchen Cui, Yunfeng Yin, Dongchao Wen", "abstract": "The blooming of social media and face recognition (FR) systems has increased\npeople's concern about privacy and security. A new type of adversarial privacy\ncloak (class-universal) can be applied to all the images of regular users, to\nprevent malicious FR systems from acquiring their identity information. In this\nwork, we discover the optimization dilemma in the existing methods -- the local\noptima problem in large-batch optimization and the gradient information\nelimination problem in small-batch optimization. To solve these problems, we\npropose Gradient Accumulation (GA) to aggregate multiple small-batch gradients\ninto a one-step iterative gradient to enhance the gradient stability and reduce\nthe usage of quantization operations. Experiments show that our proposed method\nachieves high performance on the Privacy-Commons dataset against black-box face\nrecognition models.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01575v1"}
{"title": "A Transformer-Based Adaptive Semantic Aggregation Method for UAV Visual Geo-Localization", "author": "Shishen Li, Cuiwei Liu, Huaijun Qiu, Zhaokui Li", "abstract": "This paper addresses the task of Unmanned Aerial Vehicles (UAV) visual\ngeo-localization, which aims to match images of the same geographic target\ntaken by different platforms, i.e., UAVs and satellites. In general, the key to\nachieving accurate UAV-satellite image matching lies in extracting visual\nfeatures that are robust against viewpoint changes, scale variations, and\nrotations. Current works have shown that part matching is crucial for UAV\nvisual geo-localization since part-level representations can capture image\ndetails and help to understand the semantic information of scenes. However, the\nimportance of preserving semantic characteristics in part-level representations\nis not well discussed. In this paper, we introduce a transformer-based adaptive\nsemantic aggregation method that regards parts as the most representative\nsemantics in an image. Correlations of image patches to different parts are\nlearned in terms of the transformer's feature map. Then our method decomposes\npart-level features into an adaptive sum of all patch features. By doing this,\nthe learned parts are encouraged to focus on patches with typical semantics.\nExtensive experiments on the University-1652 dataset have shown the superiority\nof our method over the current works.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01574v1"}
{"title": "View Distribution Alignment with Progressive Adversarial Learning for UAV Visual Geo-Localization", "author": "Cuiwei Liu, Jiahao Liu, Huaijun Qiu, Zhaokui Li, Xiangbin Shi", "abstract": "Unmanned Aerial Vehicle (UAV) visual geo-localization aims to match images of\nthe same geographic target captured from different views, i.e., the UAV view\nand the satellite view. It is very challenging due to the large appearance\ndifferences in UAV-satellite image pairs. Previous works map images captured by\nUAVs and satellites to a shared feature space and employ a classification\nframework to learn location-dependent features while neglecting the overall\ndistribution shift between the UAV view and the satellite view. In this paper,\nwe address these limitations by introducing distribution alignment of the two\nviews to shorten their distance in a common space. Specifically, we propose an\nend-to-end network, called PVDA (Progressive View Distribution Alignment).\nDuring training, feature encoder, location classifier, and view discriminator\nare jointly optimized by a novel progressive adversarial learning strategy.\nCompetition between feature encoder and view discriminator prompts both of them\nto be stronger. It turns out that the adversarial learning is progressively\nemphasized until UAV-view images are indistinguishable from satellite-view\nimages. As a result, the proposed PVDA becomes powerful in learning\nlocation-dependent yet view-invariant features with good scalability towards\nunseen images of new locations. Compared to the state-of-the-art methods, the\nproposed PVDA requires less inference time but has achieved superior\nperformance on the University-1652 dataset.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01573v1"}
{"title": "AttentionLut: Attention Fusion-based Canonical Polyadic LUT for Real-time Image Enhancement", "author": "Kang Fu, Yicong Peng, Zicheng Zhang, Qihang Xu, Xiaohong Liu, Jia Wang, Guangtao Zhai", "abstract": "Recently, many algorithms have employed image-adaptive lookup tables (LUTs)\nto achieve real-time image enhancement. Nonetheless, a prevailing trend among\nexisting methods has been the employment of linear combinations of basic LUTs\nto formulate image-adaptive LUTs, which limits the generalization ability of\nthese methods. To address this limitation, we propose a novel framework named\nAttentionLut for real-time image enhancement, which utilizes the attention\nmechanism to generate image-adaptive LUTs. Our proposed framework consists of\nthree lightweight modules. We begin by employing the global image context\nfeature module to extract image-adaptive features. Subsequently, the attention\nfusion module integrates the image feature with the priori attention feature\nobtained during training to generate image-adaptive canonical polyadic tensors.\nFinally, the canonical polyadic reconstruction module is deployed to\nreconstruct image-adaptive residual 3DLUT, which is subsequently utilized for\nenhancing input images. Experiments on the benchmark MIT-Adobe FiveK dataset\ndemonstrate that the proposed method achieves better enhancement performance\nquantitatively and qualitatively than the state-of-the-art methods.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01569v1"}
{"title": "One-Step Late Fusion Multi-view Clustering with Compressed Subspace", "author": "Qiyuan Ou, Pei Zhang, Sihang Zhou, En Zhu", "abstract": "Late fusion multi-view clustering (LFMVC) has become a rapidly growing class\nof methods in the multi-view clustering (MVC) field, owing to its excellent\ncomputational speed and clustering performance. One bottleneck faced by\nexisting late fusion methods is that they are usually aligned to the average\nkernel function, which makes the clustering performance highly dependent on the\nquality of datasets. Another problem is that they require subsequent k-means\nclustering after obtaining the consensus partition matrix to get the final\ndiscrete labels, and the resulting separation of the label learning and cluster\nstructure optimization processes limits the integrity of these models. To\naddress the above issues, we propose an integrated framework named One-Step\nLate Fusion Multi-view Clustering with Compressed Subspace (OS-LFMVC-CS).\nSpecifically, we use the consensus subspace to align the partition matrix while\noptimizing the partition fusion, and utilize the fused partition matrix to\nguide the learning of discrete labels. A six-step iterative optimization\napproach with verified convergence is proposed. Sufficient experiments on\nmultiple datasets validate the effectiveness and efficiency of our proposed\nmethod.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01558v1"}
{"title": "Multi-modal Learning with Missing Modality in Predicting Axillary Lymph Node Metastasis", "author": "Shichuan Zhang, Sunyi Zheng, Zhongyi Shui, Honglin Li, Lin Yang", "abstract": "Multi-modal Learning has attracted widespread attention in medical image\nanalysis. Using multi-modal data, whole slide images (WSIs) and clinical\ninformation, can improve the performance of deep learning models in the\ndiagnosis of axillary lymph node metastasis. However, clinical information is\nnot easy to collect in clinical practice due to privacy concerns, limited\nresources, lack of interoperability, etc. Although patient selection can ensure\nthe training set to have multi-modal data for model development, missing\nmodality of clinical information can appear during test. This normally leads to\nperformance degradation, which limits the use of multi-modal models in the\nclinic. To alleviate this problem, we propose a bidirectional distillation\nframework consisting of a multi-modal branch and a single-modal branch. The\nsingle-modal branch acquires the complete multi-modal knowledge from the\nmulti-modal branch, while the multi-modal learns the robust features of WSI\nfrom the single-modal. We conduct experiments on a public dataset of Lymph Node\nMetastasis in Early Breast Cancer to validate the method. Our approach not only\nachieves state-of-the-art performance with an AUC of 0.861 on the test set\nwithout missing data, but also yields an AUC of 0.842 when the rate of missing\nmodality is 80\\%. This shows the effectiveness of the approach in dealing with\nmulti-modal data and missing modality. Such a model has the potential to\nimprove treatment decision-making for early breast cancer patients who have\naxillary lymph node metastatic status.", "published": "2024-01-03", "categories": ["eess.IV", "cs.CV"], "links": "http://arxiv.org/abs/2401.01553v1"}
{"title": "CRA-PCN: Point Cloud Completion with Intra- and Inter-level Cross-Resolution Transformers", "author": "Yi Rong, Haoran Zhou, Lixin Yuan, Cheng Mei, Jiahao Wang, Tong Lu", "abstract": "Point cloud completion is an indispensable task for recovering complete point\nclouds due to incompleteness caused by occlusion, limited sensor resolution,\netc. The family of coarse-to-fine generation architectures has recently\nexhibited great success in point cloud completion and gradually became\nmainstream. In this work, we unveil one of the key ingredients behind these\nmethods: meticulously devised feature extraction operations with explicit\ncross-resolution aggregation. We present Cross-Resolution Transformer that\nefficiently performs cross-resolution aggregation with local attention\nmechanisms. With the help of our recursive designs, the proposed operation can\ncapture more scales of features than common aggregation operations, which is\nbeneficial for capturing fine geometric characteristics. While prior\nmethodologies have ventured into various manifestations of inter-level\ncross-resolution aggregation, the effectiveness of intra-level one and their\ncombination has not been analyzed. With unified designs, Cross-Resolution\nTransformer can perform intra- or inter-level cross-resolution aggregation by\nswitching inputs. We integrate two forms of Cross-Resolution Transformers into\none up-sampling block for point generation, and following the coarse-to-fine\nmanner, we construct CRA-PCN to incrementally predict complete shapes with\nstacked up-sampling blocks. Extensive experiments demonstrate that our method\noutperforms state-of-the-art methods by a large margin on several widely used\nbenchmarks. Codes are available at https://github.com/EasyRy/CRA-PCN.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01552v1"}
{"title": "Boosting of Implicit Neural Representation-based Image Denoiser", "author": "Zipei Yan, Zhengji Liu, Jizhou Li", "abstract": "Implicit Neural Representation (INR) has emerged as an effective method for\nunsupervised image denoising. However, INR models are typically\noverparameterized; consequently, these models are prone to overfitting during\nlearning, resulting in suboptimal results, even noisy ones. To tackle this\nproblem, we propose a general recipe for regularizing INR models in image\ndenoising. In detail, we propose to iteratively substitute the supervision\nsignal with the mean value derived from both the prediction and supervision\nsignal during the learning process. We theoretically prove that such a simple\niterative substitute can gradually enhance the signal-to-noise ratio of the\nsupervision signal, thereby benefiting INR models during the learning process.\nOur experimental results demonstrate that INR models can be effectively\nregularized by the proposed approach, relieving overfitting and boosting image\ndenoising performance.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01548v1"}
{"title": "DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM with Joint Semantic Encoding", "author": "Mingrui Li, Jiaming He, Guangan Jiang, Hongyu Wang", "abstract": "We propose DDN-SLAM, a real-time dense neural implicit semantic SLAM system\ndesigned for dynamic scenes. While existing neural implicit SLAM systems\nperform well in static scenes, they often encounter challenges in real-world\nenvironments with dynamic interferences, leading to ineffective tracking and\nmapping. DDN-SLAM utilizes the priors provided by the deep semantic system,\ncombined with conditional probability fields, for segmentation.By constructing\ndepth-guided static masks and employing joint multi-resolution hashing\nencoding, we ensure fast hole filling and high-quality mapping while mitigating\nthe effects of dynamic information interference. To enhance tracking\nrobustness, we utilize sparse feature points validated with optical flow and\nkeyframes, enabling loop closure detection and global bundle optimization.\nFurthermore, DDN-SLAM supports monocular, stereo, and RGB-D inputs, operating\nrobustly at a frequency of 20-30Hz. Extensive experiments on 6 virtual/real\ndatasets demonstrate that our method outperforms state-of-the-art approaches in\nboth dynamic and static scenes.", "published": "2024-01-03", "categories": ["cs.CV", "cs.RO"], "links": "http://arxiv.org/abs/2401.01545v1"}
{"title": "Collaborative Perception for Connected and Autonomous Driving: Challenges, Possible Solutions and Opportunities", "author": "Senkang Hu, Zhengru Fang, Yiqin Deng, Xianhao Chen, Yuguang Fang", "abstract": "Autonomous driving has attracted significant attention from both academia and\nindustries, which is expected to offer a safer and more efficient driving\nsystem. However, current autonomous driving systems are mostly based on a\nsingle vehicle, which has significant limitations which still poses threats to\ndriving safety. Collaborative perception with connected and autonomous vehicles\n(CAVs) shows a promising solution to overcoming these limitations. In this\narticle, we first identify the challenges of collaborative perception, such as\ndata sharing asynchrony, data volume, and pose errors. Then, we discuss the\npossible solutions to address these challenges with various technologies, where\nthe research opportunities are also elaborated. Furthermore, we propose a\nscheme to deal with communication efficiency and latency problems, which is a\nchannel-aware collaborative perception framework to dynamically adjust the\ncommunication graph and minimize latency, thereby improving perception\nperformance while increasing communication efficiency. Finally, we conduct\nexperiments to demonstrate the effectiveness of our proposed scheme.", "published": "2024-01-03", "categories": ["cs.CV", "eess.SP"], "links": "http://arxiv.org/abs/2401.01544v1"}
{"title": "Retraining-free Model Quantization via One-Shot Weight-Coupling Learning", "author": "Chen Tang, Yuan Meng, Jiacheng Jiang, Shuzhao Xie, Rongwei Lu, Xinzhu Ma, Zhi Wang, Wenwu Zhu", "abstract": "Quantization is of significance for compressing the over-parameterized deep\nneural models and deploying them on resource-limited devices. Fixed-precision\nquantization suffers from performance drop due to the limited numerical\nrepresentation ability. Conversely, mixed-precision quantization (MPQ) is\nadvocated to compress the model effectively by allocating heterogeneous\nbit-width for layers. MPQ is typically organized into a searching-retraining\ntwo-stage process. Previous works only focus on determining the optimal\nbit-width configuration in the first stage efficiently, while ignoring the\nconsiderable time costs in the second stage. However, retraining always\nconsumes hundreds of GPU-hours on the cutting-edge GPUs, thus hindering\ndeployment efficiency significantly. In this paper, we devise a one-shot\ntraining-searching paradigm for mixed-precision model compression.\nSpecifically, in the first stage, all potential bit-width configurations are\ncoupled and thus optimized simultaneously within a set of shared weights.\nHowever, our observations reveal a previously unseen and severe bit-width\ninterference phenomenon among highly coupled weights during optimization,\nleading to considerable performance degradation under a high compression ratio.\nTo tackle this problem, we first design a bit-width scheduler to dynamically\nfreeze the most turbulent bit-width of layers during training, to ensure the\nrest bit-widths converged properly. Then, taking inspiration from information\ntheory, we present an information distortion mitigation technique to align the\nbehaviour of the bad-performing bit-widths to the well-performing ones.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01543v1"}
{"title": "DDPM based X-ray Image Synthesizer", "author": "Praveen Mahaulpatha, Thulana Abeywardane, Tomson George", "abstract": "Access to high-quality datasets in the medical industry limits machine\nlearning model performance. To address this issue, we propose a Denoising\nDiffusion Probabilistic Model (DDPM) combined with a UNet architecture for\nX-ray image synthesis. Focused on pneumonia medical condition, our methodology\nemploys over 3000 pneumonia X-ray images obtained from Kaggle for training.\nResults demonstrate the effectiveness of our approach, as the model\nsuccessfully generated realistic images with low Mean Squared Error (MSE). The\nsynthesized images showed distinct differences from non-pneumonia images,\nhighlighting the model's ability to capture key features of positive cases.\nBeyond pneumonia, the applications of this synthesizer extend to various\nmedical conditions, provided an ample dataset is available. The capability to\nproduce high-quality images can potentially enhance machine learning models'\nperformance, aiding in more accurate and efficient medical diagnoses. This\ninnovative DDPM-based X-ray photo synthesizer presents a promising avenue for\naddressing the scarcity of positive medical image datasets, paving the way for\nimproved medical image analysis and diagnosis in the healthcare industry.", "published": "2024-01-03", "categories": ["eess.IV", "cs.CV"], "links": "http://arxiv.org/abs/2401.01539v1"}
{"title": "Glance and Focus: Memory Prompting for Multi-Event Video Question Answering", "author": "Ziyi Bai, Ruiping Wang, Xilin Chen", "abstract": "Video Question Answering (VideoQA) has emerged as a vital tool to evaluate\nagents' ability to understand human daily behaviors. Despite the recent success\nof large vision language models in many multi-modal tasks, complex situation\nreasoning over videos involving multiple human-object interaction events still\nremains challenging. In contrast, humans can easily tackle it by using a series\nof episode memories as anchors to quickly locate question-related key moments\nfor reasoning. To mimic this effective reasoning strategy, we propose the\nGlance-Focus model. One simple way is to apply an action detection model to\npredict a set of actions as key memories. However, these actions within a\nclosed set vocabulary are hard to generalize to various video domains. Instead\nof that, we train an Encoder-Decoder to generate a set of dynamic event\nmemories at the glancing stage. Apart from using supervised bipartite matching\nto obtain the event memories, we further design an unsupervised memory\ngeneration method to get rid of dependence on event annotations. Next, at the\nfocusing stage, these event memories act as a bridge to establish the\ncorrelation between the questions with high-level event concepts and low-level\nlengthy video content. Given the question, the model first focuses on the\ngenerated key event memory, then focuses on the most relevant moment for\nreasoning through our designed multi-level cross-attention mechanism. We\nconduct extensive experiments on four Multi-Event VideoQA benchmarks including\nSTAR, EgoTaskQA, AGQA, and NExT-QA. Our proposed model achieves\nstate-of-the-art results, surpassing current large models in various\nchallenging reasoning tasks. The code and models are available at\nhttps://github.com/ByZ0e/Glance-Focus.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01529v1"}
{"title": "Multimodal self-supervised learning for lesion localization", "author": "Hao Yang, Hong-Yu Zhou, Cheng Li, Weijian Huang, Jiarun Liu, Yong Liang, Shanshan Wang", "abstract": "Multimodal deep learning utilizing imaging and diagnostic reports has made\nimpressive progress in the field of medical imaging diagnostics, demonstrating\na particularly strong capability for auxiliary diagnosis in cases where\nsufficient annotation information is lacking. Nonetheless, localizing diseases\naccurately without detailed positional annotations remains a challenge.\nAlthough existing methods have attempted to utilize local information to\nachieve fine-grained semantic alignment, their capability in extracting the\nfine-grained semantics of the comprehensive contextual within reports is\nlimited. To solve this problem, we introduce a new method that takes full\nsentences from textual reports as the basic units for local semantic alignment.\nOur approach combines chest X-ray images with their corresponding textual\nreports, performing contrastive learning at both global and local levels. The\nleading results obtained by our method on multiple datasets confirm its\nefficacy in the task of lesion localization.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01524v1"}
{"title": "LORE++: Logical Location Regression Network for Table Structure Recognition with Pre-training", "author": "Rujiao Long, Hangdi Xing, Zhibo Yang, Qi Zheng, Zhi Yu, Cong Yao, Fei Huang", "abstract": "Table structure recognition (TSR) aims at extracting tables in images into\nmachine-understandable formats. Recent methods solve this problem by predicting\nthe adjacency relations of detected cell boxes or learning to directly generate\nthe corresponding markup sequences from the table images. However, existing\napproaches either count on additional heuristic rules to recover the table\nstructures, or face challenges in capturing long-range dependencies within\ntables, resulting in increased complexity. In this paper, we propose an\nalternative paradigm. We model TSR as a logical location regression problem and\npropose a new TSR framework called LORE, standing for LOgical location\nREgression network, which for the first time regresses logical location as well\nas spatial location of table cells in a unified network. Our proposed LORE is\nconceptually simpler, easier to train, and more accurate than other paradigms\nof TSR. Moreover, inspired by the persuasive success of pre-trained models on a\nnumber of computer vision and natural language processing tasks, we propose two\npre-training tasks to enrich the spatial and logical representations at the\nfeature level of LORE, resulting in an upgraded version called LORE++. The\nincorporation of pre-training in LORE++ has proven to enjoy significant\nadvantages, leading to a substantial enhancement in terms of accuracy,\ngeneralization, and few-shot capability compared to its predecessor.\nExperiments on standard benchmarks against methods of previous paradigms\ndemonstrate the superiority of LORE++, which highlights the potential and\npromising prospect of the logical location regression paradigm for TSR.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01522v1"}
{"title": "S$^{2}$-DMs:Skip-Step Diffusion Models", "author": "Yixuan Wang, Shuangyin Li", "abstract": "Diffusion models have emerged as powerful generative tools, rivaling GANs in\nsample quality and mirroring the likelihood scores of autoregressive models. A\nsubset of these models, exemplified by DDIMs, exhibit an inherent asymmetry:\nthey are trained over $T$ steps but only sample from a subset of $T$ during\ngeneration. This selective sampling approach, though optimized for speed,\ninadvertently misses out on vital information from the unsampled steps, leading\nto potential compromises in sample quality. To address this issue, we present\nthe S$^{2}$-DMs, which is a new training method by using an innovative\n$L_{skip}$, meticulously designed to reintegrate the information omitted during\nthe selective sampling phase. The benefits of this approach are manifold: it\nnotably enhances sample quality, is exceptionally simple to implement, requires\nminimal code modifications, and is flexible enough to be compatible with\nvarious sampling algorithms. On the CIFAR10 dataset, models trained using our\nalgorithm showed an improvement of 3.27% to 14.06% over models trained with\ntraditional methods across various sampling algorithms (DDIMs, PNDMs, DEIS) and\ndifferent numbers of sampling steps (10, 20, ..., 1000). On the CELEBA dataset,\nthe improvement ranged from 8.97% to 27.08%. Access to the code and additional\nresources is provided in the github.", "published": "2024-01-03", "categories": ["cs.CV", "cs.LG", "eess.IV"], "links": "http://arxiv.org/abs/2401.01520v1"}
{"title": "Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning for Video Question Answering", "author": "Haopeng Li, Qiuhong Ke, Mingming Gong, Tom Drummond", "abstract": "While significant advancements have been made in video question answering\n(VideoQA), the potential benefits of enhancing model generalization through\ntailored difficulty scheduling have been largely overlooked in existing\nresearch. This paper seeks to bridge that gap by incorporating VideoQA into a\ncurriculum learning (CL) framework that progressively trains models from\nsimpler to more complex data. Recognizing that conventional self-paced CL\nmethods rely on training loss for difficulty measurement, which might not\naccurately reflect the intricacies of video-question pairs, we introduce the\nconcept of uncertainty-aware CL. Here, uncertainty serves as the guiding\nprinciple for dynamically adjusting the difficulty. Furthermore, we address the\nchallenge posed by uncertainty by presenting a probabilistic modeling approach\nfor VideoQA. Specifically, we conceptualize VideoQA as a stochastic computation\ngraph, where the hidden representations are treated as stochastic variables.\nThis yields two distinct types of uncertainty: one related to the inherent\nuncertainty in the data and another pertaining to the model's confidence. In\npractice, we seamlessly integrate the VideoQA model into our framework and\nconduct comprehensive experiments. The findings affirm that our approach not\nonly achieves enhanced performance but also effectively quantifies uncertainty\nin the context of VideoQA.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01510v1"}
{"title": "Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports", "author": "Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, Chen Chen", "abstract": "Reasoning over sports videos for question answering is an important task with\nnumerous applications, such as player training and information retrieval.\nHowever, this task has not been explored due to the lack of relevant datasets\nand the challenging nature it presents. Most datasets for video question\nanswering (VideoQA) focus mainly on general and coarse-grained understanding of\ndaily-life videos, which is not applicable to sports scenarios requiring\nprofessional action understanding and fine-grained motion analysis. In this\npaper, we introduce the first dataset, named Sports-QA, specifically designed\nfor the sports VideoQA task. The Sports-QA dataset includes various types of\nquestions, such as descriptions, chronologies, causalities, and counterfactual\nconditions, covering multiple sports. Furthermore, to address the\ncharacteristics of the sports VideoQA task, we propose a new Auto-Focus\nTransformer (AFT) capable of automatically focusing on particular scales of\ntemporal information for question answering. We conduct extensive experiments\non Sports-QA, including baseline studies and the evaluation of different\nmethods. The results demonstrate that our AFT achieves state-of-the-art\nperformance.", "published": "2024-01-03", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01505v1"}
{"title": "From Pixel to Slide image: Polarization Modality-based Pathological Diagnosis Using Representation Learning", "author": "Jia Dong, Yao Yao, Yang Dong, Hui Ma", "abstract": "Thyroid cancer is the most common endocrine malignancy, and accurately\ndistinguishing between benign and malignant thyroid tumors is crucial for\ndeveloping effective treatment plans in clinical practice. Pathologically,\nthyroid tumors pose diagnostic challenges due to improper specimen sampling. In\nthis study, we have designed a three-stage model using representation learning\nto integrate pixel-level and slice-level annotations for distinguishing thyroid\ntumors. This structure includes a pathology structure recognition method to\npredict structures related to thyroid tumors, an encoder-decoder network to\nextract pixel-level annotation information by learning the feature\nrepresentations of image blocks, and an attention-based learning mechanism for\nthe final classification task. This mechanism learns the importance of\ndifferent image blocks in a pathological region, globally considering the\ninformation from each block. In the third stage, all information from the image\nblocks in a region is aggregated using attention mechanisms, followed by\nclassification to determine the category of the region. Experimental results\ndemonstrate that our proposed method can predict microscopic structures more\naccurately. After color-coding, the method achieves results on unstained\npathology slides that approximate the quality of Hematoxylin and eosin\nstaining, reducing the need for stained pathology slides. Furthermore, by\nleveraging the concept of indirect measurement and extracting polarized\nfeatures from structures correlated with lesions, the proposed method can also\nclassify samples where membrane structures cannot be obtained through sampling,\nproviding a potential objective and highly accurate indirect diagnostic\ntechnique for thyroid tumors.", "published": "2024-01-03", "categories": ["eess.IV", "cs.AI", "cs.CV"], "links": "http://arxiv.org/abs/2401.01496v1"}
{"title": "Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition", "author": "Kyle Buettner, Sina Malakouti, Xiang Lorraine Li, Adriana Kovashka", "abstract": "Existing object recognition models have been shown to lack robustness in\ndiverse geographical scenarios due to significant domain shifts in design and\ncontext. Class representations need to be adapted to more accurately reflect an\nobject concept under these shifts. In the absence of training data from target\ngeographies, we hypothesize that geography-specific descriptive knowledge of\nobject categories can be leveraged to enhance robustness. For this purpose, we\nexplore the feasibility of probing a large-language model for\ngeography-specific object knowledge, and we investigate integrating knowledge\nin zero-shot and learnable soft prompting with the CLIP vision-language model.\nIn particular, we propose a geography knowledge regularization method to ensure\nthat soft prompts trained on a source set of geographies generalize to an\nunseen target set of geographies. Our gains on DollarStreet when generalizing\nfrom a model trained only on data from Europe are as large as +2.8 on countries\nfrom Africa, and +4.6 on the hardest classes. We further show competitive\nperformance vs. few-shot target training, and provide insights into how\ndescriptive knowledge captures geographical differences.", "published": "2024-01-03", "categories": ["cs.CV", "cs.AI", "cs.LG"], "links": "http://arxiv.org/abs/2401.01482v1"}
{"title": "Token Propagation Controller for Efficient Vision Transformer", "author": "Wentao Zhu", "abstract": "Vision transformers (ViTs) have achieved promising results on a variety of\nComputer Vision tasks, however their quadratic complexity in the number of\ninput tokens has limited their application specially in resource-constrained\nsettings. Previous approaches that employ gradual token reduction to address\nthis challenge assume that token redundancy in one layer implies redundancy in\nall the following layers. We empirically demonstrate that this assumption is\noften not correct, i.e., tokens that are redundant in one layer can be useful\nin later layers. We employ this key insight to propose a novel token\npropagation controller (TPC) that incorporates two different\ntoken-distributions, i.e., pause probability and restart probability to control\nthe reduction and reuse of tokens respectively, which results in more efficient\ntoken utilization. To improve the estimates of token distributions, we propose\na smoothing mechanism that acts as a regularizer and helps remove noisy\noutliers. Furthermore, to improve the training-stability of our proposed TPC,\nwe introduce a model stabilizer that is able to implicitly encode local image\nstructures and minimize accuracy fluctuations during model training. We present\nextensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT\nand Swin models to demonstrate the effectiveness of our proposed method. For\nexample, compared to baseline models, our proposed method improves the\ninference speed of the DeiT-S by 250% while increasing the classification\naccuracy by 1.0%.", "published": "2024-01-03", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.NE"], "links": "http://arxiv.org/abs/2401.01470v1"}
