{"title": "Efficient Hybrid Zoom using Camera Fusion on Mobile Phones", "author": "Xiaotong Wu, Wei-Sheng Lai, YiChang Shih, Charles Herrmann, Michael Krainin, Deqing Sun, Chia-Kai Liang", "abstract": "DSLR cameras can achieve multiple zoom levels via shifting lens distances or\nswapping lens types. However, these techniques are not possible on smartphone\ndevices due to space constraints. Most smartphone manufacturers adopt a hybrid\nzoom system: commonly a Wide (W) camera at a low zoom level and a Telephoto (T)\ncamera at a high zoom level. To simulate zoom levels between W and T, these\nsystems crop and digitally upsample images from W, leading to significant\ndetail loss. In this paper, we propose an efficient system for hybrid zoom\nsuper-resolution on mobile devices, which captures a synchronous pair of W and\nT shots and leverages machine learning models to align and transfer details\nfrom T to W. We further develop an adaptive blending method that accounts for\ndepth-of-field mismatches, scene occlusion, flow uncertainty, and alignment\nerrors. To minimize the domain gap, we design a dual-phone camera rig to\ncapture real-world inputs and ground-truths for supervised training. Our method\ngenerates a 12-megapixel image in 500ms on a mobile platform and compares\nfavorably against state-of-the-art methods under extensive evaluation on\nreal-world scenarios.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01461v1"}
{"title": "ColorizeDiffusion: Adjustable Sketch Colorization with Reference Image and Text", "author": "Dingkun Yan, Liang Yuan, Yuma Nishioka, Issei Fujishiro, Suguru Saito", "abstract": "Recently, diffusion models have demonstrated their effectiveness in\ngenerating extremely high-quality images and have found wide-ranging\napplications, including automatic sketch colorization. However, most existing\nmodels use text to guide the conditional generation, with fewer attempts\nexploring the potential advantages of using image tokens as conditional inputs\nfor networks. As such, this paper exhaustively investigates image-guided\nmodels, specifically targeting reference-based sketch colorization, which aims\nto colorize sketch images using reference color images. We investigate three\ncritical aspects of reference-based diffusion models: the shortcomings compared\nto text-based counterparts, the training strategies, and the capability in\nzero-shot, sequential text-based manipulation. We introduce two variations of\nan image-guided latent diffusion model using different image tokens from the\npre-trained CLIP image encoder, and we propose corresponding manipulation\nmethods to adjust their results sequentially using weighted text inputs. We\nconduct comprehensive evaluations of our models through qualitative and\nquantitative experiments, as well as a user study.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01456v1"}
{"title": "A Survey on Autonomous Driving Datasets: Data Statistic, Annotation, and Outlook", "author": "Mingyu Liu, Ekim Yurtsever, Xingcheng Zhou, Jonathan Fossaert, Yuning Cui, Bare Luka Zagar, Alois C. Knoll", "abstract": "Autonomous driving has rapidly developed and shown promising performance with\nrecent advances in hardware and deep learning methods. High-quality datasets\nare fundamental for developing reliable autonomous driving algorithms. Previous\ndataset surveys tried to review the datasets but either focused on a limited\nnumber or lacked detailed investigation of the characters of datasets. To this\nend, we present an exhaustive study of over 200 autonomous driving datasets\nfrom multiple perspectives, including sensor modalities, data size, tasks, and\ncontextual conditions. We introduce a novel metric to evaluate the impact of\neach dataset, which can also be a guide for establishing new datasets. We\nfurther analyze the annotation process and quality of datasets. Additionally,\nwe conduct an in-depth analysis of the data distribution of several vital\ndatasets. Finally, we discuss the development trend of the future autonomous\ndriving datasets.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01454v1"}
{"title": "ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label Visual Classification", "author": "Ahmad Sajedi, Samir Khaki, Yuri A. Lawryshyn, Konstantinos N. Plataniotis", "abstract": "Multi-label image classification presents a challenging task in many domains,\nincluding computer vision and medical imaging. Recent advancements have\nintroduced graph-based and transformer-based methods to improve performance and\ncapture label dependencies. However, these methods often include complex\nmodules that entail heavy computation and lack interpretability. In this paper,\nwe propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel\nframework to address these challenges in multi-label image classification\ntasks. Our simple yet effective approach employs supervised contrastive\nlearning, in which samples that share enough labels with an anchor image based\non a decision threshold are introduced as a positive set. This structure\ncaptures label dependencies by pulling positive pair embeddings together and\npushing away negative samples that fall below the threshold. We enhance\nrepresentation learning by incorporating a mixture density network into\ncontrastive learning and generating Gaussian mixture distributions to explore\nthe epistemic uncertainty of the feature encoder. We validate the effectiveness\nof our framework through experimentation with datasets from the computer vision\nand medical imaging domains. Our method outperforms the existing\nstate-of-the-art methods while achieving a low computational footprint on both\ndatasets. Visualization analyses also demonstrate that ProbMCL-learned\nclassifiers maintain a meaningful semantic topology.", "published": "2024-01-02", "categories": ["cs.CV", "cs.LG"], "links": "http://arxiv.org/abs/2401.01448v1"}
{"title": "Indoor Obstacle Discovery on Reflective Ground via Monocular Camera", "author": "Feng Xue, Yicong Chang, Tianxi Wang, Yu Zhou, Anlong Ming", "abstract": "Visual obstacle discovery is a key step towards autonomous navigation of\nindoor mobile robots. Successful solutions have many applications in multiple\nscenes. One of the exceptions is the reflective ground. In this case, the\nreflections on the floor resemble the true world, which confuses the obstacle\ndiscovery and leaves navigation unsuccessful. We argue that the key to this\nproblem lies in obtaining discriminative features for reflections and\nobstacles. Note that obstacle and reflection can be separated by the ground\nplane in 3D space. With this observation, we firstly introduce a\npre-calibration based ground detection scheme that uses robot motion to predict\nthe ground plane. Due to the immunity of robot motion to reflection, this\nscheme avoids failed ground detection caused by reflection. Given the detected\nground, we design a ground-pixel parallax to describe the location of a pixel\nrelative to the ground. Based on this, a unified appearance-geometry feature\nrepresentation is proposed to describe objects inside rectangular boxes.\nEventually, based on segmenting by detection framework, an appearance-geometry\nfusion regressor is designed to utilize the proposed feature to discover the\nobstacles. It also prevents our model from concentrating too much on parts of\nobstacles instead of whole obstacles. For evaluation, we introduce a new\ndataset for Obstacle on Reflective Ground (ORG), which comprises 15 scenes with\nvarious ground reflections, a total of more than 200 image sequences and 3400\nRGB images. The pixel-wise annotations of ground and obstacle provide a\ncomparison to our method and other methods. By reducing the misdetection of the\nreflection, the proposed approach outperforms others. The source code and the\ndataset will be available at\nhttps://github.com/XuefengBUPT/IndoorObstacleDiscovery-RG.", "published": "2024-01-02", "categories": ["cs.CV", "cs.RO"], "links": "http://arxiv.org/abs/2401.01445v1"}
{"title": "Off-Road LiDAR Intensity Based Semantic Segmentation", "author": "Kasi Viswanath, Peng Jiang, Sujit PB, Srikanth Saripalli", "abstract": "LiDAR is used in autonomous driving to provide 3D spatial information and\nenable accurate perception in off-road environments, aiding in obstacle\ndetection, mapping, and path planning. Learning-based LiDAR semantic\nsegmentation utilizes machine learning techniques to automatically classify\nobjects and regions in LiDAR point clouds. Learning-based models struggle in\noff-road environments due to the presence of diverse objects with varying\ncolors, textures, and undefined boundaries, which can lead to difficulties in\naccurately classifying and segmenting objects using traditional geometric-based\nfeatures. In this paper, we address this problem by harnessing the LiDAR\nintensity parameter to enhance object segmentation in off-road environments.\nOur approach was evaluated in the RELLIS-3D data set and yielded promising\nresults as a preliminary analysis with improved mIoU for classes \"puddle\" and\n\"grass\" compared to more complex deep learning-based benchmarks. The\nmethodology was evaluated for compatibility across both Velodyne and Ouster\nLiDAR systems, assuring its cross-platform applicability. This analysis\nadvocates for the incorporation of calibrated intensity as a supplementary\ninput, aiming to enhance the prediction accuracy of learning based semantic\nsegmentation frameworks.\nhttps://github.com/MOONLABIISERB/lidar-intensity-predictor/tree/main", "published": "2024-01-02", "categories": ["cs.CV", "cs.RO"], "links": "http://arxiv.org/abs/2401.01439v1"}
{"title": "SwapTransformer: highway overtaking tactical planner model via imitation learning on OSHA dataset", "author": "Alireza Shamsoshoara, Safin B Salih, Pedram Aghazadeh", "abstract": "This paper investigates the high-level decision-making problem in highway\nscenarios regarding lane changing and over-taking other slower vehicles. In\nparticular, this paper aims to improve the Travel Assist feature for automatic\novertaking and lane changes on highways. About 9 million samples including lane\nimages and other dynamic objects are collected in simulation. This data;\nOvertaking on Simulated HighwAys (OSHA) dataset is released to tackle this\nchallenge. To solve this problem, an architecture called SwapTransformer is\ndesigned and implemented as an imitation learning approach on the OSHA dataset.\nMoreover, auxiliary tasks such as future points and car distance network\npredictions are proposed to aid the model in better understanding the\nsurrounding environment. The performance of the proposed solution is compared\nwith a multi-layer perceptron (MLP) and multi-head self-attention networks as\nbaselines in a simulation environment. We also demonstrate the performance of\nthe model with and without auxiliary tasks. All models are evaluated based on\ndifferent metrics such as time to finish each lap, number of overtakes, and\nspeed difference with speed limit. The evaluation shows that the\nSwapTransformer model outperforms other models in different traffic densities\nin the inference phase.", "published": "2024-01-02", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "links": "http://arxiv.org/abs/2401.01425v1"}
{"title": "Street Gaussians for Modeling Dynamic Urban Scenes", "author": "Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, Sida Peng", "abstract": "This paper aims to tackle the problem of modeling dynamic urban street scenes\nfrom monocular videos. Recent methods extend NeRF by incorporating tracked\nvehicle poses to animate vehicles, enabling photo-realistic view synthesis of\ndynamic urban street scenes. However, significant limitations are their slow\ntraining and rendering speed, coupled with the critical need for high precision\nin tracked vehicle poses. We introduce Street Gaussians, a new explicit scene\nrepresentation that tackles all these limitations. Specifically, the dynamic\nurban street is represented as a set of point clouds equipped with semantic\nlogits and 3D Gaussians, each associated with either a foreground vehicle or\nthe background. To model the dynamics of foreground object vehicles, each\nobject point cloud is optimized with optimizable tracked poses, along with a\ndynamic spherical harmonics model for the dynamic appearance. The explicit\nrepresentation allows easy composition of object vehicles and background, which\nin turn allows for scene editing operations and rendering at 133 FPS\n(1066$\\times$1600 resolution) within half an hour of training. The proposed\nmethod is evaluated on multiple challenging benchmarks, including KITTI and\nWaymo Open datasets. Experiments show that the proposed method consistently\noutperforms state-of-the-art methods across all datasets. Furthermore, the\nproposed representation delivers performance on par with that achieved using\nprecise ground-truth poses, despite relying only on poses from an off-the-shelf\ntracker. The code is available at https://zju3dv.github.io/street_gaussians/.", "published": "2024-01-02", "categories": ["cs.CV", "cs.GR"], "links": "http://arxiv.org/abs/2401.01339v1"}
{"title": "Image Sculpting: Precise Object Editing with 3D Geometry Control", "author": "Jiraphon Yenphraphai, Xichen Pan, Sainan Liu, Daniele Panozzo, Saining Xie", "abstract": "We present Image Sculpting, a new framework for editing 2D images by\nincorporating tools from 3D geometry and graphics. This approach differs\nmarkedly from existing methods, which are confined to 2D spaces and typically\nrely on textual instructions, leading to ambiguity and limited control. Image\nSculpting converts 2D objects into 3D, enabling direct interaction with their\n3D geometry. Post-editing, these objects are re-rendered into 2D, merging into\nthe original image to produce high-fidelity results through a coarse-to-fine\nenhancement process. The framework supports precise, quantifiable, and\nphysically-plausible editing options such as pose editing, rotation,\ntranslation, 3D composition, carving, and serial addition. It marks an initial\nstep towards combining the creative freedom of generative models with the\nprecision of graphics pipelines.", "published": "2024-01-02", "categories": ["cs.GR", "cs.CV"], "links": "http://arxiv.org/abs/2401.01702v1"}
{"title": "Deep autoregressive modeling for land use land cover", "author": "Christopher Krapu, Mark Borsuk, Ryan Calder", "abstract": "Land use / land cover (LULC) modeling is a challenging task due to long-range\ndependencies between geographic features and distinct spatial patterns related\nto topography, ecology, and human development. We identify a close connection\nbetween modeling of spatial patterns of land use and the task of image\ninpainting from computer vision and conduct a study of a modified PixelCNN\narchitecture with approximately 19 million parameters for modeling LULC. In\ncomparison with a benchmark spatial statistical model, we find that the former\nis capable of capturing much richer spatial correlation patterns such as roads\nand water bodies but does not produce a calibrated predictive distribution,\nsuggesting the need for additional tuning. We find evidence of predictive\nunderdispersion with regard to important ecologically-relevant land use\nstatistics such as patch count and adjacency which can be ameliorated to some\nextent by manipulating sampling variability.", "published": "2024-01-02", "categories": ["cs.CV", "cs.LG"], "links": "http://arxiv.org/abs/2401.01395v1"}
{"title": "Integrating Edges into U-Net Models with Explainable Activation Maps for Brain Tumor Segmentation using MR Images", "author": "Subin Sahayam, Umarani Jayaraman", "abstract": "Manual delineation of tumor regions from magnetic resonance (MR) images is\ntime-consuming, requires an expert, and is prone to human error. In recent\nyears, deep learning models have been the go-to approach for the segmentation\nof brain tumors. U-Net and its' variants for semantic segmentation of medical\nimages have achieved good results in the literature. However, U-Net and its'\nvariants tend to over-segment tumor regions and may not accurately segment the\ntumor edges. The edges of the tumor are as important as the tumor regions for\naccurate diagnosis, surgical precision, and treatment planning. In the proposed\nwork, the authors aim to extract edges from the ground truth using a\nderivative-like filter followed by edge reconstruction to obtain an edge ground\ntruth in addition to the brain tumor ground truth. Utilizing both ground\ntruths, the author studies several U-Net and its' variant architectures with\nand without tumor edges ground truth as a target along with the tumor ground\ntruth for brain tumor segmentation. The author used the BraTS2020 benchmark\ndataset to perform the study and the results are tabulated for the dice and\nHausdorff95 metrics. The mean and median metrics are calculated for the whole\ntumor (WT), tumor core (TC), and enhancing tumor (ET) regions. Compared to the\nbaseline U-Net and its variants, the models that learned edges along with the\ntumor regions performed well in core tumor regions in both training and\nvalidation datasets. The improved performance of edge-trained models trained on\nbaseline models like U-Net and V-Net achieved performance similar to baseline\nstate-of-the-art models like Swin U-Net and hybrid MR-U-Net. The edge-target\ntrained models are capable of generating edge maps that can be useful for\ntreatment planning. Additionally, for further explainability of the results,\nthe activation map generated by the hybrid MR-U-Net has been studied.", "published": "2024-01-02", "categories": ["eess.IV", "cs.CV", "cs.LG"], "links": "http://arxiv.org/abs/2401.01303v1"}
{"title": "Physics-informed Generalizable Wireless Channel Modeling with Segmentation and Deep Learning: Fundamentals, Methodologies, and Challenges", "author": "Ethan Zhu, Haijian Sun, Mingyue Ji", "abstract": "Channel modeling is fundamental in advancing wireless systems and has thus\nattracted considerable research focus. Recent trends have seen a growing\nreliance on data-driven techniques to facilitate the modeling process and yield\naccurate channel predictions. In this work, we first provide a concise overview\nof data-driven channel modeling methods, highlighting their limitations.\nSubsequently, we introduce the concept and advantages of physics-informed\nneural network (PINN)-based modeling and a summary of recent contributions in\nthis area. Our findings demonstrate that PINN-based approaches in channel\nmodeling exhibit promising attributes such as generalizability,\ninterpretability, and robustness. We offer a comprehensive architecture for\nPINN methodology, designed to inform and inspire future model development. A\ncase-study of our recent work on precise indoor channel prediction with\nsemantic segmentation and deep learning is presented. The study concludes by\naddressing the challenges faced and suggesting potential research directions in\nthis field.", "published": "2024-01-02", "categories": ["cs.IT", "cs.AI", "cs.CV", "math.IT"], "links": "http://arxiv.org/abs/2401.01288v1"}
{"title": "A Comprehensive Study of Knowledge Editing for Large Language Models", "author": "Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen", "abstract": "Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\nprovide a deeper understanding of the knowledge structures inherent within\nLLMs. Finally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.", "published": "2024-01-02", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "links": "http://arxiv.org/abs/2401.01286v1"}
{"title": "MOC-RVQ: Multilevel Codebook-assisted Digital Generative Semantic Communication", "author": "Yingbin Zhou, Yaping Sun, Guanying Chen, Xiaodong Xu, Hao Chen, Binhong Huang, Shuguang Cui, Ping Zhang", "abstract": "Vector quantization-based image semantic communication systems have\nsuccessfully boosted transmission efficiency, but face a challenge with\nconflicting requirements between codebook design and digital constellation\nmodulation. Traditional codebooks need a wide index range, while modulation\nfavors few discrete states. To address this, we propose a multilevel generative\nsemantic communication system with a two-stage training framework. In the first\nstage, we train a high-quality codebook, using a multi-head octonary codebook\n(MOC) to compress the index range. We also integrate a residual vector\nquantization (RVQ) mechanism for effective multilevel communication. In the\nsecond stage, a noise reduction block (NRB) based on Swin Transformer is\nintroduced, coupled with the multilevel codebook from the first stage, serving\nas a high-quality semantic knowledge base (SKB) for generative feature\nrestoration. Experimental results highlight MOC-RVQ's superior performance over\nmethods like BPG or JPEG, even without channel error correction coding.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01272v1"}
{"title": "VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM", "author": "Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei", "abstract": "The recent innovations and breakthroughs in diffusion models have\nsignificantly expanded the possibilities of generating high-quality videos for\nthe given prompts. Most existing works tackle the single-scene scenario with\nonly one video event occurring in a single background. Extending to generate\nmulti-scene videos nevertheless is not trivial and necessitates to nicely\nmanage the logic in between while preserving the consistent visual appearance\nof key content across video scenes. In this paper, we propose a novel\nframework, namely VideoDrafter, for content-consistent multi-scene video\ngeneration. Technically, VideoDrafter leverages Large Language Models (LLM) to\nconvert the input prompt into comprehensive multi-scene script that benefits\nfrom the logical knowledge learnt by LLM. The script for each scene includes a\nprompt describing the event, the foreground/background entities, as well as\ncamera movement. VideoDrafter identifies the common entities throughout the\nscript and asks LLM to detail each entity. The resultant entity description is\nthen fed into a text-to-image model to generate a reference image for each\nentity. Finally, VideoDrafter outputs a multi-scene video by generating each\nscene video via a diffusion process that takes the reference images, the\ndescriptive prompt of the event and camera movement into account. The diffusion\nmodel incorporates the reference images as the condition and alignment to\nstrengthen the content consistency of multi-scene videos. Extensive experiments\ndemonstrate that VideoDrafter outperforms the SOTA video generation models in\nterms of visual quality, content consistency, and user preference.", "published": "2024-01-02", "categories": ["cs.CV", "cs.CL"], "links": "http://arxiv.org/abs/2401.01256v1"}
{"title": "Deep Learning-Based Computational Model for Disease Identification in Cocoa Pods (Theobroma cacao L.)", "author": "Darlyn Buenaño Vera, Byron Oviedo, Washington Chiriboga Casanova, Cristian Zambrano-Vega", "abstract": "The early identification of diseases in cocoa pods is an important task to\nguarantee the production of high-quality cocoa. The use of artificial\nintelligence techniques such as machine learning, computer vision and deep\nlearning are promising solutions to help identify and classify diseases in\ncocoa pods. In this paper we introduce the development and evaluation of a deep\nlearning computational model applied to the identification of diseases in cocoa\npods, focusing on \"monilia\" and \"black pod\" diseases. An exhaustive review of\nstate-of-the-art of computational models was carried out, based on scientific\narticles related to the identification of plant diseases using computer vision\nand deep learning techniques. As a result of the search, EfficientDet-Lite4, an\nefficient and lightweight model for object detection, was selected. A dataset,\nincluding images of both healthy and diseased cocoa pods, has been utilized to\ntrain the model to detect and pinpoint disease manifestations with considerable\naccuracy. Significant enhancements in the model training and evaluation\ndemonstrate the capability of recognizing and classifying diseases through\nimage analysis. Furthermore, the functionalities of the model were integrated\ninto an Android native mobile with an user-friendly interface, allowing to\nyounger or inexperienced farmers a fast and accuracy identification of health\nstatus of cocoa pods", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01247v1"}
{"title": "Temporal Adaptive RGBT Tracking with Modality Prompt", "author": "Hongyu Wang, Xiaotao Liu, Yifan Li, Meng Sun, Dian Yuan, Jing Liu", "abstract": "RGBT tracking has been widely used in various fields such as robotics,\nsurveillance processing, and autonomous driving. Existing RGBT trackers fully\nexplore the spatial information between the template and the search region and\nlocate the target based on the appearance matching results. However, these RGBT\ntrackers have very limited exploitation of temporal information, either\nignoring temporal information or exploiting it through online sampling and\ntraining. The former struggles to cope with the object state changes, while the\nlatter neglects the correlation between spatial and temporal information. To\nalleviate these limitations, we propose a novel Temporal Adaptive RGBT Tracking\nframework, named as TATrack. TATrack has a spatio-temporal two-stream structure\nand captures temporal information by an online updated template, where the\ntwo-stream structure refers to the multi-modal feature extraction and\ncross-modal interaction for the initial template and the online update template\nrespectively. TATrack contributes to comprehensively exploit spatio-temporal\ninformation and multi-modal information for target localization. In addition,\nwe design a spatio-temporal interaction (STI) mechanism that bridges two\nbranches and enables cross-modal interaction to span longer time scales.\nExtensive experiments on three popular RGBT tracking benchmarks show that our\nmethod achieves state-of-the-art performance, while running at real-time speed.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01244v1"}
{"title": "IdentiFace : A VGG Based Multimodal Facial Biometric System", "author": "Mahmoud Rabea, Hanya Ahmed, Sohaila Mahmoud, Nourhan Sayed", "abstract": "The development of facial biometric systems has contributed greatly to the\ndevelopment of the computer vision field. Nowadays, there's always a need to\ndevelop a multimodal system that combines multiple biometric traits in an\nefficient, meaningful way. In this paper, we introduce \"IdentiFace\" which is a\nmultimodal facial biometric system that combines the core of facial recognition\nwith some of the most important soft biometric traits such as gender, face\nshape, and emotion. We also focused on developing the system using only VGG-16\ninspired architecture with minor changes across different subsystems. This\nunification allows for simpler integration across modalities. It makes it\neasier to interpret the learned features between the tasks which gives a good\nindication about the decision-making process across the facial modalities and\npotential connection. For the recognition problem, we acquired a 99.2% test\naccuracy for five classes with high intra-class variations using data collected\nfrom the FERET database[1]. We achieved 99.4% on our dataset and 95.15% on the\npublic dataset[2] in the gender recognition problem. We were also able to\nachieve a testing accuracy of 88.03% in the face-shape problem using the\ncelebrity face-shape dataset[3]. Finally, we achieved a decent testing accuracy\nof 66.13% in the emotion task which is considered a very acceptable accuracy\ncompared to related work on the FER2013 dataset[4].", "published": "2024-01-02", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.01227v1"}
{"title": "Distribution Matching for Multi-Task Learning of Classification Tasks: a Large-Scale Study on Faces & Beyond", "author": "Dimitrios Kollias, Viktoriia Sharmanska, Stefanos Zafeiriou", "abstract": "Multi-Task Learning (MTL) is a framework, where multiple related tasks are\nlearned jointly and benefit from a shared representation space, or parameter\ntransfer. To provide sufficient learning support, modern MTL uses annotated\ndata with full, or sufficiently large overlap across tasks, i.e., each input\nsample is annotated for all, or most of the tasks. However, collecting such\nannotations is prohibitive in many real applications, and cannot benefit from\ndatasets available for individual tasks. In this work, we challenge this setup\nand show that MTL can be successful with classification tasks with little, or\nnon-overlapping annotations, or when there is big discrepancy in the size of\nlabeled data per task. We explore task-relatedness for co-annotation and\nco-training, and propose a novel approach, where knowledge exchange is enabled\nbetween the tasks via distribution matching. To demonstrate the general\napplicability of our method, we conducted diverse case studies in the domains\nof affective computing, face recognition, species recognition, and shopping\nitem classification using nine datasets. Our large-scale study of affective\ntasks for basic expression recognition and facial action unit detection\nillustrates that our approach is network agnostic and brings large performance\nimprovements compared to the state-of-the-art in both tasks and across all\nstudied databases. In all case studies, we show that co-training via\ntask-relatedness is advantageous and prevents negative transfer (which occurs\nwhen MT model's performance is worse than that of at least one single-task\nmodel).", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01219v2"}
{"title": "Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable Noise", "author": "Qinglong Huang, Yong Liao, Yanbin Hao, Pengyuan Zhou", "abstract": "Neural radiance fields (NeRF) have been proposed as an innovative 3D\nrepresentation method. While attracting lots of attention, NeRF faces critical\nissues such as information confidentiality and security. Steganography is a\ntechnique used to embed information in another object as a means of protecting\ninformation security. Currently, there are few related studies on NeRF\nsteganography, facing challenges in low steganography quality, model weight\ndamage, and a limited amount of steganographic information. This paper proposes\na novel NeRF steganography method based on trainable noise: Noise-NeRF.\nFurthermore, we propose the Adaptive Pixel Selection strategy and Pixel\nPerturbation strategy to improve the steganography quality and efficiency. The\nextensive experiments on open-source datasets show that Noise-NeRF provides\nstate-of-the-art performances in both steganography quality and rendering\nquality, as well as effectiveness in super-resolution image steganography.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01216v1"}
{"title": "YOLO algorithm with hybrid attention feature pyramid network for solder joint defect detection", "author": "Li Ang, Siti Khatijah Nor Abdul Rahim, Raseeda Hamzah, Raihah Aminuddin, Gao Yousheng", "abstract": "Traditional manual detection for solder joint defect is no longer applied\nduring industrial production due to low efficiency, inconsistent evaluation,\nhigh cost and lack of real-time data. A new approach has been proposed to\naddress the issues of low accuracy, high false detection rates and\ncomputational cost of solder joint defect detection in surface mount technology\nof industrial scenarios. The proposed solution is a hybrid attention mechanism\ndesigned specifically for the solder joint defect detection algorithm to\nimprove quality control in the manufacturing process by increasing the accuracy\nwhile reducing the computational cost. The hybrid attention mechanism comprises\na proposed enhanced multi-head self-attention and coordinate attention\nmechanisms increase the ability of attention networks to perceive contextual\ninformation and enhances the utilization range of network features. The\ncoordinate attention mechanism enhances the connection between different\nchannels and reduces location information loss. The hybrid attention mechanism\nenhances the capability of the network to perceive long-distance position\ninformation and learn local features. The improved algorithm model has good\ndetection ability for solder joint defect detection, with mAP reaching 91.5%,\n4.3% higher than the You Only Look Once version 5 algorithm and better than\nother comparative algorithms. Compared to other versions, mean Average\nPrecision, Precision, Recall, and Frame per Seconds indicators have also\nimproved. The improvement of detection accuracy can be achieved while meeting\nreal-time detection requirements.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01214v1"}
{"title": "FGENet: Fine-Grained Extraction Network for Congested Crowd Counting", "author": "Hao-Yuan Ma, Li Zhang, Xiang-Yi Wei", "abstract": "Crowd counting has gained significant popularity due to its practical\napplications. However, mainstream counting methods ignore precise individual\nlocalization and suffer from annotation noise because of counting from\nestimating density maps. Additionally, they also struggle with high-density\nimages.To address these issues, we propose an end-to-end model called\nFine-Grained Extraction Network (FGENet). Different from methods estimating\ndensity maps, FGENet directly learns the original coordinate points that\nrepresent the precise localization of individuals.This study designs a fusion\nmodule, named Fine-Grained Feature Pyramid(FGFP), that is used to fuse feature\nmaps extracted by the backbone of FGENet. The fused features are then passed to\nboth regression and classification heads, where the former provides predicted\npoint coordinates for a given image, and the latter determines the confidence\nlevel for each predicted point being an individual. At the end, FGENet\nestablishes correspondences between prediction points and ground truth points\nby employing the Hungarian algorithm. For training FGENet, we design a robust\nloss function, named Three-Task Combination (TTC), to mitigate the impact of\nannotation noise. Extensive experiments are conducted on four widely used crowd\ncounting datasets. Experimental results demonstrate the effectiveness of\nFGENet. Notably, our method achieves a remarkable improvement of 3.14 points in\nMean Absolute Error (MAE) on the ShanghaiTech Part A dataset, showcasing its\nsuperiority over the existing state-of-the-art methods. Even more impressively,\nFGENet surpasses previous benchmarks on the UCF\\_CC\\_50 dataset with an\nastounding enhancement of 30.16 points in MAE.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01208v1"}
{"title": "Towards a Simultaneous and Granular Identity-Expression Control in Personalized Face Generation", "author": "Renshuai Liu, Bowen Ma, Wei Zhang, Zhipeng Hu, Changjie Fan, Tangjie Lv, Yu Ding, Xuan Cheng", "abstract": "In human-centric content generation, the pre-trained text-to-image models\nstruggle to produce user-wanted portrait images, which retain the identity of\nindividuals while exhibiting diverse expressions. This paper introduces our\nefforts towards personalized face generation. To this end, we propose a novel\nmulti-modal face generation framework, capable of simultaneous\nidentity-expression control and more fine-grained expression synthesis. Our\nexpression control is so sophisticated that it can be specialized by the\nfine-grained emotional vocabulary. We devise a novel diffusion model that can\nundertake the task of simultaneously face swapping and reenactment. Due to the\nentanglement of identity and expression, it's nontrivial to separately and\nprecisely control them in one framework, thus has not been explored yet. To\novercome this, we propose several innovative designs in the conditional\ndiffusion model, including balancing identity and expression encoder, improved\nmidpoint sampling, and explicitly background conditioning. Extensive\nexperiments have demonstrated the controllability and scalability of the\nproposed framework, in comparison with state-of-the-art text-to-image, face\nswapping, and face reenactment methods.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01207v1"}
{"title": "Whole-examination AI estimation of fetal biometrics from 20-week ultrasound scans", "author": "Lorenzo Venturini, Samuel Budd, Alfonso Farruggia, Robert Wright, Jacqueline Matthew, Thomas G. Day, Bernhard Kainz, Reza Razavi, Jo V. Hajnal", "abstract": "The current approach to fetal anomaly screening is based on biometric\nmeasurements derived from individually selected ultrasound images. In this\npaper, we introduce a paradigm shift that attains human-level performance in\nbiometric measurement by aggregating automatically extracted biometrics from\nevery frame across an entire scan, with no need for operator intervention. We\nuse a convolutional neural network to classify each frame of an ultrasound\nvideo recording. We then measure fetal biometrics in every frame where\nappropriate anatomy is visible. We use a Bayesian method to estimate the true\nvalue of each biometric from a large number of measurements and\nprobabilistically reject outliers. We performed a retrospective experiment on\n1457 recordings (comprising 48 million frames) of 20-week ultrasound scans,\nestimated fetal biometrics in those scans and compared our estimates to the\nmeasurements sonographers took during the scan. Our method achieves human-level\nperformance in estimating fetal biometrics and estimates well-calibrated\ncredible intervals in which the true biometric value is expected to lie.", "published": "2024-01-02", "categories": ["cs.CV", "cs.LG", "I.4.7; J.3"], "links": "http://arxiv.org/abs/2401.01201v1"}
{"title": "Skin cancer diagnosis using NIR spectroscopy data of skin lesions in vivo using machine learning algorithms", "author": "Flavio P. Loss, Pedro H. da Cunha, Matheus B. Rocha, Madson Poltronieri Zanoni, Leandro M. de Lima, Isadora Tavares Nascimento, Isabella Rezende, Tania R. P. Canuto, Luciana de Paula Vieira, Renan Rossoni, Maria C. S. Santos, Patricia Lyra Frasson, Wanderson Romão, Paulo R. Filgueiras, Renato A. Krohling", "abstract": "Skin lesions are classified in benign or malignant. Among the malignant,\nmelanoma is a very aggressive cancer and the major cause of deaths. So, early\ndiagnosis of skin cancer is very desired. In the last few years, there is a\ngrowing interest in computer aided diagnostic (CAD) using most image and\nclinical data of the lesion. These sources of information present limitations\ndue to their inability to provide information of the molecular structure of the\nlesion. NIR spectroscopy may provide an alternative source of information to\nautomated CAD of skin lesions. The most commonly used techniques and\nclassification algorithms used in spectroscopy are Principal Component Analysis\n(PCA), Partial Least Squares - Discriminant Analysis (PLS-DA), and Support\nVector Machines (SVM). Nonetheless, there is a growing interest in applying the\nmodern techniques of machine and deep learning (MDL) to spectroscopy. One of\nthe main limitations to apply MDL to spectroscopy is the lack of public\ndatasets. Since there is no public dataset of NIR spectral data to skin\nlesions, as far as we know, an effort has been made and a new dataset named\nNIR-SC-UFES, has been collected, annotated and analyzed generating the\ngold-standard for classification of NIR spectral data to skin cancer. Next, the\nmachine learning algorithms XGBoost, CatBoost, LightGBM, 1D-convolutional\nneural network (1D-CNN) were investigated to classify cancer and non-cancer\nskin lesions. Experimental results indicate the best performance obtained by\nLightGBM with pre-processing using standard normal variate (SNV), feature\nextraction providing values of 0.839 for balanced accuracy, 0.851 for recall,\n0.852 for precision, and 0.850 for F-score. The obtained results indicate the\nfirst steps in CAD of skin lesions aiming the automated triage of patients with\nskin lesions in vivo using NIR spectral data.", "published": "2024-01-02", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.01200v1"}
{"title": "JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial Example", "author": "Benedetta Tondi, Wei Guo, Mauro Barni", "abstract": "Most of the approaches proposed so far to craft targeted adversarial examples\nagainst Deep Learning classifiers are highly suboptimal and typically rely on\nincreasing the likelihood of the target class, thus implicitly focusing on\none-hot encoding settings. In this paper, we propose a more general,\ntheoretically sound, targeted attack that resorts to the minimization of a\nJacobian-induced MAhalanobis distance (JMA) term, taking into account the\neffort (in the input space) required to move the latent space representation of\nthe input sample in a given direction. The minimization is solved by exploiting\nthe Wolfe duality theorem, reducing the problem to the solution of a\nNon-Negative Least Square (NNLS) problem. The proposed algorithm provides an\noptimal solution to a linearized version of the adversarial example problem\noriginally introduced by Szegedy et al. \\cite{szegedy2013intriguing}. The\nexperiments we carried out confirm the generality of the proposed attack which\nis proven to be effective under a wide variety of output encoding schemes.\nNoticeably, the JMA attack is also effective in a multi-label classification\nscenario, being capable to induce a targeted modification of up to half the\nlabels in a complex multilabel classification scenario with 20 labels, a\ncapability that is out of reach of all the attacks proposed so far. As a\nfurther advantage, the JMA attack usually requires very few iterations, thus\nresulting more efficient than existing methods.", "published": "2024-01-02", "categories": ["cs.LG", "cs.AI", "cs.CV"], "links": "http://arxiv.org/abs/2401.01199v1"}
{"title": "Query-Based Knowledge Sharing for Open-Vocabulary Multi-Label Classification", "author": "Xuelin Zhu, Jian Liu, Dongqi Tang, Jiawei Ge, Weijia Liu, Bo Liu, Jiuxin Cao", "abstract": "Identifying labels that did not appear during training, known as multi-label\nzero-shot learning, is a non-trivial task in computer vision. To this end,\nrecent studies have attempted to explore the multi-modal knowledge of\nvision-language pre-training (VLP) models by knowledge distillation, allowing\nto recognize unseen labels in an open-vocabulary manner. However, experimental\nevidence shows that knowledge distillation is suboptimal and provides limited\nperformance gain in unseen label prediction. In this paper, a novel query-based\nknowledge sharing paradigm is proposed to explore the multi-modal knowledge\nfrom the pretrained VLP model for open-vocabulary multi-label classification.\nSpecifically, a set of learnable label-agnostic query tokens is trained to\nextract critical vision knowledge from the input image, and further shared\nacross all labels, allowing them to select tokens of interest as visual clues\nfor recognition. Besides, we propose an effective prompt pool for robust label\nembedding, and reformulate the standard ranking learning into a form of\nclassification to allow the magnitude of feature vectors for matching, which\nboth significantly benefit label recognition. Experimental results show that\nour framework significantly outperforms state-of-the-art methods on zero-shot\ntask by 5.9% and 4.5% in mAP on the NUS-WIDE and Open Images, respectively.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01181v1"}
{"title": "Accurate and Efficient Urban Street Tree Inventory with Deep Learning on Mobile Phone Imagery", "author": "Asim Khan, Umair Nawaz, Anwaar Ulhaq, Iqbal Gondal, Sajid Javed", "abstract": "Deforestation, a major contributor to climate change, poses detrimental\nconsequences such as agricultural sector disruption, global warming, flash\nfloods, and landslides. Conventional approaches to urban street tree inventory\nsuffer from inaccuracies and necessitate specialised equipment. To overcome\nthese challenges, this paper proposes an innovative method that leverages deep\nlearning techniques and mobile phone imaging for urban street tree inventory.\nOur approach utilises a pair of images captured by smartphone cameras to\naccurately segment tree trunks and compute the diameter at breast height (DBH).\nCompared to traditional methods, our approach exhibits several advantages,\nincluding superior accuracy, reduced dependency on specialised equipment, and\napplicability in hard-to-reach areas. We evaluated our method on a\ncomprehensive dataset of 400 trees and achieved a DBH estimation accuracy with\nan error rate of less than 2.5%. Our method holds significant potential for\nsubstantially improving forest management practices. By enhancing the accuracy\nand efficiency of tree inventory, our model empowers urban management to\nmitigate the adverse effects of deforestation and climate change.", "published": "2024-01-02", "categories": ["cs.CV", "cs.AI", "eess.IV"], "links": "http://arxiv.org/abs/2401.01180v1"}
{"title": "Freeze the backbones: A Parameter-Efficient Contrastive Approach to Robust Medical Vision-Language Pre-training", "author": "Jiuming Qin, Che Liu, Sibo Cheng, Yike Guo, Rossella Arcucci", "abstract": "Modern healthcare often utilises radiographic images alongside textual\nreports for diagnostics, encouraging the use of Vision-Language Self-Supervised\nLearning (VL-SSL) with large pre-trained models to learn versatile medical\nvision representations. However, most existing VL-SSL frameworks are trained\nend-to-end, which is computation-heavy and can lose vital prior information\nembedded in pre-trained encoders. To address both issues, we introduce the\nbackbone-agnostic Adaptor framework, which preserves medical knowledge in\npre-trained image and text encoders by keeping them frozen, and employs a\nlightweight Adaptor module for cross-modal learning. Experiments on medical\nimage classification and segmentation tasks across three datasets reveal that\nour framework delivers competitive performance while cutting trainable\nparameters by over 90% compared to current pre-training approaches. Notably,\nwhen fine-tuned with just 1% of data, Adaptor outperforms several\nTransformer-based methods trained on full datasets in medical image\nsegmentation.", "published": "2024-01-02", "categories": ["cs.CV", "cs.AI", "cs.LG"], "links": "http://arxiv.org/abs/2401.01179v1"}
{"title": "GBSS:a global building semantic segmentation dataset for large-scale remote sensing building extraction", "author": "Yuping Hu, Xin Huang, Jiayi Li, Zhen Zhang", "abstract": "Semantic segmentation techniques for extracting building footprints from\nhigh-resolution remote sensing images have been widely used in many fields such\nas urban planning. However, large-scale building extraction demands higher\ndiversity in training samples. In this paper, we construct a Global Building\nSemantic Segmentation (GBSS) dataset (The dataset will be released), which\ncomprises 116.9k pairs of samples (about 742k buildings) from six continents.\nThere are significant variations of building samples in terms of size and\nstyle, so the dataset can be a more challenging benchmark for evaluating the\ngeneralization and robustness of building semantic segmentation models. We\nvalidated through quantitative and qualitative comparisons between different\ndatasets, and further confirmed the potential application in the field of\ntransfer learning by conducting experiments on subsets.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01178v1"}
{"title": "Learning Surface Scattering Parameters From SAR Images Using Differentiable Ray Tracing", "author": "Jiangtao Wei, Yixiang Luomei, Xu Zhang, Feng Xu", "abstract": "Simulating high-resolution Synthetic Aperture Radar (SAR) images in complex\nscenes has consistently presented a significant research challenge. The\ndevelopment of a microwave-domain surface scattering model and its\nreversibility are poised to play a pivotal role in enhancing the authenticity\nof SAR image simulations and facilitating the reconstruction of target\nparameters. Drawing inspiration from the field of computer graphics, this paper\nproposes a surface microwave rendering model that comprehensively considers\nboth Specular and Diffuse contributions. The model is analytically represented\nby the coherent spatially varying bidirectional scattering distribution\nfunction (CSVBSDF) based on the Kirchhoff approximation (KA) and the\nperturbation method (SPM). And SAR imaging is achieved through the synergistic\ncombination of ray tracing and fast mapping projection techniques. Furthermore,\na differentiable ray tracing (DRT) engine based on SAR images was constructed\nfor CSVBSDF surface scattering parameter learning. Within this SAR image\nsimulation engine, the use of differentiable reverse ray tracing enables the\nrapid estimation of parameter gradients from SAR images. The effectiveness of\nthis approach has been validated through simulations and comparisons with real\nSAR images. By learning the surface scattering parameters, substantial\nenhancements in SAR image simulation performance under various observation\nconditions have been demonstrated.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01175v1"}
{"title": "En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data", "author": "Yifang Men, Biwen Lei, Yuan Yao, Miaomiao Cui, Zhouhui Lian, Xuansong Xie", "abstract": "We present En3D, an enhanced generative scheme for sculpting high-quality 3D\nhuman avatars. Unlike previous works that rely on scarce 3D datasets or limited\n2D collections with imbalanced viewing angles and imprecise pose priors, our\napproach aims to develop a zero-shot 3D generative scheme capable of producing\nvisually realistic, geometrically accurate and content-wise diverse 3D humans\nwithout relying on pre-existing 3D or 2D assets. To address this challenge, we\nintroduce a meticulously crafted workflow that implements accurate physical\nmodeling to learn the enhanced 3D generative model from synthetic 2D data.\nDuring inference, we integrate optimization modules to bridge the gap between\nrealistic appearances and coarse 3D shapes. Specifically, En3D comprises three\nmodules: a 3D generator that accurately models generalizable 3D humans with\nrealistic appearance from synthesized balanced, diverse, and structured human\nimages; a geometry sculptor that enhances shape quality using multi-view normal\nconstraints for intricate human anatomy; and a texturing module that\ndisentangles explicit texture maps with fidelity and editability, leveraging\nsemantical UV partitioning and a differentiable rasterizer. Experimental\nresults show that our approach significantly outperforms prior works in terms\nof image quality, geometry accuracy and content diversity. We also showcase the\napplicability of our generated avatars for animation and editing, as well as\nthe scalability of our approach for content-style free adaptation.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01173v1"}
{"title": "Distilling Local Texture Features for Colorectal Tissue Classification in Low Data Regimes", "author": "Dmitry Demidov, Roba Al Majzoub, Amandeep Kumar, Fahad Khan", "abstract": "Multi-class colorectal tissue classification is a challenging problem that is\ntypically addressed in a setting, where it is assumed that ample amounts of\ntraining data is available. However, manual annotation of fine-grained\ncolorectal tissue samples of multiple classes, especially the rare ones like\nstromal tumor and anal cancer is laborious and expensive. To address this, we\npropose a knowledge distillation-based approach, named KD-CTCNet, that\neffectively captures local texture information from few tissue samples, through\na distillation loss, to improve the standard CNN features. The resulting\nenriched feature representation achieves improved classification performance\nspecifically in low data regimes. Extensive experiments on two public datasets\nof colorectal tissues reveal the merits of the proposed contributions, with a\nconsistent gain achieved over different approaches across low data settings.\nThe code and models are publicly available on GitHub.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01164v1"}
{"title": "NU-Class Net: A Novel Deep Learning-based Approach for Video Quality Enhancement", "author": "Parham Zilouchian Moghaddam, Mehdi Modarressi, MohammadAmin Sadeghi", "abstract": "Video content has experienced a surge in popularity, asserting its dominance\nover internet traffic and Internet of Things (IoT) networks. Video compression\nhas long been regarded as the primary means of efficiently managing the\nsubstantial multimedia traffic generated by video-capturing devices.\nNevertheless, video compression algorithms entail significant computational\ndemands in order to achieve substantial compression ratios. This complexity\npresents a formidable challenge when implementing efficient video coding\nstandards in resource-constrained embedded systems, such as IoT edge node\ncameras. To tackle this challenge, this paper introduces NU-Class Net, an\ninnovative deep-learning model designed to mitigate compression artifacts\nstemming from lossy compression codecs. This enhancement significantly elevates\nthe perceptible quality of low-bit-rate videos. By employing the NU-Class Net,\nthe video encoder within the video-capturing node can reduce output quality,\nthereby generating low-bit-rate videos and effectively curtailing both\ncomputation and bandwidth requirements at the edge. On the decoder side, which\nis typically less encumbered by resource limitations, NU-Class Net is applied\nafter the video decoder to compensate for artifacts and approximate the quality\nof the original video. Experimental results affirm the efficacy of the proposed\nmodel in enhancing the perceptible quality of videos, especially those streamed\nat low bit rates.", "published": "2024-01-02", "categories": ["cs.CV", "cs.MM"], "links": "http://arxiv.org/abs/2401.01163v1"}
{"title": "Train-Free Segmentation in MRI with Cubical Persistent Homology", "author": "Anton François, Raphaël Tinarrage", "abstract": "We describe a new general method for segmentation in MRI scans using\nTopological Data Analysis (TDA), offering several advantages over traditional\nmachine learning approaches. It works in three steps, first identifying the\nwhole object to segment via automatic thresholding, then detecting a\ndistinctive subset whose topology is known in advance, and finally deducing the\nvarious components of the segmentation. Although convoking classical ideas of\nTDA, such an algorithm has never been proposed separately from deep learning\nmethods. To achieve this, our approach takes into account, in addition to the\nhomology of the image, the localization of representative cycles, a piece of\ninformation that seems never to have been exploited in this context. In\nparticular, it offers the ability to perform segmentation without the need for\nlarge annotated data sets. TDA also provides a more interpretable and stable\nframework for segmentation by explicitly mapping topological features to\nsegmentation components. By adapting the geometric object to be detected, the\nalgorithm can be adjusted to a wide range of data segmentation challenges. We\ncarefully study the examples of glioblastoma segmentation in brain MRI, where a\nsphere is to be detected, as well as myocardium in cardiac MRI, involving a\ncylinder, and cortical plate detection in fetal brain MRI, whose 2D slices are\ncircles. We compare our method to state-of-the-art algorithms.", "published": "2024-01-02", "categories": ["eess.IV", "cs.CG", "cs.CV", "cs.LG", "55N31, 68-04, 92-08, 68U10"], "links": "http://arxiv.org/abs/2401.01160v1"}
{"title": "On Optimal Sampling for Learning SDF Using MLPs Equipped with Positional Encoding", "author": "Guying Lin, Lei Yang, Yuan Liu, Congyi Zhang, Junhui Hou, Xiaogang Jin, Taku Komura, John Keyser, Wenping Wang", "abstract": "Neural implicit fields, such as the neural signed distance field (SDF) of a\nshape, have emerged as a powerful representation for many applications, e.g.,\nencoding a 3D shape and performing collision detection. Typically, implicit\nfields are encoded by Multi-layer Perceptrons (MLP) with positional encoding\n(PE) to capture high-frequency geometric details. However, a notable side\neffect of such PE-equipped MLPs is the noisy artifacts present in the learned\nimplicit fields. While increasing the sampling rate could in general mitigate\nthese artifacts, in this paper we aim to explain this adverse phenomenon\nthrough the lens of Fourier analysis. We devise a tool to determine the\nappropriate sampling rate for learning an accurate neural implicit field\nwithout undesirable side effects. Specifically, we propose a simple yet\neffective method to estimate the intrinsic frequency of a given network with\nrandomized weights based on the Fourier analysis of the network's responses. It\nis observed that a PE-equipped MLP has an intrinsic frequency much higher than\nthe highest frequency component in the PE layer. Sampling against this\nintrinsic frequency following the Nyquist-Sannon sampling theorem allows us to\ndetermine an appropriate training sampling rate. We empirically show in the\nsetting of SDF fitting that this recommended sampling rate is sufficient to\nsecure accurate fitting results, while further increasing the sampling rate\nwould not further noticeably reduce the fitting error. Training PE-equipped\nMLPs simply with our sampling strategy leads to performances superior to the\nexisting methods.", "published": "2024-01-02", "categories": ["cs.CV", "cs.GR", "cs.LG"], "links": "http://arxiv.org/abs/2401.01391v1"}
{"title": "Hybrid Pooling and Convolutional Network for Improving Accuracy and Training Convergence Speed in Object Detection", "author": "Shiwen Zhao, Wei Wang, Junhui Hou, Hai Wu", "abstract": "This paper introduces HPC-Net, a high-precision and rapidly convergent object\ndetection network.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01134v1"}
{"title": "Joint Generative Modeling of Scene Graphs and Images via Diffusion Models", "author": "Bicheng Xu, Qi Yan, Renjie Liao, Lele Wang, Leonid Sigal", "abstract": "In this paper, we present a novel generative task: joint scene graph - image\ngeneration. While previous works have explored image generation conditioned on\nscene graphs or layouts, our task is distinctive and important as it involves\ngenerating scene graphs themselves unconditionally from noise, enabling\nefficient and interpretable control for image generation. Our task is\nchallenging, requiring the generation of plausible scene graphs with\nheterogeneous attributes for nodes (objects) and edges (relations among\nobjects), including continuous object bounding boxes and discrete object and\nrelation categories. We introduce a novel diffusion model, DiffuseSG, that\njointly models the adjacency matrix along with heterogeneous node and edge\nattributes. We explore various types of encodings for the categorical data,\nrelaxing it into a continuous space. With a graph transformer being the\ndenoiser, DiffuseSG successively denoises the scene graph representation in a\ncontinuous space and discretizes the final representation to generate the clean\nscene graph. Additionally, we introduce an IoU regularization to enhance the\nempirical performance. Our model significantly outperforms existing methods in\nscene graph generation on the Visual Genome and COCO-Stuff datasets, both on\nstandard and newly introduced metrics that better capture the problem\ncomplexity. Moreover, we demonstrate the additional benefits of our model in\ntwo downstream applications: 1) excelling in a series of scene graph completion\ntasks, and 2) improving scene graph detection models by using extra training\nsamples generated from DiffuseSG.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01130v1"}
{"title": "SSP: A Simple and Safe automatic Prompt engineering method towards realistic image synthesis on LVM", "author": "Weijin Cheng, Jianzhi Liu, Jiawen Deng, Fuji Ren", "abstract": "Recently, text-to-image (T2I) synthesis has undergone significant\nadvancements, particularly with the emergence of Large Language Models (LLM)\nand their enhancement in Large Vision Models (LVM), greatly enhancing the\ninstruction-following capabilities of traditional T2I models. Nevertheless,\nprevious methods focus on improving generation quality but introduce unsafe\nfactors into prompts. We explore that appending specific camera descriptions to\nprompts can enhance safety performance. Consequently, we propose a simple and\nsafe prompt engineering method (SSP) to improve image generation quality by\nproviding optimal camera descriptions. Specifically, we create a dataset from\nmulti-datasets as original prompts. To select the optimal camera, we design an\noptimal camera matching approach and implement a classifier for original\nprompts capable of automatically matching. Appending camera descriptions to\noriginal prompts generates optimized prompts for further LVM image generation.\nExperiments demonstrate that SSP improves semantic consistency by an average of\n16% compared to others and safety metrics by 48.9%.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01128v1"}
{"title": "Q-Refine: A Perceptual Quality Refiner for AI-Generated Image", "author": "Chunyi Li, Haoning Wu, Zicheng Zhang, Hongkun Hao, Kaiwei Zhang, Lei Bai, Xiaohong Liu, Xiongkuo Min, Weisi Lin, Guangtao Zhai", "abstract": "With the rapid evolution of the Text-to-Image (T2I) model in recent years,\ntheir unsatisfactory generation result has become a challenge. However,\nuniformly refining AI-Generated Images (AIGIs) of different qualities not only\nlimited optimization capabilities for low-quality AIGIs but also brought\nnegative optimization to high-quality AIGIs. To address this issue, a\nquality-award refiner named Q-Refine is proposed. Based on the preference of\nthe Human Visual System (HVS), Q-Refine uses the Image Quality Assessment (IQA)\nmetric to guide the refining process for the first time, and modify images of\ndifferent qualities through three adaptive pipelines. Experimental shows that\nfor mainstream T2I models, Q-Refine can perform effective optimization to AIGIs\nof different qualities. It can be a general refiner to optimize AIGIs from both\nfidelity and aesthetic quality levels, thus expanding the application of the\nT2I generation models.", "published": "2024-01-02", "categories": ["cs.CV", "eess.IV"], "links": "http://arxiv.org/abs/2401.01117v1"}
{"title": "CityPulse: Fine-Grained Assessment of Urban Change with Street View Time Series", "author": "Tianyuan Huang, Zejia Wu, Jiajun Wu, Jackelyn Hwang, Ram Rajagopal", "abstract": "Urban transformations have profound societal impact on both individuals and\ncommunities at large. Accurately assessing these shifts is essential for\nunderstanding their underlying causes and ensuring sustainable urban planning.\nTraditional measurements often encounter constraints in spatial and temporal\ngranularity, failing to capture real-time physical changes. While street view\nimagery, capturing the heartbeat of urban spaces from a pedestrian point of\nview, can add as a high-definition, up-to-date, and on-the-ground visual proxy\nof urban change. We curate the largest street view time series dataset to date,\nand propose an end-to-end change detection model to effectively capture\nphysical alterations in the built environment at scale. We demonstrate the\neffectiveness of our proposed method by benchmark comparisons with previous\nliterature and implementing it at the city-wide level. Our approach has the\npotential to supplement existing dataset and serve as a fine-grained and\naccurate assessment of urban change.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01107v2"}
{"title": "Dual Teacher Knowledge Distillation with Domain Alignment for Face Anti-spoofing", "author": "Zhe Kong, Wentian Zhang, Tao Wang, Kaihao Zhang, Yuexiang Li, Xiaoying Tang, Wenhan Luo", "abstract": "Face recognition systems have raised concerns due to their vulnerability to\ndifferent presentation attacks, and system security has become an increasingly\ncritical concern. Although many face anti-spoofing (FAS) methods perform well\nin intra-dataset scenarios, their generalization remains a challenge. To\naddress this issue, some methods adopt domain adversarial training (DAT) to\nextract domain-invariant features. However, the competition between the encoder\nand the domain discriminator can cause the network to be difficult to train and\nconverge. In this paper, we propose a domain adversarial attack (DAA) method to\nmitigate the training instability problem by adding perturbations to the input\nimages, which makes them indistinguishable across domains and enables domain\nalignment. Moreover, since models trained on limited data and types of attacks\ncannot generalize well to unknown attacks, we propose a dual perceptual and\ngenerative knowledge distillation framework for face anti-spoofing that\nutilizes pre-trained face-related models containing rich face priors.\nSpecifically, we adopt two different face-related models as teachers to\ntransfer knowledge to the target student model. The pre-trained teacher models\nare not from the task of face anti-spoofing but from perceptual and generative\ntasks, respectively, which implicitly augment the data. By combining both DAA\nand dual-teacher knowledge distillation, we develop a dual teacher knowledge\ndistillation with domain alignment framework (DTDA) for face anti-spoofing. The\nadvantage of our proposed method has been verified through extensive ablation\nstudies and comparison with state-of-the-art methods on public datasets across\nmultiple protocols.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01102v1"}
{"title": "Robust single-particle cryo-EM image denoising and restoration", "author": "Jing Zhang, Tengfei Zhao, ShiYu Hu, Xin Zhao", "abstract": "Cryo-electron microscopy (cryo-EM) has achieved near-atomic level resolution\nof biomolecules by reconstructing 2D micrographs. However, the resolution and\naccuracy of the reconstructed particles are significantly reduced due to the\nextremely low signal-to-noise ratio (SNR) and complex noise structure of\ncryo-EM images. In this paper, we introduce a diffusion model with\npost-processing framework to effectively denoise and restore single particle\ncryo-EM images. Our method outperforms the state-of-the-art (SOTA) denoising\nmethods by effectively removing structural noise that has not been addressed\nbefore. Additionally, more accurate and high-resolution three-dimensional\nreconstruction structures can be obtained from denoised cryo-EM images.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01097v1"}
{"title": "Exploring Hyperspectral Anomaly Detection with Human Vision: A Small Target Aware Detector", "author": "Jitao Ma, Weiying Xie, Yunsong Li", "abstract": "Hyperspectral anomaly detection (HAD) aims to localize pixel points whose\nspectral features differ from the background. HAD is essential in scenarios of\nunknown or camouflaged target features, such as water quality monitoring, crop\ngrowth monitoring and camouflaged target detection, where prior information of\ntargets is difficult to obtain. Existing HAD methods aim to objectively detect\nand distinguish background and anomalous spectra, which can be achieved almost\neffortlessly by human perception. However, the underlying processes of human\nvisual perception are thought to be quite complex. In this paper, we analyze\nhyperspectral image (HSI) features under human visual perception, and transfer\nthe solution process of HAD to the more robust feature space for the first\ntime. Specifically, we propose a small target aware detector (STAD), which\nintroduces saliency maps to capture HSI features closer to human visual\nperception. STAD not only extracts more anomalous representations, but also\nreduces the impact of low-confidence regions through a proposed small target\nfilter (STF). Furthermore, considering the possibility of HAD algorithms being\napplied to edge devices, we propose a full connected network to convolutional\nnetwork knowledge distillation strategy. It can learn the spectral and spatial\nfeatures of the HSI while lightening the network. We train the network on the\nHAD100 training set and validate the proposed method on the HAD100 test set.\nOur method provides a new solution space for HAD that is closer to human visual\nperception with high confidence. Sufficient experiments on real HSI with\nmultiple method comparisons demonstrate the excellent performance and unique\npotential of the proposed method. The code is available at\nhttps://github.com/majitao-xd/STAD-HAD.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01093v1"}
{"title": "Depth-discriminative Metric Learning for Monocular 3D Object Detection", "author": "Wonhyeok Choi, Mingyu Shin, Sunghoon Im", "abstract": "Monocular 3D object detection poses a significant challenge due to the lack\nof depth information in RGB images. Many existing methods strive to enhance the\nobject depth estimation performance by allocating additional parameters for\nobject depth estimation, utilizing extra modules or data. In contrast, we\nintroduce a novel metric learning scheme that encourages the model to extract\ndepth-discriminative features regardless of the visual attributes without\nincreasing inference time and model size. Our method employs the\ndistance-preserving function to organize the feature space manifold in relation\nto ground-truth object depth. The proposed (K, B, eps)-quasi-isometric loss\nleverages predetermined pairwise distance restriction as guidance for adjusting\nthe distance among object descriptors without disrupting the non-linearity of\nthe natural feature manifold. Moreover, we introduce an auxiliary head for\nobject-wise depth estimation, which enhances depth quality while maintaining\nthe inference time. The broad applicability of our method is demonstrated\nthrough experiments that show improvements in overall performance when\nintegrated into various baselines. The results show that our method\nconsistently improves the performance of various baselines by 23.51% and 5.78%\non average across KITTI and Waymo, respectively.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01075v1"}
{"title": "AliFuse: Aligning and Fusing Multi-modal Medical Data for Computer-Aided Diagnosis", "author": "Qiuhui Chen, Xinyue Hu, Zirui Wang, Yi Hong", "abstract": "Medical data collected for making a diagnostic decision are typically\nmulti-modal and provide complementary perspectives of a subject. A\ncomputer-aided diagnosis system welcomes multi-modal inputs; however, how to\neffectively fuse such multi-modal data is a challenging task and attracts a lot\nof attention in the medical research field. In this paper, we propose a\ntransformer-based framework, called Alifuse, for aligning and fusing\nmulti-modal medical data. Specifically, we convert images and unstructured and\nstructured texts into vision and language tokens, and use intramodal and\nintermodal attention mechanisms to learn holistic representations of all\nimaging and non-imaging data for classification. We apply Alifuse to classify\nAlzheimer's disease and obtain state-of-the-art performance on five public\ndatasets, by outperforming eight baselines. The source code will be available\nonline later.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01074v1"}
{"title": "DTBS: Dual-Teacher Bi-directional Self-training for Domain Adaptation in Nighttime Semantic Segmentation", "author": "Fanding Huang, Zihao Yao, Wenhui Zhou", "abstract": "Due to the poor illumination and the difficulty in annotating, nighttime\nconditions pose a significant challenge for autonomous vehicle perception\nsystems. Unsupervised domain adaptation (UDA) has been widely applied to\nsemantic segmentation on such images to adapt models from normal conditions to\ntarget nighttime-condition domains. Self-training (ST) is a paradigm in UDA,\nwhere a momentum teacher is utilized for pseudo-label prediction, but a\nconfirmation bias issue exists. Because the one-directional knowledge transfer\nfrom a single teacher is insufficient to adapt to a large domain shift. To\nmitigate this issue, we propose to alleviate domain gap by incrementally\nconsidering style influence and illumination change. Therefore, we introduce a\none-stage Dual-Teacher Bi-directional Self-training (DTBS) framework for smooth\nknowledge transfer and feedback. Based on two teacher models, we present a\nnovel pipeline to respectively decouple style and illumination shift. In\naddition, we propose a new Re-weight exponential moving average (EMA) to merge\nthe knowledge of style and illumination factors, and provide feedback to the\nstudent model. In this way, our method can be embedded in other UDA methods to\nenhance their performance. For example, the Cityscapes to ACDC night task\nyielded 53.8 mIoU (\\%), which corresponds to an improvement of +5\\% over the\nprevious state-of-the-art. The code is available at\n\\url{https://github.com/hf618/DTBS}.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01066v1"}
{"title": "BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving", "author": "Dafeng Wei, Tian Gao, Zhengyu Jia, Changwei Cai, Chengkai Hou, Peng Jia, Fu Liu, Kun Zhan, Jingchen Fan, Yixing Zhao, Yang Wang", "abstract": "The demand for the retrieval of complex scene data in autonomous driving is\nincreasing, especially as passenger vehicles have been equipped with the\nability to navigate urban settings, with the imperative to address long-tail\nscenarios. Meanwhile, under the pre-existing two dimensional image retrieval\nmethod, some problems may arise with scene retrieval, such as lack of global\nfeature representation and subpar text retrieval ability. To address these\nissues, we have proposed \\textbf{BEV-CLIP}, the first multimodal Bird's-Eye\nView(BEV) retrieval methodology that utilizes descriptive text as an input to\nretrieve corresponding scenes. This methodology applies the semantic feature\nextraction abilities of a large language model (LLM) to facilitate zero-shot\nretrieval of extensive text descriptions, and incorporates semi-structured\ninformation from a knowledge graph to improve the semantic richness and variety\nof the language embedding. Our experiments result in 87.66% accuracy on\nNuScenes dataset in text-to-BEV feature retrieval. The demonstrated cases in\nour paper support that our retrieval method is also indicated to be effective\nin identifying certain long-tail corner scenes.", "published": "2024-01-02", "categories": ["cs.CV", "cs.AI", "I.2.10"], "links": "http://arxiv.org/abs/2401.01065v1"}
{"title": "Relating Events and Frames Based on Self-Supervised Learning and Uncorrelated Conditioning for Unsupervised Domain Adaptation", "author": "Mohammad Rostami, Dayuan Jian", "abstract": "Event-based cameras provide accurate and high temporal resolution\nmeasurements for performing computer vision tasks in challenging scenarios,\nsuch as high-dynamic range environments and fast-motion maneuvers. Despite\ntheir advantages, utilizing deep learning for event-based vision encounters a\nsignificant obstacle due to the scarcity of annotated data caused by the\nrelatively recent emergence of event-based cameras. To overcome this\nlimitation, leveraging the knowledge available from annotated data obtained\nwith conventional frame-based cameras presents an effective solution based on\nunsupervised domain adaptation. We propose a new algorithm tailored for\nadapting a deep neural network trained on annotated frame-based data to\ngeneralize well on event-based unannotated data. Our approach incorporates\nuncorrelated conditioning and self-supervised learning in an adversarial\nlearning scheme to close the gap between the two source and target domains. By\napplying self-supervised learning, the algorithm learns to align the\nrepresentations of event-based data with those from frame-based camera data,\nthereby facilitating knowledge transfer.Furthermore, the inclusion of\nuncorrelated conditioning ensures that the adapted model effectively\ndistinguishes between event-based and conventional data, enhancing its ability\nto classify event-based images accurately.Through empirical experimentation and\nevaluation, we demonstrate that our algorithm surpasses existing approaches\ndesigned for the same purpose using two benchmarks. The superior performance of\nour solution is attributed to its ability to effectively utilize annotated data\nfrom frame-based cameras and transfer the acquired knowledge to the event-based\nvision domain.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01042v1"}
{"title": "Online Continual Domain Adaptation for Semantic Image Segmentation Using Internal Representations", "author": "Serban Stan, Mohammad Rostami", "abstract": "Semantic segmentation models trained on annotated data fail to generalize\nwell when the input data distribution changes over extended time period,\nleading to requiring re-training to maintain performance. Classic Unsupervised\ndomain adaptation (UDA) attempts to address a similar problem when there is\ntarget domain with no annotated data points through transferring knowledge from\na source domain with annotated data. We develop an online UDA algorithm for\nsemantic segmentation of images that improves model generalization on\nunannotated domains in scenarios where source data access is restricted during\nadaptation. We perform model adaptation is by minimizing the distributional\ndistance between the source latent features and the target features in a shared\nembedding space. Our solution promotes a shared domain-agnostic latent feature\nspace between the two domains, which allows for classifier generalization on\nthe target dataset. To alleviate the need of access to source samples during\nadaptation, we approximate the source latent feature distribution via an\nappropriate surrogate distribution, in this case a Gassian mixture model (GMM).\nWe evaluate our approach on well established semantic segmentation datasets and\ndemonstrate it compares favorably against state-of-the-art (SOTA) UDA semantic\nsegmentation methods.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.01035v1"}
{"title": "Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt", "author": "Jiaqi Liu, Kai Wu, Qiang Nie, Ying Chen, Bin-Bin Gao, Yong Liu, Jinbao Wang, Chengjie Wang, Feng Zheng", "abstract": "Unsupervised Anomaly Detection (UAD) with incremental training is crucial in\nindustrial manufacturing, as unpredictable defects make obtaining sufficient\nlabeled data infeasible. However, continual learning methods primarily rely on\nsupervised annotations, while the application in UAD is limited due to the\nabsence of supervision. Current UAD methods train separate models for different\nclasses sequentially, leading to catastrophic forgetting and a heavy\ncomputational burden. To address this issue, we introduce a novel Unsupervised\nContinual Anomaly Detection framework called UCAD, which equips the UAD with\ncontinual learning capability through contrastively-learned prompts. In the\nproposed UCAD, we design a Continual Prompting Module (CPM) by utilizing a\nconcise key-prompt-knowledge memory bank to guide task-invariant `anomaly'\nmodel predictions using task-specific `normal' knowledge. Moreover,\nStructure-based Contrastive Learning (SCL) is designed with the Segment\nAnything Model (SAM) to improve prompt learning and anomaly segmentation\nresults. Specifically, by treating SAM's masks as structure, we draw features\nwithin the same mask closer and push others apart for general feature\nrepresentations. We conduct comprehensive experiments and set the benchmark on\nunsupervised continual anomaly detection and segmentation, demonstrating that\nour method is significantly better than anomaly detection methods, even with\nrehearsal training. The code will be available at\nhttps://github.com/shirowalker/UCAD.", "published": "2024-01-02", "categories": ["cs.CV", "cs.LG"], "links": "http://arxiv.org/abs/2401.01010v1"}
{"title": "Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network", "author": "Yongqi Ding, Lin Zuo, Mengmeng Jing, Pei He, Yongjun Xiao", "abstract": "Neuromorphic object recognition with spiking neural networks (SNNs) is the\ncornerstone of low-power neuromorphic computing. However, existing SNNs suffer\nfrom significant latency, utilizing 10 to 40 timesteps or more, to recognize\nneuromorphic objects. At low latencies, the performance of existing SNNs is\ndrastically degraded. In this work, we propose the Shrinking SNN (SSNN) to\nachieve low-latency neuromorphic object recognition without reducing\nperformance. Concretely, we alleviate the temporal redundancy in SNNs by\ndividing SNNs into multiple stages with progressively shrinking timesteps,\nwhich significantly reduces the inference latency. During timestep shrinkage,\nthe temporal transformer smoothly transforms the temporal scale and preserves\nthe information maximally. Moreover, we add multiple early classifiers to the\nSNN during training to mitigate the mismatch between the surrogate gradient and\nthe true gradient, as well as the gradient vanishing/exploding, thus\neliminating the performance degradation at low latency. Extensive experiments\non neuromorphic datasets, CIFAR10-DVS, N-Caltech101, and DVS-Gesture have\nrevealed that SSNN is able to improve the baseline accuracy by 6.55% ~ 21.41%.\nWith only 5 average timesteps and without any data augmentation, SSNN is able\nto achieve an accuracy of 73.63% on CIFAR10-DVS. This work presents a\nheterogeneous temporal scale SNN and provides valuable insights into the\ndevelopment of high-performance, low-latency SNNs.", "published": "2024-01-02", "categories": ["cs.CV", "cs.LG", "eess.IV"], "links": "http://arxiv.org/abs/2401.01912v1"}
{"title": "Diversity-aware Buffer for Coping with Temporally Correlated Data Streams in Online Test-time Adaptation", "author": "Mario Döbler, Florian Marencke, Robert A. Marsden, Bin Yang", "abstract": "Since distribution shifts are likely to occur after a model's deployment and\ncan drastically decrease the model's performance, online test-time adaptation\n(TTA) continues to update the model during test-time, leveraging the current\ntest data. In real-world scenarios, test data streams are not always\nindependent and identically distributed (i.i.d.). Instead, they are frequently\ntemporally correlated, making them non-i.i.d. Many existing methods struggle to\ncope with this scenario. In response, we propose a diversity-aware and\ncategory-balanced buffer that can simulate an i.i.d. data stream, even in\nnon-i.i.d. scenarios. Combined with a diversity and entropy-weighted entropy\nloss, we show that a stable adaptation is possible on a wide range of\ncorruptions and natural domain shifts, based on ImageNet. We achieve\nstate-of-the-art results on most considered benchmarks.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00989v1"}
{"title": "Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models", "author": "Xinpeng Ding, Jinahua Han, Hang Xu, Xiaodan Liang, Wei Zhang, Xiaomeng Li", "abstract": "The rise of multimodal large language models (MLLMs) has spurred interest in\nlanguage-based driving tasks. However, existing research typically focuses on\nlimited tasks and often omits key multi-view and temporal information which is\ncrucial for robust autonomous driving. To bridge these gaps, we introduce\nNuInstruct, a novel dataset with 91K multi-view video-QA pairs across 17\nsubtasks, where each task demands holistic information (e.g., temporal,\nmulti-view, and spatial), significantly elevating the challenge level. To\nobtain NuInstruct, we propose a novel SQL-based method to generate\ninstruction-response pairs automatically, which is inspired by the driving\nlogical progression of humans. We further present BEV-InMLLM, an end-to-end\nmethod for efficiently deriving instruction-aware Bird's-Eye-View (BEV)\nfeatures, language-aligned for large language models. BEV-InMLLM integrates\nmulti-view, spatial awareness, and temporal semantics to enhance MLLMs'\ncapabilities on NuInstruct tasks. Moreover, our proposed BEV injection module\nis a plug-and-play method for existing MLLMs. Our experiments on NuInstruct\ndemonstrate that BEV-InMLLM significantly outperforms existing MLLMs, e.g.\naround 9% improvement on various tasks. We plan to release our NuInstruct for\nfuture research development.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00988v1"}
{"title": "Real-Time Object Detection in Occluded Environment with Background Cluttering Effects Using Deep Learning", "author": "Syed Muhammad Aamir, Hongbin Ma, Malak Abid Ali Khan, Muhammad Aaqib", "abstract": "Detection of small, undetermined moving objects or objects in an occluded\nenvironment with a cluttered background is the main problem of computer vision.\nThis greatly affects the detection accuracy of deep learning models. To\novercome these problems, we concentrate on deep learning models for real-time\ndetection of cars and tanks in an occluded environment with a cluttered\nbackground employing SSD and YOLO algorithms and improved precision of\ndetection and reduce problems faced by these models. The developed method makes\nthe custom dataset and employs a preprocessing technique to clean the noisy\ndataset. For training the developed model we apply the data augmentation\ntechnique to balance and diversify the data. We fine-tuned, trained, and\nevaluated these models on the established dataset by applying these techniques\nand highlighting the results we got more accurately than without applying these\ntechniques. The accuracy and frame per second of the SSD-Mobilenet v2 model are\nhigher than YOLO V3 and YOLO V4. Furthermore, by employing various techniques\nlike data enhancement, noise reduction, parameter optimization, and model\nfusion we improve the effectiveness of detection and recognition. We further\nadded a counting algorithm, and target attributes experimental comparison, and\nmade a graphical user interface system for the developed model with features of\nobject counting, alerts, status, resolution, and frame per second.\nSubsequently, to justify the importance of the developed method analysis of\nYOLO V3, V4, and SSD were incorporated. Which resulted in the overall\ncompletion of the proposed method.", "published": "2024-01-02", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.00986v1"}
{"title": "3D Visibility-aware Generalizable Neural Radiance Fields for Interacting Hands", "author": "Xuan Huang, Hanhui Li, Zejun Yang, Zhisheng Wang, Xiaodan Liang", "abstract": "Neural radiance fields (NeRFs) are promising 3D representations for scenes,\nobjects, and humans. However, most existing methods require multi-view inputs\nand per-scene training, which limits their real-life applications. Moreover,\ncurrent methods focus on single-subject cases, leaving scenes of interacting\nhands that involve severe inter-hand occlusions and challenging view variations\nremain unsolved. To tackle these issues, this paper proposes a generalizable\nvisibility-aware NeRF (VA-NeRF) framework for interacting hands. Specifically,\ngiven an image of interacting hands as input, our VA-NeRF first obtains a\nmesh-based representation of hands and extracts their corresponding geometric\nand textural features. Subsequently, a feature fusion module that exploits the\nvisibility of query points and mesh vertices is introduced to adaptively merge\nfeatures of both hands, enabling the recovery of features in unseen areas.\nAdditionally, our VA-NeRF is optimized together with a novel discriminator\nwithin an adversarial learning paradigm. In contrast to conventional\ndiscriminators that predict a single real/fake label for the synthesized image,\nthe proposed discriminator generates a pixel-wise visibility map, providing\nfine-grained supervision for unseen areas and encouraging the VA-NeRF to\nimprove the visual quality of synthesized images. Experiments on the\nInterhand2.6M dataset demonstrate that our proposed VA-NeRF outperforms\nconventional NeRFs significantly. Project Page:\n\\url{https://github.com/XuanHuang0/VANeRF}.", "published": "2024-01-02", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.00979v1"}
