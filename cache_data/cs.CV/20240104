{"title": "Learning to Prompt with Text Only Supervision for Vision-Language Models", "author": "Muhammad Uzair Khattak, Muhammad Ferjad Naeem, Muzammal Naseer, Luc Van Gool, Federico Tombari", "abstract": "Foundational vision-language models such as CLIP are becoming a new paradigm\nin vision, due to their excellent generalization abilities. However, adapting\nthese models for downstream tasks while maintaining their generalization\nremains a challenge. In literature, one branch of methods adapts CLIP by\nlearning prompts using visual information. While effective, most of these works\nrequire labeled data which is not practical, and often struggle to generalize\ntowards new datasets due to over-fitting on the source data. An alternative\napproach resorts to training-free methods by generating class descriptions from\nlarge language models (LLMs) and perform prompt ensembling. However, these\nmethods often generate class specific prompts that cannot be transferred to\nother classes, which incur higher costs by generating LLM descriptions for each\nclass separately. In this work, we propose to combine the strengths of these\nboth streams of methods by learning prompts using only text data derived from\nLLMs. As supervised training of prompts is not trivial due to absence of\nimages, we develop a training approach that allows prompts to extract rich\ncontextual knowledge from LLM data. Moreover, with LLM contextual data mapped\nwithin the learned prompts, it enables zero-shot transfer of prompts to new\nclasses and datasets potentially cutting the LLM prompt engineering cost. To\nthe best of our knowledge, this is the first work that learns generalized\nprompts using text only data. We perform extensive evaluations on 4 benchmarks\nwhere our method improves over prior ensembling works while being competitive\nto those utilizing labeled images. Our code and pre-trained models are\navailable at https://github.com/muzairkhattak/ProText.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02418v1"}
{"title": "ODIN: A Single Model for 2D and 3D Perception", "author": "Ayush Jain, Pushkal Katara, Nikolaos Gkanatsios, Adam W. Harley, Gabriel Sarch, Kriti Aggarwal, Vishrav Chaudhary, Katerina Fragkiadaki", "abstract": "State-of-the-art models on contemporary 3D perception benchmarks like ScanNet\nconsume and label dataset-provided 3D point clouds, obtained through post\nprocessing of sensed multiview RGB-D images. They are typically trained\nin-domain, forego large-scale 2D pre-training and outperform alternatives that\nfeaturize the posed RGB-D multiview images instead. The gap in performance\nbetween methods that consume posed images versus post-processed 3D point clouds\nhas fueled the belief that 2D and 3D perception require distinct model\narchitectures. In this paper, we challenge this view and propose ODIN\n(Omni-Dimensional INstance segmentation), a model that can segment and label\nboth 2D RGB images and 3D point clouds, using a transformer architecture that\nalternates between 2D within-view and 3D cross-view information fusion. Our\nmodel differentiates 2D and 3D feature operations through the positional\nencodings of the tokens involved, which capture pixel coordinates for 2D patch\ntokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art\nperformance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation\nbenchmarks, and competitive performance on ScanNet, S3DIS and COCO. It\noutperforms all previous works by a wide margin when the sensed 3D point cloud\nis used in place of the point cloud sampled from 3D mesh. When used as the 3D\nperception engine in an instructable embodied agent architecture, it sets a new\nstate-of-the-art on the TEACh action-from-dialogue benchmark. Our code and\ncheckpoints can be found at the project website: https://odin-seg.github.io.", "published": "2024-01-04", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "links": "http://arxiv.org/abs/2401.02416v1"}
{"title": "Bring Metric Functions into Diffusion Models", "author": "Jie An, Zhengyuan Yang, Jianfeng Wang, Linjie Li, Zicheng Liu, Lijuan Wang, Jiebo Luo", "abstract": "We introduce a Cascaded Diffusion Model (Cas-DM) that improves a Denoising\nDiffusion Probabilistic Model (DDPM) by effectively incorporating additional\nmetric functions in training. Metric functions such as the LPIPS loss have been\nproven highly effective in consistency models derived from the score matching.\nHowever, for the diffusion counterparts, the methodology and efficacy of adding\nextra metric functions remain unclear. One major challenge is the mismatch\nbetween the noise predicted by a DDPM at each step and the desired clean image\nthat the metric function works well on. To address this problem, we propose\nCas-DM, a network architecture that cascades two network modules to effectively\napply metric functions to the diffusion model training. The first module,\nsimilar to a standard DDPM, learns to predict the added noise and is unaffected\nby the metric function. The second cascaded module learns to predict the clean\nimage, thereby facilitating the metric function computation. Experiment results\nshow that the proposed diffusion model backbone enables the effective use of\nthe LPIPS loss, leading to state-of-the-art image quality (FID, sFID, IS) on\nvarious established benchmarks.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02414v1"}
{"title": "LLM Augmented LLMs: Expanding Capabilities through Composition", "author": "Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar", "abstract": "Foundational models with billions of parameters which have been trained on\nlarge corpora of data have demonstrated non-trivial skills in a variety of\ndomains. However, due to their monolithic structure, it is challenging and\nexpensive to augment them or impart new skills. On the other hand, due to their\nadaptation abilities, several new instances of these models are being trained\ntowards new domains and tasks. In this work, we study the problem of efficient\nand practical composition of existing foundation models with more specific\nmodels to enable newer capabilities. To this end, we propose CALM --\nComposition to Augment Language Models -- which introduces cross-attention\nbetween models to compose their representations and enable new capabilities.\nSalient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'\nexisting LLMs along with a few additional parameters and data, (ii) Existing\nmodel weights are kept intact, and hence preserves existing capabilities, and\n(iii) Applies to diverse domains and settings. We illustrate that augmenting\nPaLM2-S with a smaller model trained on low-resource languages results in an\nabsolute improvement of up to 13\\% on tasks like translation into English and\narithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is\naugmented with a code-specific model, we see a relative improvement of 40\\%\nover the base model for code generation and explanation tasks -- on-par with\nfully fine-tuned counterparts.", "published": "2024-01-04", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "links": "http://arxiv.org/abs/2401.02412v1"}
{"title": "What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs", "author": "Alex Trevithick, Matthew Chan, Towaki Takikawa, Umar Iqbal, Shalini De Mello, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano", "abstract": "3D-aware Generative Adversarial Networks (GANs) have shown remarkable\nprogress in learning to generate multi-view-consistent images and 3D geometries\nof scenes from collections of 2D images via neural volume rendering. Yet, the\nsignificant memory and computational costs of dense sampling in volume\nrendering have forced 3D GANs to adopt patch-based training or employ\nlow-resolution rendering with post-processing 2D super resolution, which\nsacrifices multiview consistency and the quality of resolved geometry.\nConsequently, 3D GANs have not yet been able to fully resolve the rich 3D\ngeometry present in 2D images. In this work, we propose techniques to scale\nneural volume rendering to the much higher resolution of native 2D images,\nthereby resolving fine-grained 3D geometry with unprecedented detail. Our\napproach employs learning-based samplers for accelerating neural rendering for\n3D GAN training using up to 5 times fewer depth samples. This enables us to\nexplicitly \"render every pixel\" of the full-resolution image during training\nand inference without post-processing superresolution in 2D. Together with our\nstrategy to learn high-quality surface geometry, our method synthesizes\nhigh-resolution 3D geometry and strictly view-consistent images while\nmaintaining image quality on par with baselines relying on post-processing\nsuper resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ\nand AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D\nGANs.", "published": "2024-01-04", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "links": "http://arxiv.org/abs/2401.02411v1"}
{"title": "3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language Distillation", "author": "Zihao Xiao, Longlong Jing, Shangxuan Wu, Alex Zihao Zhu, Jingwei Ji, Chiyu Max Jiang, Wei-Chih Hung, Thomas Funkhouser, Weicheng Kuo, Anelia Angelova, Yin Zhou, Shiwei Sheng", "abstract": "3D panoptic segmentation is a challenging perception task, which aims to\npredict both semantic and instance annotations for 3D points in a scene.\nAlthough prior 3D panoptic segmentation approaches have achieved great\nperformance on closed-set benchmarks, generalizing to novel categories remains\nan open problem. For unseen object categories, 2D open-vocabulary segmentation\nhas achieved promising results that solely rely on frozen CLIP backbones and\nensembling multiple classification outputs. However, we find that simply\nextending these 2D models to 3D does not achieve good performance due to poor\nper-mask classification quality on novel categories. In this paper, we propose\nthe first method to tackle 3D open-vocabulary panoptic segmentation. Our model\ntakes advantage of the fusion between learnable LiDAR features and dense frozen\nvision CLIP features, using a single classification head to make predictions\nfor both base and novel classes. To further improve the classification\nperformance on novel classes and leverage the CLIP model, we propose two novel\nloss functions: object-level distillation loss and voxel-level distillation\nloss. Our experiments on the nuScenes and SemanticKITTI datasets show that our\nmethod outperforms strong baselines by a large margin.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02402v1"}
{"title": "Learning the 3D Fauna of the Web", "author": "Zizhang Li, Dor Litvak, Ruining Li, Yunzhi Zhang, Tomas Jakab, Christian Rupprecht, Shangzhe Wu, Andrea Vedaldi, Jiajun Wu", "abstract": "Learning 3D models of all animals on the Earth requires massively scaling up\nexisting solutions. With this ultimate goal in mind, we develop 3D-Fauna, an\napproach that learns a pan-category deformable 3D animal model for more than\n100 animal species jointly. One crucial bottleneck of modeling animals is the\nlimited availability of training data, which we overcome by simply learning\nfrom 2D Internet images. We show that prior category-specific attempts fail to\ngeneralize to rare species with limited training images. We address this\nchallenge by introducing the Semantic Bank of Skinned Models (SBSM), which\nautomatically discovers a small set of base animal shapes by combining\ngeometric inductive priors with semantic knowledge implicitly captured by an\noff-the-shelf self-supervised feature extractor. To train such a model, we also\ncontribute a new large-scale dataset of diverse animal species. At inference\ntime, given a single image of any quadruped animal, our model reconstructs an\narticulated 3D mesh in a feed-forward fashion within seconds.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02400v1"}
{"title": "ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning", "author": "Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao, Kaipeng Zhang, Yu Qiao, Ping Luo", "abstract": "Charts play a vital role in data visualization, understanding data patterns,\nand informed decision-making. However, their unique combination of graphical\nelements (e.g., bars, lines) and textual components (e.g., labels, legends)\nposes challenges for general-purpose multimodal models. While vision-language\nmodels trained on chart data excel in comprehension, they struggle with\ngeneralization and require task-specific fine-tuning. To address these\nchallenges, we propose ChartAssistant, a chart-based vision-language model for\nuniversal chart comprehension and reasoning. ChartAssistant leverages ChartSFT,\na comprehensive dataset covering diverse chart-related tasks with basic and\nspecialized chart types. It undergoes a two-stage training process, starting\nwith pre-training on chart-to-table parsing to align chart and text, followed\nby multitask instruction-following fine-tuning. This approach enables\nChartAssistant to achieve competitive performance across various chart tasks\nwithout task-specific fine-tuning. Experimental results demonstrate significant\nperformance gains over the state-of-the-art UniChart method, outperforming\nOpenAI's GPT-4V(ision) on real-world chart data. The code and data are\navailable at https://github.com/OpenGVLab/ChartAst.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02384v1"}
{"title": "Survey of 3D Human Body Pose and Shape Estimation Methods for Contemporary Dance Applications", "author": "Darshan Venkatrayappa, Alain Tremeau, Damien Muselet, Philippe Colantoni", "abstract": "3D human body shape and pose estimation from RGB images is a challenging\nproblem with potential applications in augmented/virtual reality, healthcare\nand fitness technology and virtual retail. Recent solutions have focused on\nthree types of inputs: i) single images, ii) multi-view images and iii) videos.\nIn this study, we surveyed and compared 3D body shape and pose estimation\nmethods for contemporary dance and performing arts, with a special focus on\nhuman body pose and dressing, camera viewpoint, illumination conditions and\nbackground conditions. We demonstrated that multi-frame methods, such as PHALP,\nprovide better results than single-frame method for pose estimation when\ndancers are performing contemporary dances.", "published": "2024-01-04", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.02383v1"}
{"title": "An Open and Comprehensive Pipeline for Unified Object Grounding and Detection", "author": "Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, Haian Huang", "abstract": "Grounding-DINO is a state-of-the-art open-set detection model that tackles\nmultiple vision tasks including Open-Vocabulary Detection (OVD), Phrase\nGrounding (PG), and Referring Expression Comprehension (REC). Its effectiveness\nhas led to its widespread adoption as a mainstream architecture for various\ndownstream applications. However, despite its significance, the original\nGrounding-DINO model lacks comprehensive public technical details due to the\nunavailability of its training code. To bridge this gap, we present\nMM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline,\nwhich is built with the MMDetection toolbox. It adopts abundant vision datasets\nfor pre-training and various detection and grounding datasets for fine-tuning.\nWe give a comprehensive analysis of each reported result and detailed settings\nfor reproduction. The extensive experiments on the benchmarks mentioned\ndemonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny\nbaseline. We release all our models to the research community. Codes and\ntrained models are released at\nhttps://github.com/open-mmlab/mmdetection/configs/mm_grounding_dino.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02361v1"}
{"title": "A novel method to enhance pneumonia detection via a model-level ensembling of CNN and vision transformer", "author": "Sandeep Angara, Nishith Reddy Mannuru, Aashrith Mannuru, Sharath Thirunagaru", "abstract": "Pneumonia remains a leading cause of morbidity and mortality worldwide. Chest\nX-ray (CXR) imaging is a fundamental diagnostic tool, but traditional analysis\nrelies on time-intensive expert evaluation. Recently, deep learning has shown\nimmense potential for automating pneumonia detection from CXRs. This paper\nexplores applying neural networks to improve CXR-based pneumonia diagnosis. We\ndeveloped a novel model fusing Convolution Neural networks (CNN) and Vision\nTransformer networks via model-level ensembling. Our fusion architecture\ncombines a ResNet34 variant and a Multi-Axis Vision Transformer small model.\nBoth base models are initialized with ImageNet pre-trained weights. The output\nlayers are removed, and features are combined using a flattening layer before\nfinal classification. Experiments used the Kaggle pediatric pneumonia dataset\ncontaining 1,341 normal and 3,875 pneumonia CXR images. We compared our model\nagainst standalone ResNet34, Vision Transformer, and Swin Transformer Tiny\nbaseline models using identical training procedures. Extensive data\naugmentation, Adam optimization, learning rate warmup, and decay were employed.\nThe fusion model achieved a state-of-the-art accuracy of 94.87%, surpassing the\nbaselines. We also attained excellent sensitivity, specificity, kappa score,\nand positive predictive value. Confusion matrix analysis confirms fewer\nmisclassifications. The ResNet34 and Vision Transformer combination enables\njointly learning robust features from CNNs and Transformer paradigms. This\nmodel-level ensemble technique effectively integrates their complementary\nstrengths for enhanced pneumonia classification.", "published": "2024-01-04", "categories": ["eess.IV", "cs.CV"], "links": "http://arxiv.org/abs/2401.02358v1"}
{"title": "Fit-NGP: Fitting Object Models to Neural Graphics Primitives", "author": "Marwan Taher, Ignacio Alzugaray, Andrew J. Davison", "abstract": "Accurate 3D object pose estimation is key to enabling many robotic\napplications that involve challenging object interactions. In this work, we\nshow that the density field created by a state-of-the-art efficient radiance\nfield reconstruction method is suitable for highly accurate and robust pose\nestimation for objects with known 3D models, even when they are very small and\nwith challenging reflective surfaces. We present a fully automatic object pose\nestimation system based on a robot arm with a single wrist-mounted camera,\nwhich can scan a scene from scratch, detect and estimate the 6-Degrees of\nFreedom (DoF) poses of multiple objects within a couple of minutes of\noperation. Small objects such as bolts and nuts are estimated with accuracy on\norder of 1mm.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02357v1"}
{"title": "Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training", "author": "Longtian Qiu, Shan Ning, Xuming He", "abstract": "Image captioning aims at generating descriptive and meaningful textual\ndescriptions of images, enabling a broad range of vision-language applications.\nPrior works have demonstrated that harnessing the power of Contrastive Image\nLanguage Pre-training (CLIP) offers a promising approach to achieving zero-shot\ncaptioning, eliminating the need for expensive caption annotations. However,\nthe widely observed modality gap in the latent space of CLIP harms the\nperformance of zero-shot captioning by breaking the alignment between paired\nimage-text features. To address this issue, we conduct an analysis on the CLIP\nlatent space which leads to two findings. Firstly, we observe that the CLIP's\nvisual feature of image subregions can achieve closer proximity to the paired\ncaption due to the inherent information loss in text descriptions. In addition,\nwe show that the modality gap between a paired image-text can be empirically\nmodeled as a zero-mean Gaussian distribution. Motivated by the findings, we\npropose a novel zero-shot image captioning framework with text-only training to\nreduce the modality gap. In particular, we introduce a subregion feature\naggregation to leverage local region information, which produces a compact\nvisual representation for matching text representation. Moreover, we\nincorporate a noise injection and CLIP reranking strategy to boost captioning\nperformance. We also extend our framework to build a zero-shot VQA pipeline,\ndemonstrating its generality. Through extensive experiments on common\ncaptioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that\nour method achieves remarkable performance improvements. Code is available at\nhttps://github.com/Artanic30/MacCap.", "published": "2024-01-04", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.02347v1"}
{"title": "Linguistic Profiling of Deepfakes: An Open Database for Next-Generation Deepfake Detection", "author": "Yabin Wang, Zhiwu Huang, Zhiheng Ma, Xiaopeng Hong", "abstract": "The emergence of text-to-image generative models has revolutionized the field\nof deepfakes, enabling the creation of realistic and convincing visual content\ndirectly from textual descriptions. However, this advancement presents\nconsiderably greater challenges in detecting the authenticity of such content.\nExisting deepfake detection datasets and methods often fall short in\neffectively capturing the extensive range of emerging deepfakes and offering\nsatisfactory explanatory information for detection. To address the significant\nissue, this paper introduces a deepfake database (DFLIP-3K) for the development\nof convincing and explainable deepfake detection. It encompasses about 300K\ndiverse deepfake samples from approximately 3K generative models, which boasts\nthe largest number of deepfake models in the literature. Moreover, it collects\naround 190K linguistic footprints of these deepfakes. The two distinguished\nfeatures enable DFLIP-3K to develop a benchmark that promotes progress in\nlinguistic profiling of deepfakes, which includes three sub-tasks namely\ndeepfake detection, model identification, and prompt prediction. The deepfake\nmodel and prompt are two essential components of each deepfake, and thus\ndissecting them linguistically allows for an invaluable exploration of\ntrustworthy and interpretable evidence in deepfake detection, which we believe\nis the key for the next-generation deepfake detection. Furthermore, DFLIP-3K is\nenvisioned as an open database that fosters transparency and encourages\ncollaborative efforts to further enhance its growth. Our extensive experiments\non the developed benchmark verify that our DFLIP-3K database is capable of\nserving as a standardized resource for evaluating and comparing\nlinguistic-based deepfake detection, identification, and prompt prediction\ntechniques.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02335v1"}
{"title": "LLaVA-$φ$: Efficient Multi-Modal Assistant with Small Language Model", "author": "Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, Jian Tang", "abstract": "In this paper, we introduce LLaVA-$\\phi$ (LLaVA-Phi), an efficient\nmulti-modal assistant that harnesses the power of the recently advanced small\nlanguage model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a\nnotable advancement in the realm of compact multi-modal models. It demonstrates\nthat even smaller language models, with as few as 2.7B parameters, can\neffectively engage in intricate dialogues that integrate both textual and\nvisual elements, provided they are trained with high-quality corpora. Our model\ndelivers commendable performance on publicly available benchmarks that\nencompass visual comprehension, reasoning, and knowledge-based perception.\nBeyond its remarkable performance in multi-modal dialogue tasks, our model\nopens new avenues for applications in time-sensitive environments and systems\nthat require real-time interaction, such as embodied agents. It highlights the\npotential of smaller language models to achieve sophisticated levels of\nunderstanding and interaction, while maintaining greater resource\nefficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.", "published": "2024-01-04", "categories": ["cs.CV", "cs.CL"], "links": "http://arxiv.org/abs/2401.02330v1"}
{"title": "ClassWise-SAM-Adapter: Parameter Efficient Fine-tuning Adapts Segment Anything to SAR Domain for Semantic Segmentation", "author": "Xinyang Pu, Hecheng Jia, Linghao Zheng, Feng Wang, Feng Xu", "abstract": "In the realm of artificial intelligence, the emergence of foundation models,\nbacked by high computing capabilities and extensive data, has been\nrevolutionary. Segment Anything Model (SAM), built on the Vision Transformer\n(ViT) model with millions of parameters and vast training dataset SA-1B, excels\nin various segmentation scenarios relying on its significance of semantic\ninformation and generalization ability. Such achievement of visual foundation\nmodel stimulates continuous researches on specific downstream tasks in computer\nvision. The ClassWise-SAM-Adapter (CWSAM) is designed to adapt the\nhigh-performing SAM for landcover classification on space-borne Synthetic\nAperture Radar (SAR) images. The proposed CWSAM freezes most of SAM's\nparameters and incorporates lightweight adapters for parameter efficient\nfine-tuning, and a classwise mask decoder is designed to achieve semantic\nsegmentation task. This adapt-tuning method allows for efficient landcover\nclassification of SAR images, balancing the accuracy with computational demand.\nIn addition, the task specific input module injects low frequency information\nof SAR images by MLP-based layers to improve the model performance. Compared to\nconventional state-of-the-art semantic segmentation algorithms by extensive\nexperiments, CWSAM showcases enhanced performance with fewer computing\nresources, highlighting the potential of leveraging foundational models like\nSAM for specific downstream tasks in the SAR domain. The source code is\navailable at: https://github.com/xypu98/CWSAM.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02326v1"}
{"title": "BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model", "author": "Yiran Song, Qianyu Zhou, Xiangtai Li, Deng-Ping Fan, Xuequan Lu, Lizhuang Ma", "abstract": "In this paper, we address the challenge of image resolution variation for the\nSegment Anything Model (SAM). SAM, known for its zero-shot generalizability,\nexhibits a performance degradation when faced with datasets with varying image\nsizes. Previous approaches tend to resize the image to a fixed size or adopt\nstructure modifications, hindering the preservation of SAM's rich prior\nknowledge. Besides, such task-specific tuning necessitates a complete\nretraining of the model, which is cost-expensive and unacceptable for\ndeployment in the downstream tasks. In this paper, we reformulate this issue as\na length extrapolation problem, where token sequence length varies while\nmaintaining a consistent patch size for images of different sizes. To this end,\nwe propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM's\nadaptability to varying image resolutions while eliminating the need for\nstructure modifications. Firstly, we introduce a new scaling factor to ensure\nconsistent magnitude in the attention layer's dot product values when the token\nsequence length changes. Secondly, we present a bias-mode attention mask that\nallows each token to prioritize neighboring information, mitigating the impact\nof untrained distant information. Our BA-SAM demonstrates efficacy in two\nscenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets,\nincluding DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to\nsignificantly mitigate performance degradation in the zero-shot setting and\nachieve state-of-the-art performance with minimal fine-tuning. Furthermore, we\npropose a generalized model and benchmark, showcasing BA-SAM's generalizability\nacross all four datasets simultaneously.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02317v1"}
{"title": "SuperEdge: Towards a Generalization Model for Self-Supervised Edge Detection", "author": "Leng Kai, Zhang Zhijie, Liu Jie, Zed Boukhers, Sui Wei, Cong Yang, Li Zhijun", "abstract": "Edge detection is a fundamental technique in various computer vision tasks.\nEdges are indeed effectively delineated by pixel discontinuity and can offer\nreliable structural information even in textureless areas. State-of-the-art\nheavily relies on pixel-wise annotations, which are labor-intensive and subject\nto inconsistencies when acquired manually. In this work, we propose a novel\nself-supervised approach for edge detection that employs a multi-level,\nmulti-homography technique to transfer annotations from synthetic to real-world\ndatasets. To fully leverage the generated edge annotations, we developed\nSuperEdge, a streamlined yet efficient model capable of concurrently extracting\nedges at pixel-level and object-level granularity. Thanks to self-supervised\ntraining, our method eliminates the dependency on manual annotated edge labels,\nthereby enhancing its generalizability across diverse datasets. Comparative\nevaluations reveal that SuperEdge advances edge detection, demonstrating\nimprovements of 4.9% in ODS and 3.3% in OIS over the existing STEdge method on\nBIPEDv2.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02313v1"}
{"title": "TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection", "author": "Hao Sun, Mingyao Zhou, Wenjing Chen, Wei Xie", "abstract": "Video moment retrieval (MR) and highlight detection (HD) based on natural\nlanguage queries are two highly related tasks, which aim to obtain relevant\nmoments within videos and highlight scores of each video clip. Recently,\nseveral methods have been devoted to building DETR-based networks to solve both\nMR and HD jointly. These methods simply add two separate task heads after\nmulti-modal feature extraction and feature interaction, achieving good\nperformance. Nevertheless, these approaches underutilize the reciprocal\nrelationship between two tasks. In this paper, we propose a task-reciprocal\ntransformer based on DETR (TR-DETR) that focuses on exploring the inherent\nreciprocity between MR and HD. Specifically, a local-global multi-modal\nalignment module is first built to align features from diverse modalities into\na shared latent space. Subsequently, a visual feature refinement is designed to\neliminate query-irrelevant information from visual features for modal\ninteraction. Finally, a task cooperation module is constructed to refine the\nretrieval pipeline and the highlight score prediction process by utilizing the\nreciprocity between MR and HD. Comprehensive experiments on QVHighlights,\nCharades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing\nstate-of-the-art methods. Codes are available at\n\\url{https://github.com/mingyao1120/TR-DETR}.", "published": "2024-01-04", "categories": ["cs.CV", "cs.MM"], "links": "http://arxiv.org/abs/2401.02309v1"}
{"title": "GridFormer: Point-Grid Transformer for Surface Reconstruction", "author": "Shengtao Li, Ge Gao, Yudong Liu, Yu-Shen Liu, Ming Gu", "abstract": "Implicit neural networks have emerged as a crucial technology in 3D surface\nreconstruction. To reconstruct continuous surfaces from discrete point clouds,\nencoding the input points into regular grid features (plane or volume) has been\ncommonly employed in existing approaches. However, these methods typically use\nthe grid as an index for uniformly scattering point features. Compared with the\nirregular point features, the regular grid features may sacrifice some\nreconstruction details but improve efficiency. To take full advantage of these\ntwo types of features, we introduce a novel and high-efficiency attention\nmechanism between the grid and point features named Point-Grid Transformer\n(GridFormer). This mechanism treats the grid as a transfer point connecting the\nspace and point cloud. Our method maximizes the spatial expressiveness of grid\nfeatures and maintains computational efficiency. Furthermore, optimizing\npredictions over the entire space could potentially result in blurred\nboundaries. To address this issue, we further propose a boundary optimization\nstrategy incorporating margin binary cross-entropy loss and boundary sampling.\nThis approach enables us to achieve a more precise representation of the object\nstructure. Our experiments validate that our method is effective and\noutperforms the state-of-the-art approaches under widely used benchmarks by\nproducing more precise geometry reconstructions. The code is available at\nhttps://github.com/list17/GridFormer.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02292v1"}
{"title": "Distillation-based fabric anomaly detection", "author": "Simon Thomine, Hichem Snoussi", "abstract": "Unsupervised texture anomaly detection has been a concerning topic in a vast\namount of industrial processes. Patterned textures inspection, particularly in\nthe context of fabric defect detection, is indeed a widely encountered use\ncase. This task involves handling a diverse spectrum of colors and textile\ntypes, encompassing a wide range of fabrics. Given the extensive variability in\ncolors, textures, and defect types, fabric defect detection poses a complex and\nchallenging problem in the field of patterned textures inspection. In this\narticle, we propose a knowledge distillation-based approach tailored\nspecifically for addressing the challenge of unsupervised anomaly detection in\ntextures resembling fabrics. Our method aims to redefine the recently\nintroduced reverse distillation approach, which advocates for an\nencoder-decoder design to mitigate classifier bias and to prevent the student\nfrom reconstructing anomalies. In this study, we present a new reverse\ndistillation technique for the specific task of fabric defect detection. Our\napproach involves a meticulous design selection that strategically highlights\nhigh-level features. To demonstrate the capabilities of our approach both in\nterms of performance and inference speed, we conducted a series of experiments\non multiple texture datasets, including MVTEC AD, AITEX, and TILDA, alongside\nconducting experiments on a dataset acquired from a textile manufacturing\nfacility. The main contributions of this paper are the following: a robust\ntexture anomaly detector utilizing a reverse knowledge-distillation technique\nsuitable for both anomaly detection and domain generalization and a novel\ndataset encompassing a diverse range of fabrics and defects.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02287v1"}
{"title": "PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for 6DOF Object Pose Dataset Generation", "author": "Lukas Meyer, Floris Erich, Yusuke Yoshiyasu, Marc Stamminger, Noriaki Ando, Yukiyasu Domae", "abstract": "We introduce Physically Enhanced Gaussian Splatting Simulation System\n(PEGASUS) for 6DOF object pose dataset generation, a versatile dataset\ngenerator based on 3D Gaussian Splatting. Environment and object\nrepresentations can be easily obtained using commodity cameras to reconstruct\nwith Gaussian Splatting. PEGASUS allows the composition of new scenes by\nmerging the respective underlying Gaussian Splatting point cloud of an\nenvironment with one or multiple objects. Leveraging a physics engine enables\nthe simulation of natural object placement within a scene through interaction\nbetween meshes extracted for the objects and the environment. Consequently, an\nextensive amount of new scenes - static or dynamic - can be created by\ncombining different environments and objects. By rendering scenes from various\nperspectives, diverse data points such as RGB images, depth maps, semantic\nmasks, and 6DoF object poses can be extracted. Our study demonstrates that\ntraining on data generated by PEGASUS enables pose estimation networks to\nsuccessfully transfer from synthetic data to real-world data. Moreover, we\nintroduce the Ramen dataset, comprising 30 Japanese cup noodle items. This\ndataset includes spherical scans that captures images from both object\nhemisphere and the Gaussian Splatting reconstruction, making them compatible\nwith PEGASUS.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02281v1"}
{"title": "Lightweight Fish Classification Model for Sustainable Marine Management: Indonesian Case", "author": "Febrian Kurniawan, Gandeva Bayu Satrya, Firuz Kamalov", "abstract": "The enormous demand for seafood products has led to exploitation of marine\nresources and near-extinction of some species. In particular, overfishing is\none the main issues in sustainable marine development. In alignment with the\nprotection of marine resources and sustainable fishing, this study proposes to\nadvance fish classification techniques that support identifying protected fish\nspecies using state-of-the-art machine learning. We use a custom modification\nof the MobileNet model to design a lightweight classifier called M-MobileNet\nthat is capable of running on limited hardware. As part of the study, we\ncompiled a labeled dataset of 37,462 images of fish found in the waters of the\nIndonesian archipelago. The proposed model is trained on the dataset to\nclassify images of the captured fish into their species and give\nrecommendations on whether they are consumable or not. Our modified MobileNet\nmodel uses only 50\\% of the top layer parameters with about 42% GTX 860M\nutility and achieves up to 97% accuracy in fish classification and determining\nits consumability. Given the limited computing capacity available on many\nfishing vessels, the proposed model provides a practical solution to on-site\nfish classification. In addition, synchronized implementation of the proposed\nmodel on multiple vessels can supply valuable information about the movement\nand location of different species of fish.", "published": "2024-01-04", "categories": ["cs.CV", "cs.LG"], "links": "http://arxiv.org/abs/2401.02278v1"}
{"title": "ShapeAug: Occlusion Augmentation for Event Camera Data", "author": "Katharina Bendig, René Schuster, Didier Stricker", "abstract": "Recently, Dynamic Vision Sensors (DVSs) sparked a lot of interest due to\ntheir inherent advantages over conventional RGB cameras. These advantages\ninclude a low latency, a high dynamic range and a low energy consumption.\nNevertheless, the processing of DVS data using Deep Learning (DL) methods\nremains a challenge, particularly since the availability of event training data\nis still limited. This leads to a need for event data augmentation techniques\nin order to improve accuracy as well as to avoid over-fitting on the training\ndata. Another challenge especially in real world automotive applications is\nocclusion, meaning one object is hindering the view onto the object behind it.\nIn this paper, we present a novel event data augmentation approach, which\naddresses this problem by introducing synthetic events for randomly moving\nobjects in a scene. We test our method on multiple DVS classification datasets,\nresulting in an relative improvement of up to 6.5 % in top1-accuracy. Moreover,\nwe apply our augmentation technique on the real world Gen1 Automotive Event\nDataset for object detection, where we especially improve the detection of\npedestrians by up to 5 %.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02274v1"}
{"title": "Slot-guided Volumetric Object Radiance Fields", "author": "Di Qi, Tong Yang, Xiangyu Zhang", "abstract": "We present a novel framework for 3D object-centric representation learning.\nOur approach effectively decomposes complex scenes into individual objects from\na single image in an unsupervised fashion. This method, called slot-guided\nVolumetric Object Radiance Fields (sVORF), composes volumetric object radiance\nfields with object slots as a guidance to implement unsupervised 3D scene\ndecomposition. Specifically, sVORF obtains object slots from a single image via\na transformer module, maps these slots to volumetric object radiance fields\nwith a hypernetwork and composes object radiance fields with the guidance of\nobject slots at a 3D location. Moreover, sVORF significantly reduces memory\nrequirement due to small-sized pixel rendering during training. We demonstrate\nthe effectiveness of our approach by showing top results in scene decomposition\nand generation tasks of complex synthetic datasets (e.g., Room-Diverse).\nFurthermore, we also confirm the potential of sVORF to segment objects in\nreal-world scenes (e.g., the LLFF dataset). We hope our approach can provide\npreliminary understanding of the physical world and help ease future research\nin 3D object-centric representation learning.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02241v1"}
{"title": "Nodule detection and generation on chest X-rays: NODE21 Challenge", "author": "Ecem Sogancioglu, Bram van Ginneken, Finn Behrendt, Marcel Bengs, Alexander Schlaefer, Miron Radu, Di Xu, Ke Sheng, Fabien Scalzo, Eric Marcus, Samuele Papa, Jonas Teuwen, Ernst Th. Scholten, Steven Schalekamp, Nils Hendrix, Colin Jacobs, Ward Hendrix, Clara I Sánchez, Keelin Murphy", "abstract": "Pulmonary nodules may be an early manifestation of lung cancer, the leading\ncause of cancer-related deaths among both men and women. Numerous studies have\nestablished that deep learning methods can yield high-performance levels in the\ndetection of lung nodules in chest X-rays. However, the lack of gold-standard\npublic datasets slows down the progression of the research and prevents\nbenchmarking of methods for this task. To address this, we organized a public\nresearch challenge, NODE21, aimed at the detection and generation of lung\nnodules in chest X-rays. While the detection track assesses state-of-the-art\nnodule detection systems, the generation track determines the utility of nodule\ngeneration algorithms to augment training data and hence improve the\nperformance of the detection systems. This paper summarizes the results of the\nNODE21 challenge and performs extensive additional experiments to examine the\nimpact of the synthetically generated nodule training images on the detection\nalgorithm performance.", "published": "2024-01-04", "categories": ["eess.IV", "cs.CV", "cs.LG"], "links": "http://arxiv.org/abs/2401.02192v1"}
{"title": "Prompt Decoupling for Text-to-Image Person Re-identification", "author": "Weihao Li, Lei Tan, Pingyang Dai, Yan Zhang", "abstract": "Text-to-image person re-identification (TIReID) aims to retrieve the target\nperson from an image gallery via a textual description query. Recently,\npre-trained vision-language models like CLIP have attracted significant\nattention and have been widely utilized for this task due to their robust\ncapacity for semantic concept learning and rich multi-modal knowledge. However,\nrecent CLIP-based TIReID methods commonly rely on direct fine-tuning of the\nentire network to adapt the CLIP model for the TIReID task. Although these\nmethods show competitive performance on this topic, they are suboptimal as they\nnecessitate simultaneous domain adaptation and task adaptation. To address this\nissue, we attempt to decouple these two processes during the training stage.\nSpecifically, we introduce the prompt tuning strategy to enable domain\nadaptation and propose a two-stage training approach to disentangle domain\nadaptation from task adaptation. In the first stage, we freeze the two encoders\nfrom CLIP and solely focus on optimizing the prompts to alleviate domain gap\nbetween the original training data of CLIP and downstream tasks. In the second\nstage, we maintain the fixed prompts and fine-tune the CLIP model to prioritize\ncapturing fine-grained information, which is more suitable for TIReID task.\nFinally, we evaluate the effectiveness of our method on three widely used\ndatasets. Compared to the directly fine-tuned approach, our method achieves\nsignificant improvements.", "published": "2024-01-04", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.02173v1"}
{"title": "Frequency Domain Nuances Mining for Visible-Infrared Person Re-identification", "author": "Yukang Zhang, Yang Lu, Yan Yan, Hanzi Wang, Xuelong Li", "abstract": "The key of visible-infrared person re-identification (VIReID) lies in how to\nminimize the modality discrepancy between visible and infrared images. Existing\nmethods mainly exploit the spatial information while ignoring the\ndiscriminative frequency information. To address this issue, this paper aims to\nreduce the modality discrepancy from the frequency domain perspective.\nSpecifically, we propose a novel Frequency Domain Nuances Mining (FDNM) method\nto explore the cross-modality frequency domain information, which mainly\nincludes an amplitude guided phase (AGP) module and an amplitude nuances mining\n(ANM) module. These two modules are mutually beneficial to jointly explore\nfrequency domain visible-infrared nuances, thereby effectively reducing the\nmodality discrepancy in the frequency domain. Besides, we propose a\ncenter-guided nuances mining loss to encourage the ANM module to preserve\ndiscriminative identity information while discovering diverse cross-modality\nnuances. To the best of our knowledge, this is the first work that explores the\npotential frequency information for VIReID research. Extensive experiments show\nthat the proposed FDNM has significant advantages in improving the performance\nof VIReID. Specifically, our method outperforms the second-best method by 5.2\\%\nin Rank-1 accuracy and 5.8\\% in mAP on the SYSU-MM01 dataset under the indoor\nsearch mode, respectively. Besides, we also validate the effectiveness and\ngeneralization of our method on the challenging visible-infrared face\nrecognition task. \\textcolor{magenta}{The code will be available.}", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02162v1"}
{"title": "Enhancing RAW-to-sRGB with Decoupled Style Structure in Fourier Domain", "author": "Xuanhua He, Tao Hu, Guoli Wang, Zejin Wang, Run Wang, Qian Zhang, Keyu Yan, Ziyi Chen, Rui Li, Chenjun Xie, Jie Zhang, Man Zhou", "abstract": "RAW to sRGB mapping, which aims to convert RAW images from smartphones into\nRGB form equivalent to that of Digital Single-Lens Reflex (DSLR) cameras, has\nbecome an important area of research. However, current methods often ignore the\ndifference between cell phone RAW images and DSLR camera RGB images, a\ndifference that goes beyond the color matrix and extends to spatial structure\ndue to resolution variations. Recent methods directly rebuild color mapping and\nspatial structure via shared deep representation, limiting optimal performance.\nInspired by Image Signal Processing (ISP) pipeline, which distinguishes image\nrestoration and enhancement, we present a novel Neural ISP framework, named\nFourierISP. This approach breaks the image down into style and structure within\nthe frequency domain, allowing for independent optimization. FourierISP is\ncomprised of three subnetworks: Phase Enhance Subnet for structural refinement,\nAmplitude Refine Subnet for color learning, and Color Adaptation Subnet for\nblending them in a smooth manner. This approach sharpens both color and\nstructure, and extensive evaluations across varied datasets confirm that our\napproach realizes state-of-the-art results. Code will be available at\n~\\url{https://github.com/alexhe101/FourierISP}.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02161v1"}
{"title": "Frequency-Adaptive Pan-Sharpening with Mixture of Experts", "author": "Xuanhua He, Keyu Yan, Rui Li, Chengjun Xie, Jie Zhang, Man Zhou", "abstract": "Pan-sharpening involves reconstructing missing high-frequency information in\nmulti-spectral images with low spatial resolution, using a higher-resolution\npanchromatic image as guidance. Although the inborn connection with frequency\ndomain, existing pan-sharpening research has not almost investigated the\npotential solution upon frequency domain. To this end, we propose a novel\nFrequency Adaptive Mixture of Experts (FAME) learning framework for\npan-sharpening, which consists of three key components: the Adaptive Frequency\nSeparation Prediction Module, the Sub-Frequency Learning Expert Module, and the\nExpert Mixture Module. In detail, the first leverages the discrete cosine\ntransform to perform frequency separation by predicting the frequency mask. On\nthe basis of generated mask, the second with low-frequency MOE and\nhigh-frequency MOE takes account for enabling the effective low-frequency and\nhigh-frequency information reconstruction. Followed by, the final fusion module\ndynamically weights high-frequency and low-frequency MOE knowledge to adapt to\nremote sensing images with significant content variations. Quantitative and\nqualitative experiments over multiple datasets demonstrate that our method\nperforms the best against other state-of-the-art ones and comprises a strong\ngeneralization ability for real-world scenes. Code will be made publicly at\n\\url{https://github.com/alexhe101/FAME-Net}.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02151v1"}
{"title": "Marginal Debiased Network for Fair Visual Recognition", "author": "Mei Wang, Weihong Deng, Sen Su", "abstract": "Deep neural networks (DNNs) are often prone to learn the spurious\ncorrelations between target classes and bias attributes, like gender and race,\ninherent in a major portion of training data (bias-aligned samples), thus\nshowing unfair behavior and arising controversy in the modern pluralistic and\negalitarian society. In this paper, we propose a novel marginal debiased\nnetwork (MDN) to learn debiased representations. More specifically, a marginal\nsoftmax loss (MSL) is designed by introducing the idea of margin penalty into\nthe fairness problem, which assigns a larger margin for bias-conflicting\nsamples (data without spurious correlations) than for bias-aligned ones, so as\nto deemphasize the spurious correlations and improve generalization on unbiased\ntest criteria. To determine the margins, our MDN is optimized through a meta\nlearning framework. We propose a meta equalized loss (MEL) to perceive the\nmodel fairness, and adaptively update the margin parameters by metaoptimization\nwhich requires the trained model guided by the optimal margins should minimize\nMEL computed on an unbiased meta-validation set. Extensive experiments on\nBiasedMNIST, Corrupted CIFAR-10, CelebA and UTK-Face datasets demonstrate that\nour MDN can achieve a remarkable performance on under-represented samples and\nobtain superior debiased results against the previous approaches.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02150v1"}
{"title": "Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case Study", "author": "Ziqiang Zheng, Yiwei Chen, Jipeng Zhang, Tuan-Anh Vu, Huimin Zeng, Yue Him Wong Tim, Sai-Kit Yeung", "abstract": "Large language models (LLMs) have demonstrated a powerful ability to answer\nvarious queries as a general-purpose assistant. The continuous multi-modal\nlarge language models (MLLM) empower LLMs with the ability to perceive visual\nsignals. The launch of GPT-4 (Generative Pre-trained Transformers) has\ngenerated significant interest in the research communities. GPT-4V(ison) has\ndemonstrated significant power in both academia and industry fields, as a focal\npoint in a new artificial intelligence generation. Though significant success\nwas achieved by GPT-4V, exploring MLLMs in domain-specific analysis (e.g.,\nmarine analysis) that required domain-specific knowledge and expertise has\ngained less attention. In this study, we carry out the preliminary and\ncomprehensive case study of utilizing GPT-4V for marine analysis. This report\nconducts a systematic evaluation of existing GPT-4V, assessing the performance\nof GPT-4V on marine research and also setting a new standard for future\ndevelopments in MLLMs. The experimental results of GPT-4V show that the\nresponses generated by GPT-4V are still far away from satisfying the\ndomain-specific requirements of the marine professions. All images and prompts\nused in this study will be available at\nhttps://github.com/hkust-vgd/Marine_GPT-4V_Eval", "published": "2024-01-04", "categories": ["cs.CL", "cs.CV"], "links": "http://arxiv.org/abs/2401.02147v1"}
{"title": "GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion Generation", "author": "Xuehao Gao, Yang Yang, Zhenyu Xie, Shaoyi Du, Zhongqian Sun, Yang Wu", "abstract": "In this paper, we propose a novel cascaded diffusion-based generative\nframework for text-driven human motion synthesis, which exploits a strategy\nnamed GradUally Enriching SyntheSis (GUESS as its abbreviation). The strategy\nsets up generation objectives by grouping body joints of detailed skeletons in\nclose semantic proximity together and then replacing each of such joint group\nwith a single body-part node. Such an operation recursively abstracts a human\npose to coarser and coarser skeletons at multiple granularity levels. With\ngradually increasing the abstraction level, human motion becomes more and more\nconcise and stable, significantly benefiting the cross-modal motion synthesis\ntask. The whole text-driven human motion synthesis problem is then divided into\nmultiple abstraction levels and solved with a multi-stage generation framework\nwith a cascaded latent diffusion model: an initial generator first generates\nthe coarsest human motion guess from a given text description; then, a series\nof successive generators gradually enrich the motion details based on the\ntextual description and the previous synthesized results. Notably, we further\nintegrate GUESS with the proposed dynamic multi-condition fusion mechanism to\ndynamically balance the cooperative effects of the given textual condition and\nsynthesized coarse motion prompt in different generation stages. Extensive\nexperiments on large-scale datasets verify that GUESS outperforms existing\nstate-of-the-art methods by large margins in terms of accuracy, realisticness,\nand diversity. Code is available at https://github.com/Xuehao-Gao/GUESS.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02142v1"}
{"title": "Bayesian Intrinsic Groupwise Image Registration: Unsupervised Disentanglement of Anatomy and Geometry", "author": "Xinzhe Luo, Xin Wang, Linda Shapiro, Chun Yuan, Jianfeng Feng, Xiahai Zhuang", "abstract": "This article presents a general Bayesian learning framework for multi-modal\ngroupwise registration on medical images. The method builds on probabilistic\nmodelling of the image generative process, where the underlying common anatomy\nand geometric variations of the observed images are explicitly disentangled as\nlatent variables. Thus, groupwise registration is achieved through the solution\nto Bayesian inference. We propose a novel hierarchical variational\nauto-encoding architecture to realize the inference procedure of the latent\nvariables, where the registration parameters can be calculated in a\nmathematically interpretable fashion. Remarkably, this new paradigm can learn\ngroupwise registration in an unsupervised closed-loop self-reconstruction\nprocess, sparing the burden of designing complex intensity-based similarity\nmeasures. The computationally efficient disentangled architecture is also\ninherently scalable and flexible, allowing for groupwise registration on\nlarge-scale image groups with variable sizes. Furthermore, the inferred\nstructural representations from disentanglement learning are capable of\ncapturing the latent anatomy of the observations with visual semantics.\nExtensive experiments were conducted to validate the proposed framework,\nincluding four datasets from cardiac, brain and abdominal medical images. The\nresults have demonstrated the superiority of our method over conventional\nsimilarity-based approaches in terms of accuracy, efficiency, scalability and\ninterpretability.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02141v1"}
{"title": "Explore Human Parsing Modality for Action Recognition", "author": "Jinfu Liu, Runwei Ding, Yuhang Wen, Nan Dai, Fanyang Meng, Shen Zhao, Mengyuan Liu", "abstract": "Multimodal-based action recognition methods have achieved high success using\npose and RGB modality. However, skeletons sequences lack appearance depiction\nand RGB images suffer irrelevant noise due to modality limitations. To address\nthis, we introduce human parsing feature map as a novel modality, since it can\nselectively retain effective semantic features of the body parts, while\nfiltering out most irrelevant noise. We propose a new dual-branch framework\ncalled Ensemble Human Parsing and Pose Network (EPP-Net), which is the first to\nleverage both skeletons and human parsing modalities for action recognition.\nThe first human pose branch feeds robust skeletons in graph convolutional\nnetwork to model pose features, while the second human parsing branch also\nleverages depictive parsing feature maps to model parsing festures via\nconvolutional backbones. The two high-level features will be effectively\ncombined through a late fusion strategy for better action recognition.\nExtensive experiments on NTU RGB+D and NTU RGB+D 120 benchmarks consistently\nverify the effectiveness of our proposed EPP-Net, which outperforms the\nexisting action recognition methods. Our code is available at:\nhttps://github.com/liujf69/EPP-Net-Action.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02138v1"}
{"title": "SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment", "author": "Ziping Ma, Furong Xu, Jian Liu, Ming Yang, Qingpei Guo", "abstract": "Multimodal alignment between language and vision is the fundamental topic in\ncurrent vision-language model research. Contrastive Captioners (CoCa), as a\nrepresentative method, integrates Contrastive Language-Image Pretraining (CLIP)\nand Image Caption (IC) into a unified framework, resulting in impressive\nresults. CLIP imposes a bidirectional constraints on global representation of\nentire images and sentences. Although IC conducts an unidirectional\nimage-to-text generation on local representation, it lacks any constraint on\nlocal text-to-image reconstruction, which limits the ability to understand\nimages at a fine-grained level when aligned with texts. To achieve multimodal\nalignment from both global and local perspectives, this paper proposes\nSymmetrizing Contrastive Captioners (SyCoCa), which introduces bidirectional\ninteractions on images and texts across the global and local representation\nlevels. Specifically, we expand a Text-Guided Masked Image Modeling (TG-MIM)\nhead based on ITC and IC heads. The improved SyCoCa can further leverage\ntextual cues to reconstruct contextual images and visual cues to predict\ntextual contents. When implementing bidirectional local interactions, the local\ncontents of images tend to be cluttered or unrelated to their textual\ndescriptions. Thus, we employ an attentive masking strategy to select effective\nimage patches for interaction. Extensive experiments on five vision-language\ntasks, including image-text retrieval, image-captioning, visual question\nanswering, and zero-shot/finetuned image classification, validate the\neffectiveness of our proposed method.", "published": "2024-01-04", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.02137v1"}
{"title": "Unified Diffusion-Based Rigid and Non-Rigid Editing with Text and Image Guidance", "author": "Jiacheng Wang, Ping Liu, Wei Xu", "abstract": "Existing text-to-image editing methods tend to excel either in rigid or\nnon-rigid editing but encounter challenges when combining both, resulting in\nmisaligned outputs with the provided text prompts. In addition, integrating\nreference images for control remains challenging. To address these issues, we\npresent a versatile image editing framework capable of executing both rigid and\nnon-rigid edits, guided by either textual prompts or reference images. We\nleverage a dual-path injection scheme to handle diverse editing scenarios and\nintroduce an integrated self-attention mechanism for fusion of appearance and\nstructural information. To mitigate potential visual artifacts, we further\nemploy latent fusion techniques to adjust intermediate latents. Compared to\nprevious work, our approach represents a significant advance in achieving\nprecise and versatile image editing. Comprehensive experiments validate the\nefficacy of our method, showcasing competitive or superior results in\ntext-based editing and appearance transfer tasks, encompassing both rigid and\nnon-rigid settings.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02126v1"}
{"title": "Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation", "author": "Zipeng Fu, Tony Z. Zhao, Chelsea Finn", "abstract": "Imitation learning from human demonstrations has shown impressive performance\nin robotics. However, most results focus on table-top manipulation, lacking the\nmobility and dexterity necessary for generally useful tasks. In this work, we\ndevelop a system for imitating mobile manipulation tasks that are bimanual and\nrequire whole-body control. We first present Mobile ALOHA, a low-cost and\nwhole-body teleoperation system for data collection. It augments the ALOHA\nsystem with a mobile base, and a whole-body teleoperation interface. Using data\ncollected with Mobile ALOHA, we then perform supervised behavior cloning and\nfind that co-training with existing static ALOHA datasets boosts performance on\nmobile manipulation tasks. With 50 demonstrations for each task, co-training\ncan increase success rates by up to 90%, allowing Mobile ALOHA to autonomously\ncomplete complex mobile manipulation tasks such as sauteing and serving a piece\nof shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling\nand entering an elevator, and lightly rinsing a used pan using a kitchen\nfaucet. Project website: https://mobile-aloha.github.io", "published": "2024-01-04", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "links": "http://arxiv.org/abs/2401.02117v1"}
{"title": "Source-Free Online Domain Adaptive Semantic Segmentation of Satellite Images under Image Degradation", "author": "Fahim Faisal Niloy, Kishor Kumar Bhaumik, Simon S. Woo", "abstract": "Online adaptation to distribution shifts in satellite image segmentation\nstands as a crucial yet underexplored problem. In this paper, we address\nsource-free and online domain adaptation, i.e., test-time adaptation (TTA), for\nsatellite images, with the focus on mitigating distribution shifts caused by\nvarious forms of image degradation. Towards achieving this goal, we propose a\nnovel TTA approach involving two effective strategies. First, we progressively\nestimate the global Batch Normalization (BN) statistics of the target\ndistribution with incoming data stream. Leveraging these statistics during\ninference has the ability to effectively reduce domain gap. Furthermore, we\nenhance prediction quality by refining the predicted masks using global class\ncenters. Both strategies employ dynamic momentum for fast and stable\nconvergence. Notably, our method is backpropagation-free and hence fast and\nlightweight, making it highly suitable for on-the-fly adaptation to new domain.\nThrough comprehensive experiments across various domain adaptation scenarios,\nwe demonstrate the robust performance of our method.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02113v1"}
{"title": "Significance of Anatomical Constraints in Virtual Try-On", "author": "Debapriya Roy, Sanchayan Santra, Diganta Mukherjee, Bhabatosh Chanda", "abstract": "The system of Virtual Try-ON (VTON) allows a user to try a product virtually.\nIn general, a VTON system takes a clothing source and a person's image to\npredict the try-on output of the person in the given clothing. Although\nexisting methods perform well for simple poses, in case of bent or crossed arms\nposture or when there is a significant difference between the alignment of the\nsource clothing and the pose of the target person, these methods fail by\ngenerating inaccurate clothing deformations. In the VTON methods that employ\nThin Plate Spline (TPS) based clothing transformations, this mainly occurs for\ntwo reasons - (1)~the second-order smoothness constraint of TPS that restricts\nthe bending of the object plane. (2)~Overlaps among different clothing parts\n(e.g., sleeves and torso) can not be modeled by a single TPS transformation, as\nit assumes the clothing as a single planar object; therefore, disregards the\nindependence of movement of different clothing parts. To this end, we make two\nmajor contributions. Concerning the bending limitations of TPS, we propose a\nhuman AnaTomy-Aware Geometric (ATAG) transformation. Regarding the overlap\nissue, we propose a part-based warping approach that divides the clothing into\nindependently warpable parts to warp them separately and later combine them.\nExtensive analysis shows the efficacy of this approach.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02110v1"}
{"title": "CLAPP: Contrastive Language-Audio Pre-training in Passive Underwater Vessel Classification", "author": "Zeyu Li, Jingsheng Gao, Tong Yu, Suncheng Xiang, Jiacheng Ruan, Ting Liu, Yuzhuo Fu", "abstract": "Existing research on audio classification faces challenges in recognizing\nattributes of passive underwater vessel scenarios and lacks well-annotated\ndatasets due to data privacy concerns. In this study, we introduce CLAPP\n(Contrastive Language-Audio Pre-training in Passive Underwater Vessel\nClassification), a novel model. Our aim is to train a neural network using a\nwide range of vessel audio and vessel state text pairs obtained from an\noceanship dataset. CLAPP is capable of directly learning from raw vessel audio\ndata and, when available, from carefully curated labels, enabling improved\nrecognition of vessel attributes in passive underwater vessel scenarios.\nModel's zero-shot capability allows predicting the most relevant vessel state\ndescription for a given vessel audio, without directly optimizing for the task.\nOur approach aims to solve 2 challenges: vessel audio-text classification and\npassive underwater vessel audio attribute recognition. The proposed method\nachieves new state-of-the-art results on both Deepship and Shipsear public\ndatasets, with a notable margin of about 7%-13% for accuracy compared to prior\nmethods on zero-shot task.", "published": "2024-01-04", "categories": ["cs.CV", "cs.SD", "eess.AS"], "links": "http://arxiv.org/abs/2401.02099v1"}
{"title": "Preserving Image Properties Through Initializations in Diffusion Models", "author": "Jeffrey Zhang, Shao-Yu Chang, Kedan Li, David Forsyth", "abstract": "Retail photography imposes specific requirements on images. For instance,\nimages may need uniform background colors, consistent model poses, centered\nproducts, and consistent lighting. Minor deviations from these standards impact\na site's aesthetic appeal, making the images unsuitable for use. We show that\nStable Diffusion methods, as currently applied, do not respect these\nrequirements. The usual practice of training the denoiser with a very noisy\nimage and starting inference with a sample of pure noise leads to inconsistent\ngenerated images during inference. This inconsistency occurs because it is easy\nto tell the difference between samples of the training and inference\ndistributions. As a result, a network trained with centered retail product\nimages with uniform backgrounds generates images with erratic backgrounds. The\nproblem is easily fixed by initializing inference with samples from an\napproximation of noisy images. However, in using such an approximation, the\njoint distribution of text and noisy image at inference time still slightly\ndiffers from that at training time. This discrepancy is corrected by training\nthe network with samples from the approximate noisy image distribution.\nExtensive experiments on real application data show significant qualitative and\nquantitative improvements in performance from adopting these procedures.\nFinally, our procedure can interact well with other control-based methods to\nfurther enhance the controllability of diffusion-based methods.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02097v1"}
{"title": "Federated Class-Incremental Learning with Prototype Guided Transformer", "author": "Haiyang Guo, Fei Zhu, Wenzhuo Liu, Xu-Yao Zhang, Cheng-Lin Liu", "abstract": "Existing federated learning methods have effectively addressed decentralized\nlearning in scenarios involving data privacy and non-IID data. However, in\nreal-world situations, each client dynamically learns new classes, requiring\nthe global model to maintain discriminative capabilities for both new and old\nclasses. To effectively mitigate the effects of catastrophic forgetting and\ndata heterogeneity under low communication costs, we designed a simple and\neffective method named PLoRA. On the one hand, we adopt prototype learning to\nlearn better feature representations and leverage the heuristic information\nbetween prototypes and class features to design a prototype re-weight module to\nsolve the classifier bias caused by data heterogeneity without retraining the\nclassification layer. On the other hand, our approach utilizes a pre-trained\nmodel as the backbone and utilizes LoRA to fine-tune with a tiny amount of\nparameters when learning new classes. Moreover, PLoRA does not rely on\nsimilarity-based module selection strategies, thereby further reducing\ncommunication overhead. Experimental results on standard datasets indicate that\nour method outperforms the state-of-the-art approaches significantly. More\nimportantly, our method exhibits strong robustness and superiority in various\nscenarios and degrees of data heterogeneity. Our code will be publicly\navailable.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02094v1"}
{"title": "Leveraging SAM for Single-Source Domain Generalization in Medical Image Segmentation", "author": "Hanhui Wang, Huaize Ye, Yi Xia, Xueyan Zhang", "abstract": "Domain Generalization (DG) aims to reduce domain shifts between domains to\nachieve promising performance on the unseen target domain, which has been\nwidely practiced in medical image segmentation. Single-source domain\ngeneralization (SDG) is the most challenging setting that trains on only one\nsource domain. Although existing methods have made considerable progress on SDG\nof medical image segmentation, the performances are still far from the\napplicable standards when faced with a relatively large domain shift. In this\npaper, we leverage the Segment Anything Model (SAM) to SDG to greatly improve\nthe ability of generalization. Specifically, we introduce a parallel framework,\nthe source images are sent into the SAM module and normal segmentation module\nrespectively. To reduce the calculation resources, we apply a merging strategy\nbefore sending images to the SAM module. We extract the bounding boxes from the\nsegmentation module and send the refined version as prompts to the SAM module.\nWe evaluate our model on a classic DG dataset and achieve competitive results\ncompared to other state-of-the-art DG methods. Furthermore, We conducted a\nseries of ablation experiments to prove the effectiveness of the proposed\nmethod. The code is publicly available at https://github.com/SARIHUST/SAMMed.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02076v1"}
{"title": "Generalizable vision-language pre-training for annotation-free pathology localization", "author": "Hao Yang, Hong-Yu Zhou, Cheng Li, Weijian Huang, Jiarun Liu, Shanshan Wang", "abstract": "Locating pathologies automatically from medical images aids the understanding\nof the emergence and progression of diseases, and such an ability can\nsignificantly benefit clinical diagnostics. However, existing deep learning\nmodels heavily rely on expert annotations and lack generalization capabilities\nin open clinical environments. In this study, we present a generalizable\nvision-language pre-training model for Annotation-Free pathology Localization\n(AFLoc). The core strength of AFLoc lies in its image annotation-free\nmulti-level semantic structure-based contrastive learning, which\ncomprehensively aligns multi-granularity medical concepts from reports with\nabundant image features, to adapt to the diverse expressions of observed and\nemerging unseen pathologies. We conducted extensive experimental validation\nacross 4 distinct external datasets, encompassing 11 types of chest\npathologies, to verify its generalization ability. The results demonstrate that\nAFLoc surpasses 6 state-of-the-art methods and even outperforms the human\nbenchmark in locating 5 different pathologies, underscoring its suitability for\ncomplex clinical environments.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02044v1"}
{"title": "Efficient Cloud-edge Collaborative Inference for Object Re-identification", "author": "Chuanming Wang, Yuxin Yang, Mengshi Qi, Huadong Ma", "abstract": "Current object re-identification (ReID) system follows the centralized\nprocessing paradigm, i.e., all computations are conducted in the cloud server\nand edge devices are only used to capture and send images. As the number of\nvideos experiences a rapid escalation, this paradigm has become impractical due\nto the finite computational resources. In such a scenario, the ReID system\nshould be converted to fit in the cloud-edge collaborative processing paradigm,\nwhich is crucial to boost the scalability and practicality of ReID systems.\nHowever, current relevant work lacks research on this issue, making it\nchallenging for ReID methods to be adapted effectively. Therefore, we pioneer a\ncloud-edge collaborative inference framework for ReID systems and particularly\npropose a distribution-aware correlation modeling network (DaCM) to make the\ndesired image return to the cloud server as soon as possible via learning to\nmodel the spatial-temporal correlations among instances. DaCM embeds the\nspatial-temporal correlations implicitly included in the timestamps into a\ngraph structure, and it can be applied in the cloud to regulate the size of the\nupload window and on the edge device to adjust the sequence of images,\nrespectively. Traditional ReID methods can be combined with DaCM seamlessly,\nenabling their application within our proposed edge-cloud collaborative\nframework. Extensive experiments demonstrate that our method obviously reduces\ntransmission overhead and significantly improves performance. We will release\nour code and model.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02041v1"}
{"title": "DiffusionEdge: Diffusion Probabilistic Model for Crisp Edge Detection", "author": "Yunfan Ye, Kai Xu, Yuhang Huang, Renjiao Yi, Zhiping Cai", "abstract": "Limited by the encoder-decoder architecture, learning-based edge detectors\nusually have difficulty predicting edge maps that satisfy both correctness and\ncrispness. With the recent success of the diffusion probabilistic model (DPM),\nwe found it is especially suitable for accurate and crisp edge detection since\nthe denoising process is directly applied to the original image size.\nTherefore, we propose the first diffusion model for the task of general edge\ndetection, which we call DiffusionEdge. To avoid expensive computational\nresources while retaining the final performance, we apply DPM in the latent\nspace and enable the classic cross-entropy loss which is uncertainty-aware in\npixel level to directly optimize the parameters in latent space in a\ndistillation manner. We also adopt a decoupled architecture to speed up the\ndenoising process and propose a corresponding adaptive Fourier filter to adjust\nthe latent features of specific frequencies. With all the technical designs,\nDiffusionEdge can be stably trained with limited resources, predicting crisp\nand accurate edge maps with much fewer augmentation strategies. Extensive\nexperiments on four edge detection benchmarks demonstrate the superiority of\nDiffusionEdge both in correctness and crispness. On the NYUDv2 dataset,\ncompared to the second best, we increase the ODS, OIS (without post-processing)\nand AC by 30.2%, 28.1% and 65.1%, respectively. Code:\nhttps://github.com/GuHuangAI/DiffusionEdge.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02032v1"}
{"title": "Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack", "author": "Ruofei Wang, Renjie Wan, Zongyu Guo, Qing Guo, Rui Huang", "abstract": "Backdoor attack aims to deceive a victim model when facing backdoor instances\nwhile maintaining its performance on benign data. Current methods use manual\npatterns or special perturbations as triggers, while they often overlook the\nrobustness against data corruption, making backdoor attacks easy to defend in\npractice. To address this issue, we propose a novel backdoor attack method\nnamed Spy-Watermark, which remains effective when facing data collapse and\nbackdoor defense. Therein, we introduce a learnable watermark embedded in the\nlatent domain of images, serving as the trigger. Then, we search for a\nwatermark that can withstand collapse during image decoding, cooperating with\nseveral anti-collapse operations to further enhance the resilience of our\ntrigger against data corruption. Extensive experiments are conducted on\nCIFAR10, GTSRB, and ImageNet datasets, demonstrating that Spy-Watermark\novertakes ten state-of-the-art methods in terms of robustness and stealthiness.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02031v1"}
{"title": "Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN Ticket", "author": "Zhaokun Zhou, Kaiwei Che, Wei Fang, Keyu Tian, Yuesheng Zhu, Shuicheng Yan, Yonghong Tian, Li Yuan", "abstract": "Spiking Neural Networks (SNNs), known for their biologically plausible\narchitecture, face the challenge of limited performance. The self-attention\nmechanism, which is the cornerstone of the high-performance Transformer and\nalso a biologically inspired structure, is absent in existing SNNs. To this\nend, we explore the potential of leveraging both self-attention capability and\nbiological properties of SNNs, and propose a novel Spiking Self-Attention (SSA)\nand Spiking Transformer (Spikformer). The SSA mechanism eliminates the need for\nsoftmax and captures the sparse visual feature employing spike-based Query,\nKey, and Value. This sparse computation without multiplication makes SSA\nefficient and energy-saving. Further, we develop a Spiking Convolutional Stem\n(SCS) with supplementary convolutional layers to enhance the architecture of\nSpikformer. The Spikformer enhanced with the SCS is referred to as Spikformer\nV2. To train larger and deeper Spikformer V2, we introduce a pioneering\nexploration of Self-Supervised Learning (SSL) within the SNN. Specifically, we\npre-train Spikformer V2 with masking and reconstruction style inspired by the\nmainstream self-supervised Transformer, and then finetune the Spikformer V2 on\nthe image classification on ImageNet. Extensive experiments show that\nSpikformer V2 outperforms other previous surrogate training and ANN2SNN\nmethods. An 8-layer Spikformer V2 achieves an accuracy of 80.38% using 4 time\nsteps, and after SSL, a 172M 16-layer Spikformer V2 reaches an accuracy of\n81.10% with just 1 time step. To the best of our knowledge, this is the first\ntime that the SNN achieves 80+% accuracy on ImageNet. The code will be\navailable at Spikformer V2.", "published": "2024-01-04", "categories": ["cs.NE", "cs.CV", "cs.LG"], "links": "http://arxiv.org/abs/2401.02020v1"}
{"title": "Improving Diffusion-Based Image Synthesis with Context Prediction", "author": "Ling Yang, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin Huang, Zheming Cai, Wentao Zhang, Bin Cui", "abstract": "Diffusion models are a new class of generative models, and have dramatically\npromoted image generation with unprecedented quality and diversity. Existing\ndiffusion models mainly try to reconstruct input image from a corrupted one\nwith a pixel-wise or feature-wise constraint along spatial axes. However, such\npoint-based reconstruction may fail to make each predicted pixel/feature fully\npreserve its neighborhood context, impairing diffusion-based image synthesis.\nAs a powerful source of automatic supervisory signal, context has been well\nstudied for learning representations. Inspired by this, we for the first time\npropose ConPreDiff to improve diffusion-based image synthesis with context\nprediction. We explicitly reinforce each point to predict its neighborhood\ncontext (i.e., multi-stride features/tokens/pixels) with a context decoder at\nthe end of diffusion denoising blocks in training stage, and remove the decoder\nfor inference. In this way, each point can better reconstruct itself by\npreserving its semantic connections with neighborhood context. This new\nparadigm of ConPreDiff can generalize to arbitrary discrete and continuous\ndiffusion backbones without introducing extra parameters in sampling procedure.\nExtensive experiments are conducted on unconditional image generation,\ntext-to-image generation and image inpainting tasks. Our ConPreDiff\nconsistently outperforms previous methods and achieves a new SOTA text-to-image\ngeneration results on MS-COCO, with a zero-shot FID score of 6.21.", "published": "2024-01-04", "categories": ["cs.CV"], "links": "http://arxiv.org/abs/2401.02015v1"}
