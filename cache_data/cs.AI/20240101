{"title": "Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models", "author": "Yinan Cheng, Chi-Hua Wang, Vamsi K. Potluru, Tucker Balch, Guang Cheng", "abstract": "Devising procedures for downstream task-oriented generative model selections\nis an unresolved problem of practical importance. Existing studies focused on\nthe utility of a single family of generative models. They provided limited\ninsights on how synthetic data practitioners select the best family generative\nmodels for synthetic training tasks given a specific combination of machine\nlearning model class and performance metric. In this paper, we approach the\ndownstream task-oriented generative model selections problem in the case of\ntraining fraud detection models and investigate the best practice given\ndifferent combinations of model interpretability and model performance\nconstraints. Our investigation supports that, while both Neural\nNetwork(NN)-based and Bayesian Network(BN)-based generative models are both\ngood to complete synthetic training task under loose model interpretability\nconstrain, the BN-based generative models is better than NN-based when\nsynthetic training fraud detection model under strict model interpretability\nconstrain. Our results provides practical guidance for machine learning\npractitioner who is interested in replacing their training dataset from real to\nsynthetic, and shed lights on more general downstream task-oriented generative\nmodel selection problems.", "published": "2024-01-01", "categories": ["cs.LG", "cs.AI"], "links": "http://arxiv.org/abs/2401.00974v1"}
{"title": "Directional Antenna Systems for Long-Range Through-Wall Human Activity Recognition", "author": "Julian Strohmayer, Martin Kampel", "abstract": "WiFi Channel State Information (CSI)-based human activity recognition (HAR)\nenables contactless, long-range sensing in spatially constrained environments\nwhile preserving visual privacy. However, despite the presence of numerous\nWiFi-enabled devices around us, few expose CSI to users, resulting in a lack of\nsensing hardware options. Variants of the Espressif ESP32 have emerged as\npotential low-cost and easy-to-deploy solutions for WiFi CSI-based HAR. In this\nwork, four ESP32-S3-based 2.4GHz directional antenna systems are evaluated for\ntheir ability to facilitate long-range through-wall HAR. Two promising systems\nare proposed, one of which combines the ESP32-S3 with a directional biquad\nantenna. This combination represents, to the best of our knowledge, the first\ndemonstration of such a system in WiFi-based HAR. The second system relies on\nthe built-in printed inverted-F antenna (PIFA) of the ESP32-S3 and achieves\ndirectionality through a plane reflector. In a comprehensive evaluation of\nline-of-sight (LOS) and non-line-of-sight (NLOS) HAR performance, both systems\nare deployed in an office environment spanning a distance of 18 meters across\nfive rooms. In this experimental setup, the Wallhack1.8k dataset, comprising\n1806 CSI amplitude spectrograms of human activities, is collected and made\npublicly available. Based on Wallhack1.8k, we train activity recognition models\nusing the EfficientNetV2 architecture to assess system performance in LOS and\nNLOS scenarios. For the core NLOS activity recognition problem, the biquad\nantenna and PIFA-based systems achieve accuracies of 92.0$\\pm$3.5 and\n86.8$\\pm$4.7, respectively, demonstrating the feasibility of long-range\nthrough-wall HAR with the proposed systems.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI", "cs.LG"], "links": "http://arxiv.org/abs/2401.01388v1"}
{"title": "Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human Activity Recognition", "author": "Julian Strohmayer, Martin Kampel", "abstract": "The recognition of human activities based on WiFi Channel State Information\n(CSI) enables contactless and visual privacy-preserving sensing in indoor\nenvironments. However, poor model generalization, due to varying environmental\nconditions and sensing hardware, is a well-known problem in this space. To\naddress this issue, in this work, data augmentation techniques commonly used in\nimage-based learning are applied to WiFi CSI to investigate their effects on\nmodel generalization performance in cross-scenario and cross-system settings.\nIn particular, we focus on the generalization between line-of-sight (LOS) and\nnon-line-of-sight (NLOS) through-wall scenarios, as well as on the\ngeneralization between different antenna systems, which remains under-explored.\nWe collect and make publicly available a dataset of CSI amplitude spectrograms\nof human activities. Utilizing this data, an ablation study is conducted in\nwhich activity recognition models based on the EfficientNetV2 architecture are\ntrained, allowing us to assess the effects of each augmentation on model\ngeneralization performance. The gathered results show that specific\ncombinations of simple data augmentation techniques applied to CSI amplitude\ndata can significantly improve cross-scenario and cross-system generalization.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI", "cs.LG"], "links": "http://arxiv.org/abs/2401.00964v1"}
{"title": "Automated Model Selection for Tabular Data", "author": "Avinash Amballa, Anmol Mekala, Gayathri Akkinapalli, Manas Madine, Naga Pavana Priya Yarrabolu, Przemyslaw A. Grabowicz", "abstract": "Structured data in the form of tabular datasets contain features that are\ndistinct and discrete, with varying individual and relative importances to the\ntarget. Combinations of one or more features may be more predictive and\nmeaningful than simple individual feature contributions. R's mixed effect\nlinear models library allows users to provide such interactive feature\ncombinations in the model design. However, given many features and possible\ninteractions to select from, model selection becomes an exponentially difficult\ntask. We aim to automate the model selection process for predictions on tabular\ndatasets incorporating feature interactions while keeping computational costs\nsmall. The framework includes two distinct approaches for feature selection: a\nPriority-based Random Grid Search and a Greedy Search method. The\nPriority-based approach efficiently explores feature combinations using prior\nprobabilities to guide the search. The Greedy method builds the solution\niteratively by adding or removing features based on their impact. Experiments\non synthetic demonstrate the ability to effectively capture predictive feature\ncombinations.", "published": "2024-01-01", "categories": ["cs.LG", "cs.AI"], "links": "http://arxiv.org/abs/2401.00961v1"}
{"title": "Refining Pre-Trained Motion Models", "author": "Xinglong Sun, Adam W. Harley, Leonidas J. Guibas", "abstract": "Given the difficulty of manually annotating motion in video, the current best\nmotion estimation methods are trained with synthetic data, and therefore\nstruggle somewhat due to a train/test gap. Self-supervised methods hold the\npromise of training directly on real video, but typically perform worse. These\ninclude methods trained with warp error (i.e., color constancy) combined with\nsmoothness terms, and methods that encourage cycle-consistency in the estimates\n(i.e., tracking backwards should yield the opposite trajectory as tracking\nforwards). In this work, we take on the challenge of improving state-of-the-art\nsupervised models with self-supervised training. We find that when the\ninitialization is supervised weights, most existing self-supervision techniques\nactually make performance worse instead of better, which suggests that the\nbenefit of seeing the new data is overshadowed by the noise in the training\nsignal. Focusing on obtaining a ``clean'' training signal from real-world\nunlabelled video, we propose to separate label-making and training into two\ndistinct stages. In the first stage, we use the pre-trained model to estimate\nmotion in a video, and then select the subset of motion estimates which we can\nverify with cycle-consistency. This produces a sparse but accurate\npseudo-labelling of the video. In the second stage, we fine-tune the model to\nreproduce these outputs, while also applying augmentations on the input. We\ncomplement this boot-strapping method with simple techniques that densify and\nre-balance the pseudo-labels, ensuring that we do not merely train on ``easy''\ntracks. We show that our method yields reliable gains over fully-supervised\nmethods in real videos, for both short-term (flow-based) and long-range\n(multi-frame) pixel tracking.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.00850v1"}
{"title": "Taking the Next Step with Generative Artificial Intelligence: The Transformative Role of Multimodal Large Language Models in Science Education", "author": "Arne Bewersdorff, Christian Hartmann, Marie Hornberger, Kathrin Seßler, Maria Bannert, Enkelejda Kasneci, Gjergji Kasneci, Xiaoming Zhai, Claudia Nerdel", "abstract": "The integration of Artificial Intelligence (AI), particularly Large Language\nModel (LLM)-based systems, in education has shown promise in enhancing teaching\nand learning experiences. However, the advent of Multimodal Large Language\nModels (MLLMs) like GPT-4 with vision (GPT-4V), capable of processing\nmultimodal data including text, sound, and visual inputs, opens a new era of\nenriched, personalized, and interactive learning landscapes in education.\nGrounded in theory of multimedia learning, this paper explores the\ntransformative role of MLLMs in central aspects of science education by\npresenting exemplary innovative learning scenarios. Possible applications for\nMLLMs could range from content creation to tailored support for learning,\nfostering competencies in scientific practices, and providing assessment and\nfeedback. These scenarios are not limited to text-based and uni-modal formats\nbut can be multimodal, increasing thus personalization, accessibility, and\npotential learning effectiveness. Besides many opportunities, challenges such\nas data protection and ethical considerations become more salient, calling for\nrobust frameworks to ensure responsible integration. This paper underscores the\nnecessity for a balanced approach in implementing MLLMs, where the technology\ncomplements rather than supplants the educator's role, ensuring thus an\neffective and ethical use of AI in science education. It calls for further\nresearch to explore the nuanced implications of MLLMs on the evolving role of\neducators and to extend the discourse beyond science education to other\ndisciplines. Through the exploration of potentials, challenges, and future\nimplications, we aim to contribute to a preliminary understanding of the\ntransformative trajectory of MLLMs in science education and beyond.", "published": "2024-01-01", "categories": ["cs.AI", "cs.CY"], "links": "http://arxiv.org/abs/2401.00832v1"}
{"title": "Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases", "author": "Yifei Chen, Chenyan Zhang, Ben Chen, Yiyu Huang, Yifei Sun, Changmiao Wang, Xianjun Fu, Yuxing Dai, Feiwei Qin, Yong Peng, Yu Gao", "abstract": "In standard hospital blood tests, the traditional process requires doctors to\nmanually isolate leukocytes from microscopic images of patients' blood using\nmicroscopes. These isolated leukocytes are then categorized via automatic\nleukocyte classifiers to determine the proportion and volume of different types\nof leukocytes present in the blood samples, aiding disease diagnosis. This\nmethodology is not only time-consuming and labor-intensive, but it also has a\nhigh propensity for errors due to factors such as image quality and\nenvironmental conditions, which could potentially lead to incorrect subsequent\nclassifications and misdiagnosis. To address these issues, this paper proposes\nan innovative method of leukocyte detection: the Multi-level Feature Fusion and\nDeformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte\nscale disparity, we designed the High-level Screening-feature Fusion Pyramid\n(HS-FPN), enabling multi-level fusion. This model uses high-level features as\nweights to filter low-level feature information via a channel attention module\nand then merges the screened information with the high-level features, thus\nenhancing the model's feature expression capability. Further, we address the\nissue of leukocyte feature scarcity by incorporating a multi-scale deformable\nself-attention module in the encoder and using the self-attention and\ncross-deformable attention mechanisms in the decoder, which aids in the\nextraction of the global features of the leukocyte feature maps. The\neffectiveness, superiority, and generalizability of the proposed MFDS-DETR\nmethod are confirmed through comparisons with other cutting-edge leukocyte\ndetection models using the private WBCDD, public LISC and BCCD datasets. Our\nsource code and private WBCCD dataset are available at\nhttps://github.com/JustlfC03/MFDS-DETR.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.00926v2"}
{"title": "Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models", "author": "Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries, Qian Liu, Niklas Muennighoff", "abstract": "The high cost of full-parameter fine-tuning (FFT) of Large Language Models\n(LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods.\nHowever, it remains unclear which methods provide the best cost-performance\ntrade-off at different model scales. We introduce Astraios, a suite of 28\ninstruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up\nto 16 billion parameters. Through investigations across 5 tasks and 8 different\ndatasets encompassing both code comprehension and code generation tasks, we\nfind that FFT generally leads to the best downstream performance across all\nscales, and PEFT methods differ significantly in their efficacy based on the\nmodel scale. LoRA usually offers the most favorable trade-off between cost and\nperformance. Further investigation into the effects of these methods on both\nmodel robustness and code security reveals that larger models tend to\ndemonstrate reduced robustness and less security. At last, we explore the\nrelationships among updated parameters, cross-entropy loss, and task\nperformance. We find that the tuning effectiveness observed in small models\ngeneralizes well to larger models, and the validation loss in instruction\ntuning can be a reliable indicator of overall downstream performance.", "published": "2024-01-01", "categories": ["cs.CL", "cs.AI", "cs.SE"], "links": "http://arxiv.org/abs/2401.00788v1"}
{"title": "Temporal Validity Change Prediction", "author": "Georg Wenzel, Adam Jatowt", "abstract": "Temporal validity is an important property of text that is useful for many\ndownstream applications, such as recommender systems, conversational AI, or\nstory understanding. Existing benchmarking tasks often require models to\nidentify the temporal validity duration of a single statement. However, in many\ncases, additional contextual information, such as sentences in a story or posts\non a social media profile, can be collected from the available text stream.\nThis contextual information may greatly alter the duration for which a\nstatement is expected to be valid. We propose Temporal Validity Change\nPrediction, a natural language processing task benchmarking the capability of\nmachine learning models to detect contextual statements that induce such\nchange. We create a dataset consisting of temporal target statements sourced\nfrom Twitter and crowdsource sample context statements. We then benchmark a set\nof transformer-based language models on our dataset. Finally, we experiment\nwith temporal validity duration prediction as an auxiliary task to improve the\nperformance of the state-of-the-art model.", "published": "2024-01-01", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "links": "http://arxiv.org/abs/2401.00779v1"}
{"title": "Edge Computing based Human-Robot Cognitive Fusion: A Medical Case Study in the Autism Spectrum Disorder Therapy", "author": "Qin Yang", "abstract": "In recent years, edge computing has served as a paradigm that enables many\nfuture technologies like AI, Robotics, IoT, and high-speed wireless sensor\nnetworks (like 5G) by connecting cloud computing facilities and services to the\nend users. Especially in medical and healthcare applications, it provides\nremote patient monitoring and increases voluminous multimedia. From the\nrobotics angle, robot-assisted therapy (RAT) is an active-assistive robotic\ntechnology in rehabilitation robotics, attracting many researchers to study and\nbenefit people with disability like autism spectrum disorder (ASD) children.\nHowever, the main challenge of RAT is that the model capable of detecting the\naffective states of ASD people exists and can recall individual preferences.\nMoreover, involving expert diagnosis and recommendations to guide robots in\nupdating the therapy approach to adapt to different statuses and scenarios is a\ncrucial part of the ASD therapy process. This paper proposes the architecture\nof edge cognitive computing by combining human experts and assisted robots\ncollaborating in the same framework to help ASD patients with long-term\nsupport. By integrating the real-time computing and analysis of a new cognitive\nrobotic model for ASD therapy, the proposed architecture can achieve a seamless\nremote diagnosis, round-the-clock symptom monitoring, emergency warning,\ntherapy alteration, and advanced assistance.", "published": "2024-01-01", "categories": ["cs.RO", "cs.AI", "cs.DC", "cs.LG", "cs.MA"], "links": "http://arxiv.org/abs/2401.00776v1"}
{"title": "Unsupervised Outlier Detection using Random Subspace and Subsampling Ensembles of Dirichlet Process Mixtures", "author": "Dongwook Kim, Juyeon Park, Hee Cheol Chung, Seonghyun Jeong", "abstract": "Probabilistic mixture models are acknowledged as a valuable tool for\nunsupervised outlier detection owing to their interpretability and intuitive\ngrounding in statistical principles. Within this framework, Dirichlet process\nmixture models emerge as a compelling alternative to conventional finite\nmixture models for both clustering and outlier detection tasks. However,\ndespite their evident advantages, the widespread adoption of Dirichlet process\nmixture models in unsupervised outlier detection has been hampered by\nchallenges related to computational inefficiency and sensitivity to outliers\nduring the construction of detectors. To tackle these challenges, we propose a\nnovel outlier detection method based on ensembles of Dirichlet process Gaussian\nmixtures. The proposed method is a fully unsupervised algorithm that\ncapitalizes on random subspace and subsampling ensembles, not only ensuring\nefficient computation but also enhancing the robustness of the resulting\noutlier detector. Moreover, the proposed method leverages variational inference\nfor Dirichlet process mixtures to ensure efficient and fast computation.\nEmpirical studies with benchmark datasets demonstrate that our method\noutperforms existing approaches for unsupervised outlier detection.", "published": "2024-01-01", "categories": ["cs.LG", "cs.AI", "stat.ML"], "links": "http://arxiv.org/abs/2401.00773v1"}
{"title": "New Job, New Gender? Measuring the Social Bias in Image Generation Models", "author": "Wenxuan Wang, Haonan Bai, Jen-tse Huang, Yuxuan Wan, Youliang Yuan, Haoyi Qiu, Nanyun Peng, Michael R. Lyu", "abstract": "Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel metamorphic testing\nframework that can accurately, automatically and comprehensively trigger social\nbias in image generation models. BiasPainter uses a diverse range of seed\nimages of individuals and prompts the image generation models to edit these\nimages using gender, race, and age-neutral queries. These queries span 62\nprofessions, 39 activities, 57 types of objects, and 70 personality traits. The\nframework then compares the edited images to the original seed images, focusing\non any changes related to gender, race, and age. BiasPainter adopts a testing\noracle that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. To evaluate the\neffectiveness of BiasPainter, we use BiasPainter to test five widely-used\ncommercial image generation software and models, such as stable diffusion and\nMidjourney. Experimental results show that 100\\% of the generated test cases\ncan successfully trigger social bias in image generation models.", "published": "2024-01-01", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "links": "http://arxiv.org/abs/2401.00763v1"}
{"title": "The Earth is Flat? Unveiling Factual Errors in Large Language Models", "author": "Wenxuan Wang, Juluan Shi, Zhaopeng Tu, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu", "abstract": "Large Language Models (LLMs) like ChatGPT are foundational in various\napplications due to their extensive knowledge from pre-training and\nfine-tuning. Despite this, they are prone to generating factual and commonsense\nerrors, raising concerns in critical areas like healthcare, journalism, and\neducation to mislead users. Current methods for evaluating LLMs' veracity are\nlimited by test data leakage or the need for extensive human labor, hindering\nefficient and accurate error detection. To tackle this problem, we introduce a\nnovel, automatic testing framework, FactChecker, aimed at uncovering factual\ninaccuracies in LLMs. This framework involves three main steps: First, it\nconstructs a factual knowledge graph by retrieving fact triplets from a\nlarge-scale knowledge database. Then, leveraging the knowledge graph,\nFactChecker employs a rule-based approach to generates three types of questions\n(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and\nmulti-hop relations, along with correct answers. Lastly, it assesses the LLMs'\nresponses for accuracy using tailored matching strategies for each question\ntype. Our extensive tests on six prominent LLMs, including text-davinci-002,\ntext-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal\nthat FactChecker can trigger factual errors in up to 45\\% of questions in these\nmodels. Moreover, we demonstrate that FactChecker's test cases can improve\nLLMs' factual accuracy through in-context learning and fine-tuning (e.g.,\nllama-2-13b-chat's accuracy increase from 35.3\\% to 68.5\\%). We are making all\ncode, data, and results available for future research endeavors.", "published": "2024-01-01", "categories": ["cs.SE", "cs.AI", "cs.CL"], "links": "http://arxiv.org/abs/2401.00761v1"}
{"title": "A & B == B & A: Triggering Logical Reasoning Failures in Large Language Models", "author": "Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-tse Huang, Pinjia He, Wenxiang Jiao, Michael R. Lyu", "abstract": "Recent advancements in large language models (LLMs) have propelled Artificial\nIntelligence (AI) to new heights, enabling breakthroughs in various tasks such\nas writing assistance, code generation, and machine translation. A significant\ndistinction of advanced LLMs, such as ChatGPT, is their demonstrated ability to\n\"reason.\" However, evaluating the reasoning ability of LLMs remains a challenge\nas most existing evaluations focus on their accuracy on the downstream tasks\nrather than directly assessing their reasoning processes. Efforts have been\nmade to develop benchmarks and metrics to assess reasoning in LLMs, but they\nsuffer from data leakage or limited scope. In this paper, we introduce\nLogicAsker, an automatic approach that comprehensively evaluates and improves\nthe logical reasoning abilities of LLMs under a set of atomic reasoning skills\nbased on propositional and predicate logic. The results provide insights into\nLLMs' reasoning abilities and reveal the logical rules the LLMs did not learn\nwell. We evaluate LogicAsker on six widely deployed LLMs, including GPT-3,\nChatGPT, GPT-4, Bard, Vicuna, and Guanaco. The results show that test cases\nfrom LogicAsker can find logical reasoning failures in different LLMs with a\nrate of 25\\% - 94\\%. In addition, the test cases of LogicAsker can be further\nused to design demonstration examples for in-context learning, which\neffectively improves the logical reasoning ability of LLMs, e.g., 10\\% for\nGPT-4. As far as we know, our work is the first to create prompts based on\ntesting results to improve LLMs' formal reasoning ability effectively. All the\ncode, data, and results will be released for reproduction and future research.", "published": "2024-01-01", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LO"], "links": "http://arxiv.org/abs/2401.00757v1"}
{"title": "Strong Transitivity Relations and Graph Neural Networks", "author": "Yassin Mohamadi, Mostafa Haghir Chehreghani", "abstract": "Local neighborhoods play a crucial role in embedding generation in\ngraph-based learning. It is commonly believed that nodes ought to have\nembeddings that resemble those of their neighbors. In this research, we try to\ncarefully expand the concept of similarity from nearby neighborhoods to the\nentire graph. We provide an extension of similarity that is based on\ntransitivity relations, which enables Graph Neural Networks (GNNs) to capture\nboth global similarities and local similarities over the whole graph. We\nintroduce Transitivity Graph Neural Network (TransGNN), which more than local\nnode similarities, takes into account global similarities by distinguishing\nstrong transitivity relations from weak ones and exploiting them. We evaluate\nour model over several real-world datasets and showed that it considerably\nimproves the performance of several well-known GNN models, for tasks such as\nnode classification.", "published": "2024-01-01", "categories": ["cs.SI", "cs.AI", "cs.LG"], "links": "http://arxiv.org/abs/2401.01384v1"}
{"title": "MPRE: Multi-perspective Patient Representation Extractor for Disease Prediction", "author": "Ziyue Yu, Jiayi Wang, Wuman Luo, Rita Tse, Giovanni Pau", "abstract": "Patient representation learning based on electronic health records (EHR) is a\ncritical task for disease prediction. This task aims to effectively extract\nuseful information on dynamic features. Although various existing works have\nachieved remarkable progress, the model performance can be further improved by\nfully extracting the trends, variations, and the correlation between the trends\nand variations in dynamic features. In addition, sparse visit records limit the\nperformance of deep learning models. To address these issues, we propose the\nMulti-perspective Patient Representation Extractor (MPRE) for disease\nprediction. Specifically, we propose Frequency Transformation Module (FTM) to\nextract the trend and variation information of dynamic features in the\ntime-frequency domain, which can enhance the feature representation. In the 2D\nMulti-Extraction Network (2D MEN), we form the 2D temporal tensor based on\ntrend and variation. Then, the correlations between trend and variation are\ncaptured by the proposed dilated operation. Moreover, we propose the\nFirst-Order Difference Attention Mechanism (FODAM) to calculate the\ncontributions of differences in adjacent variations to the disease diagnosis\nadaptively. To evaluate the performance of MPRE and baseline methods, we\nconduct extensive experiments on two real-world public datasets. The experiment\nresults show that MPRE outperforms state-of-the-art baseline methods in terms\nof AUROC and AUPRC.", "published": "2024-01-01", "categories": ["cs.LG", "cs.AI"], "links": "http://arxiv.org/abs/2401.00756v1"}
{"title": "ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios", "author": "Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, Xuanjing Huang", "abstract": "Existing evaluations of tool learning primarily focus on validating the\nalignment of selected tools for large language models (LLMs) with expected\noutcomes. However, these approaches rely on a limited set of scenarios where\nanswers can be pre-determined, diverging from genuine needs. Furthermore, a\nsole emphasis on outcomes disregards the intricate capabilities essential for\nLLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a\nfine-grained system tailored for the evaluation of the LLMs' tool learning\ncapabilities in authentic scenarios. The system meticulously examines seven\nreal-world scenarios, analyzing five dimensions crucial to LLMs in tool\nlearning: format alignment, intent comprehension, behavior planning, tool\nselection, and answer organization. Additionally, ToolEyes incorporates a tool\nlibrary boasting approximately 600 tools, serving as an intermediary between\nLLMs and the physical world. Evaluations involving ten LLMs across three\ncategories reveal a preference for specific scenarios and limited cognitive\nabilities in tool learning. Intriguingly, expanding the model size even\nexacerbates the hindrance to tool learning. These findings offer instructive\ninsights aimed at advancing the field of tool learning. The data is available\natt https://github.com/Junjie-Ye/ToolEyes.git.", "published": "2024-01-01", "categories": ["cs.CL", "cs.AI"], "links": "http://arxiv.org/abs/2401.00741v1"}
{"title": "DiffMorph: Text-less Image Morphing with Diffusion Models", "author": "Shounak Chatterjee", "abstract": "Text-conditioned image generation models are a prevalent use of AI image\nsynthesis, yet intuitively controlling output guided by an artist remains\nchallenging. Current methods require multiple images and textual prompts for\neach object to specify them as concepts to generate a single customized image.\n  On the other hand, our work, \\verb|DiffMorph|, introduces a novel approach\nthat synthesizes images that mix concepts without the use of textual prompts.\nOur work integrates a sketch-to-image module to incorporate user sketches as\ninput. \\verb|DiffMorph| takes an initial image with conditioning artist-drawn\nsketches to generate a morphed image.\n  We employ a pre-trained text-to-image diffusion model and fine-tune it to\nreconstruct each image faithfully. We seamlessly merge images and concepts from\nsketches into a cohesive composition. The image generation capability of our\nwork is demonstrated through our results and a comparison of these with\nprompt-based image generation.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.00739v1"}
{"title": "Searching, fast and slow, through product catalogs", "author": "Dayananda Ubrangala, Juhi Sharma, Sharath Kumar Rangappa, Kiran R, Ravi Prasad Kondapalli, Laurent Boué", "abstract": "String matching algorithms in the presence of abbreviations, such as in Stock\nKeeping Unit (SKU) product catalogs, remains a relatively unexplored topic. In\nthis paper, we present a unified architecture for SKU search that provides both\na real-time suggestion system (based on a Trie data structure) as well as a\nlower latency search system (making use of character level TF-IDF in\ncombination with language model vector embeddings) where users initiate the\nsearch process explicitly. We carry out ablation studies that justify designing\na complex search system composed of multiple components to address the delicate\ntrade-off between speed and accuracy. Using SKU search in the Dynamics CRM as\nan example, we show how our system vastly outperforms, in all aspects, the\nresults provided by the default search engine. Finally, we show how SKU\ndescriptions may be enhanced via generative text models (using gpt-3.5-turbo)\nso that the consumers of the search results may get more context and a\ngenerally better experience when presented with the results of their SKU\nsearch.", "published": "2024-01-01", "categories": ["cs.IR", "cs.AI", "cs.LG", "cs.SE"], "links": "http://arxiv.org/abs/2401.00737v1"}
{"title": "Diffusion Models, Image Super-Resolution And Everything: A Survey", "author": "Brian B. Moser, Arundhati S. Shanbhag, Federico Raue, Stanislav Frolov, Sebastian Palacio, Andreas Dengel", "abstract": "Diffusion Models (DMs) represent a significant advancement in image\nSuper-Resolution (SR), aligning technical image quality more closely with human\npreferences and expanding SR applications. DMs address critical limitations of\nprevious methods, enhancing overall realism and details in SR images. However,\nDMs suffer from color-shifting issues, and their high computational costs call\nfor efficient sampling alternatives, underscoring the challenge of balancing\ncomputational efficiency and image quality. This survey gives an overview of\nDMs applied to image SR and offers a detailed analysis that underscores the\nunique characteristics and methodologies within this domain, distinct from\nbroader existing reviews in the field. It presents a unified view of DM\nfundamentals and explores research directions, including alternative input\ndomains, conditioning strategies, guidance, corruption spaces, and zero-shot\nmethods. This survey provides insights into the evolution of image SR with DMs,\naddressing current trends, challenges, and future directions in this rapidly\nevolving field.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI", "cs.GL", "cs.LG", "cs.MM"], "links": "http://arxiv.org/abs/2401.00736v1"}
{"title": "Depth Map Denoising Network and Lightweight Fusion Network for Enhanced 3D Face Recognition", "author": "Ruizhuo Xu, Ke Wang, Chao Deng, Mei Wang, Xi Chen, Wenhui Huang, Junlan Feng, Weihong Deng", "abstract": "With the increasing availability of consumer depth sensors, 3D face\nrecognition (FR) has attracted more and more attention. However, the data\nacquired by these sensors are often coarse and noisy, making them impractical\nto use directly. In this paper, we introduce an innovative Depth map denoising\nnetwork (DMDNet) based on the Denoising Implicit Image Function (DIIF) to\nreduce noise and enhance the quality of facial depth images for low-quality 3D\nFR. After generating clean depth faces using DMDNet, we further design a\npowerful recognition network called Lightweight Depth and Normal Fusion network\n(LDNFNet), which incorporates a multi-branch fusion block to learn unique and\ncomplementary features between different modalities such as depth and normal\nimages. Comprehensive experiments conducted on four distinct low-quality\ndatabases demonstrate the effectiveness and robustness of our proposed methods.\nFurthermore, when combining DMDNet and LDNFNet, we achieve state-of-the-art\nresults on the Lock3DFace database.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.00719v1"}
{"title": "Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data", "author": "Michalis Pistos, Islem Rekik", "abstract": "The understanding of the convoluted evolution of infant brain networks during\nthe first postnatal year is pivotal for identifying the dynamics of early brain\nconnectivity development. Existing deep learning solutions suffer from three\nmajor limitations. First, they cannot generalize to multi-trajectory prediction\ntasks, where each graph trajectory corresponds to a particular imaging modality\nor connectivity type (e.g., T1-w MRI). Second, existing models require\nextensive training datasets to achieve satisfactory performance which are often\nchallenging to obtain. Third, they do not efficiently utilize incomplete time\nseries data. To address these limitations, we introduce FedGmTE-Net++, a\nfederated graph-based multi-trajectory evolution network. Using the power of\nfederation, we aggregate local learnings among diverse hospitals with limited\ndatasets. As a result, we enhance the performance of each hospital's local\ngenerative model, while preserving data privacy. The three key innovations of\nFedGmTE-Net++ are: (i) presenting the first federated learning framework\nspecifically designed for brain multi-trajectory evolution prediction in a\ndata-scarce environment, (ii) incorporating an auxiliary regularizer in the\nlocal objective function to exploit all the longitudinal brain connectivity\nwithin the evolution trajectory and maximize data utilization, (iii)\nintroducing a two-step imputation process, comprising a preliminary KNN-based\nprecompletion followed by an imputation refinement step that employs regressors\nto improve similarity scores and refine imputations. Our comprehensive\nexperimental results showed the outperformance of FedGmTE-Net++ in brain\nmulti-trajectory prediction from a single baseline graph in comparison with\nbenchmark methods.", "published": "2024-01-01", "categories": ["q-bio.NC", "cs.AI", "cs.CV", "cs.LG"], "links": "http://arxiv.org/abs/2401.01383v1"}
{"title": "Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven Body Controllable Attribute", "author": "Chaoqun Gong, Yuqin Dai, Ronghui Li, Achun Bao, Jun Li, Jian Yang, Yachao Zhang, Xiu Li", "abstract": "Generating 3D human models directly from text helps reduce the cost and time\nof character modeling. However, achieving multi-attribute controllable and\nrealistic 3D human avatar generation is still challenging due to feature\ncoupling and the scarcity of realistic 3D human avatar datasets. To address\nthese issues, we propose Text2Avatar, which can generate realistic-style 3D\navatars based on the coupled text prompts. Text2Avatar leverages a discrete\ncodebook as an intermediate feature to establish a connection between text and\navatars, enabling the disentanglement of features. Furthermore, to alleviate\nthe scarcity of realistic style 3D human avatar data, we utilize a pre-trained\nunconditional 3D human avatar generation model to obtain a large amount of 3D\navatar pseudo data, which allows Text2Avatar to achieve realistic style\ngeneration. Experimental results demonstrate that our method can generate\nrealistic 3D avatars from coupled textual data, which is challenging for other\nexisting methods in this field.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.00711v1"}
{"title": "An attempt to generate new bridge types from latent space of generative adversarial network", "author": "Hongjun Zhang", "abstract": "Try to generate new bridge types using generative artificial intelligence\ntechnology. Symmetric structured image dataset of three-span beam bridge, arch\nbridge, cable-stayed bridge and suspension bridge are used . Based on Python\nprogramming language, TensorFlow and Keras deep learning platform framework ,\nas well as Wasserstein loss function and Lipschitz constraints, generative\nadversarial network is constructed and trained. From the obtained low\ndimensional bridge-type latent space sampling, new bridge types with asymmetric\nstructures can be generated. Generative adversarial network can create new\nbridge types by organically combining different structural components on the\nbasis of human original bridge types. It has a certain degree of human original\nability. Generative artificial intelligence technology can open up imagination\nspace and inspire humanity.", "published": "2024-01-01", "categories": ["cs.LG", "cs.AI", "cs.CV"], "links": "http://arxiv.org/abs/2401.00700v1"}
{"title": "Large Language Models aren't all that you need", "author": "Kiran Voderhobli Holla, Chaithanya Kumar, Aryan Singh", "abstract": "This paper describes the architecture and systems built towards solving the\nSemEval 2023 Task 2: MultiCoNER II (Multilingual Complex Named Entity\nRecognition) [1]. We evaluate two approaches (a) a traditional Conditional\nRandom Fields model and (b) a Large Language Model (LLM) fine-tuned with a\ncustomized head and compare the two approaches. The novel ideas explored are:\n1) Decaying auxiliary loss (with residual) - where we train the model on an\nauxiliary task of Coarse-Grained NER and include this task as a part of the\nloss function 2) Triplet token blending - where we explore ways of blending the\nembeddings of neighboring tokens in the final NER layer prior to prediction 3)\nTask-optimal heads - where we explore a variety of custom heads and learning\nrates for the final layer of the LLM. We also explore multiple LLMs including\nGPT-3 and experiment with a variety of dropout and other hyperparameter\nsettings before arriving at our final model which achieves micro & macro f1 of\n0.85/0.84 (on dev) and 0.67/0.61 on the test data . We show that while\npre-trained LLMs, by themselves, bring about a large improvement in scores as\ncompared to traditional models, we also demonstrate that tangible improvements\nto the Macro-F1 score can be made by augmenting the LLM with additional\nfeature/loss/model engineering techniques described above.", "published": "2024-01-01", "categories": ["cs.CL", "cs.AI", "cs.LG"], "links": "http://arxiv.org/abs/2401.00698v1"}
{"title": "Large language model for Bible sentiment analysis: Sermon on the Mount", "author": "Mahek Vora, Tom Blau, Vansh Kachhwal, Ashu M. G. Solo, Rohitash Chandra", "abstract": "The revolution of natural language processing via large language models has\nmotivated its use in multidisciplinary areas that include social sciences and\nhumanities and more specifically, comparative religion. Sentiment analysis\nprovides a mechanism to study the emotions expressed in text. Recently,\nsentiment analysis has been used to study and compare translations of the\nBhagavad Gita, which is a fundamental and sacred Hindu text. In this study, we\nuse sentiment analysis for studying selected chapters of the Bible. These\nchapters are known as the Sermon on the Mount. We utilize a pre-trained\nlanguage model for sentiment analysis by reviewing five translations of the\nSermon on the Mount, which include the King James version, the New\nInternational Version, the New Revised Standard Version, the Lamsa Version, and\nthe Basic English Version. We provide a chapter-by-chapter and verse-by-verse\ncomparison using sentiment and semantic analysis and review the major\nsentiments expressed. Our results highlight the varying sentiments across the\nchapters and verses. We found that the vocabulary of the respective\ntranslations is significantly different. We detected different levels of\nhumour, optimism, and empathy in the respective chapters that were used by\nJesus to deliver his message.", "published": "2024-01-01", "categories": ["cs.CL", "cs.AI"], "links": "http://arxiv.org/abs/2401.00689v1"}
{"title": "Communication-Efficient Federated Learning for LEO Constellations Integrated with HAPs Using Hybrid NOMA-OFDM", "author": "Mohamed Elmahallawy, Tie Luo, Khaled Ramadan", "abstract": "Space AI has become increasingly important and sometimes even necessary for\ngovernment, businesses, and society. An active research topic under this\nmission is integrating federated learning (FL) with satellite communications\n(SatCom) so that numerous low Earth orbit (LEO) satellites can collaboratively\ntrain a machine learning model. However, the special communication environment\nof SatCom leads to a very slow FL training process up to days and weeks. This\npaper proposes NomaFedHAP, a novel FL-SatCom approach tailored to LEO\nsatellites, that (1) utilizes high-altitude platforms (HAPs) as distributed\nparameter servers (PS) to enhance satellite visibility, and (2) introduces\nnon-orthogonal multiple access (NOMA) into LEO to enable fast and\nbandwidth-efficient model transmissions. In addition, NomaFedHAP includes (3) a\nnew communication topology that exploits HAPs to bridge satellites among\ndifferent orbits to mitigate the Doppler shift, and (4) a new FL model\naggregation scheme that optimally balances models between different orbits and\nshells. Moreover, we (5) derive a closed-form expression of the outage\nprobability for satellites in near and far shells, as well as for the entire\nsystem. Our extensive simulations have validated the mathematical analysis and\ndemonstrated the superior performance of NomaFedHAP in achieving fast and\nefficient FL model convergence with high accuracy as compared to the\nstate-of-the-art.", "published": "2024-01-01", "categories": ["cs.LG", "cs.AI", "cs.DC"], "links": "http://arxiv.org/abs/2401.00685v1"}
{"title": "Data Assimilation in Chaotic Systems Using Deep Reinforcement Learning", "author": "Mohamad Abed El Rahman Hammoud, Naila Raboudi, Edriss S. Titi, Omar Knio, Ibrahim Hoteit", "abstract": "Data assimilation (DA) plays a pivotal role in diverse applications, ranging\nfrom climate predictions and weather forecasts to trajectory planning for\nautonomous vehicles. A prime example is the widely used ensemble Kalman filter\n(EnKF), which relies on linear updates to minimize variance among the ensemble\nof forecast states. Recent advancements have seen the emergence of deep\nlearning approaches in this domain, primarily within a supervised learning\nframework. However, the adaptability of such models to untrained scenarios\nremains a challenge. In this study, we introduce a novel DA strategy that\nutilizes reinforcement learning (RL) to apply state corrections using full or\npartial observations of the state variables. Our investigation focuses on\ndemonstrating this approach to the chaotic Lorenz '63 system, where the agent's\nobjective is to minimize the root-mean-squared error between the observations\nand corresponding forecast states. Consequently, the agent develops a\ncorrection strategy, enhancing model forecasts based on available system state\nobservations. Our strategy employs a stochastic action policy, enabling a Monte\nCarlo-based DA framework that relies on randomly sampling the policy to\ngenerate an ensemble of assimilated realizations. Results demonstrate that the\ndeveloped RL algorithm performs favorably when compared to the EnKF.\nAdditionally, we illustrate the agent's capability to assimilate non-Gaussian\ndata, addressing a significant limitation of the EnKF.", "published": "2024-01-01", "categories": ["math.DS", "cs.AI", "cs.LG", "physics.ao-ph"], "links": "http://arxiv.org/abs/2401.00916v1"}
{"title": "1st Place Solution for 5th LSVOS Challenge: Referring Video Object Segmentation", "author": "Zhuoyan Luo, Yicheng Xiao, Yong Liu, Yitong Wang, Yansong Tang, Xiu Li, Yujiu Yang", "abstract": "The recent transformer-based models have dominated the Referring Video Object\nSegmentation (RVOS) task due to the superior performance. Most prior works\nadopt unified DETR framework to generate segmentation masks in\nquery-to-instance manner. In this work, we integrate strengths of that leading\nRVOS models to build up an effective paradigm. We first obtain binary mask\nsequences from the RVOS models. To improve the consistency and quality of\nmasks, we propose Two-Stage Multi-Model Fusion strategy. Each stage rationally\nensembles RVOS models based on framework design as well as training strategy,\nand leverages different video object segmentation (VOS) models to enhance mask\ncoherence by object propagation mechanism. Our method achieves 75.7% J&F on\nRef-Youtube-VOS validation set and 70% J&F on test set, which ranks 1st place\non 5th Large-scale Video Object Segmentation Challenge (ICCV 2023) track 3.\nCode is available at https://github.com/RobertLuo1/iccv2023_RVOS_Challenge.", "published": "2024-01-01", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.00663v1"}
{"title": "Coordinated Deep Neural Networks: A Versatile Edge Offloading Algorithm", "author": "Alireza Maleki, Hamed Shah-Mansouri, Babak H. Khalaj", "abstract": "As artificial intelligence (AI) applications continue to expand, there is a\ngrowing need for deep neural network (DNN) models. Although DNN models deployed\nat the edge are promising to provide AI as a service with low latency, their\ncooperation is yet to be explored. In this paper, we consider the DNN service\nproviders share their computing resources as well as their models' parameters\nand allow other DNNs to offload their computations without mirroring. We\npropose a novel algorithm called coordinated DNNs on edge (\\textbf{CoDE}) that\nfacilitates coordination among DNN services by creating multi-task DNNs out of\nindividual models. CoDE aims to find the optimal path that results in the\nlowest possible cost, where the cost reflects the inference delay, model\naccuracy, and local computation workload. With CoDE, DNN models can make new\npaths for inference by using their own or other models' parameters. We then\nevaluate the performance of CoDE through numerical experiments. The results\ndemonstrate a $75\\%$ reduction in the local service computation workload while\ndegrading the accuracy by only $2\\%$ and having the same inference time in a\nbalanced load condition. Under heavy load, CoDE can further decrease the\ninference time by $30\\%$ while the accuracy is reduced by only $4\\%$.", "published": "2024-01-01", "categories": ["cs.NI", "cs.AI", "eess.SP"], "links": "http://arxiv.org/abs/2401.00631v1"}
