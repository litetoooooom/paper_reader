{"title": "ODIN: A Single Model for 2D and 3D Perception", "author": "Ayush Jain, Pushkal Katara, Nikolaos Gkanatsios, Adam W. Harley, Gabriel Sarch, Kriti Aggarwal, Vishrav Chaudhary, Katerina Fragkiadaki", "abstract": "State-of-the-art models on contemporary 3D perception benchmarks like ScanNet\nconsume and label dataset-provided 3D point clouds, obtained through post\nprocessing of sensed multiview RGB-D images. They are typically trained\nin-domain, forego large-scale 2D pre-training and outperform alternatives that\nfeaturize the posed RGB-D multiview images instead. The gap in performance\nbetween methods that consume posed images versus post-processed 3D point clouds\nhas fueled the belief that 2D and 3D perception require distinct model\narchitectures. In this paper, we challenge this view and propose ODIN\n(Omni-Dimensional INstance segmentation), a model that can segment and label\nboth 2D RGB images and 3D point clouds, using a transformer architecture that\nalternates between 2D within-view and 3D cross-view information fusion. Our\nmodel differentiates 2D and 3D feature operations through the positional\nencodings of the tokens involved, which capture pixel coordinates for 2D patch\ntokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art\nperformance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation\nbenchmarks, and competitive performance on ScanNet, S3DIS and COCO. It\noutperforms all previous works by a wide margin when the sensed 3D point cloud\nis used in place of the point cloud sampled from 3D mesh. When used as the 3D\nperception engine in an instructable embodied agent architecture, it sets a new\nstate-of-the-art on the TEACh action-from-dialogue benchmark. Our code and\ncheckpoints can be found at the project website: https://odin-seg.github.io.", "published": "2024-01-04", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "links": "http://arxiv.org/abs/2401.02416v1"}
{"title": "LLM Augmented LLMs: Expanding Capabilities through Composition", "author": "Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar", "abstract": "Foundational models with billions of parameters which have been trained on\nlarge corpora of data have demonstrated non-trivial skills in a variety of\ndomains. However, due to their monolithic structure, it is challenging and\nexpensive to augment them or impart new skills. On the other hand, due to their\nadaptation abilities, several new instances of these models are being trained\ntowards new domains and tasks. In this work, we study the problem of efficient\nand practical composition of existing foundation models with more specific\nmodels to enable newer capabilities. To this end, we propose CALM --\nComposition to Augment Language Models -- which introduces cross-attention\nbetween models to compose their representations and enable new capabilities.\nSalient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'\nexisting LLMs along with a few additional parameters and data, (ii) Existing\nmodel weights are kept intact, and hence preserves existing capabilities, and\n(iii) Applies to diverse domains and settings. We illustrate that augmenting\nPaLM2-S with a smaller model trained on low-resource languages results in an\nabsolute improvement of up to 13\\% on tasks like translation into English and\narithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is\naugmented with a code-specific model, we see a relative improvement of 40\\%\nover the base model for code generation and explanation tasks -- on-par with\nfully fine-tuned counterparts.", "published": "2024-01-04", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "links": "http://arxiv.org/abs/2401.02412v1"}
{"title": "What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs", "author": "Alex Trevithick, Matthew Chan, Towaki Takikawa, Umar Iqbal, Shalini De Mello, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano", "abstract": "3D-aware Generative Adversarial Networks (GANs) have shown remarkable\nprogress in learning to generate multi-view-consistent images and 3D geometries\nof scenes from collections of 2D images via neural volume rendering. Yet, the\nsignificant memory and computational costs of dense sampling in volume\nrendering have forced 3D GANs to adopt patch-based training or employ\nlow-resolution rendering with post-processing 2D super resolution, which\nsacrifices multiview consistency and the quality of resolved geometry.\nConsequently, 3D GANs have not yet been able to fully resolve the rich 3D\ngeometry present in 2D images. In this work, we propose techniques to scale\nneural volume rendering to the much higher resolution of native 2D images,\nthereby resolving fine-grained 3D geometry with unprecedented detail. Our\napproach employs learning-based samplers for accelerating neural rendering for\n3D GAN training using up to 5 times fewer depth samples. This enables us to\nexplicitly \"render every pixel\" of the full-resolution image during training\nand inference without post-processing superresolution in 2D. Together with our\nstrategy to learn high-quality surface geometry, our method synthesizes\nhigh-resolution 3D geometry and strictly view-consistent images while\nmaintaining image quality on par with baselines relying on post-processing\nsuper resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ\nand AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D\nGANs.", "published": "2024-01-04", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "links": "http://arxiv.org/abs/2401.02411v1"}
{"title": "Real-Time 2D Temperature Field Prediction in Metal Additive Manufacturing Using Physics-Informed Neural Networks", "author": "Pouyan Sajadi, Mostafa Rahmani Dehaghani, Yifan Tang, G. Gary Wang", "abstract": "Accurately predicting the temperature field in metal additive manufacturing\n(AM) processes is critical to preventing overheating, adjusting process\nparameters, and ensuring process stability. While physics-based computational\nmodels offer precision, they are often time-consuming and unsuitable for\nreal-time predictions and online control in iterative design scenarios.\nConversely, machine learning models rely heavily on high-quality datasets,\nwhich can be costly and challenging to obtain within the metal AM domain. Our\nwork addresses this by introducing a physics-informed neural network framework\nspecifically designed for temperature field prediction in metal AM. This\nframework incorporates a physics-informed input, physics-informed loss\nfunction, and a Convolutional Long Short-Term Memory (ConvLSTM) architecture.\nUtilizing real-time temperature data from the process, our model predicts 2D\ntemperature fields for future timestamps across diverse geometries, deposition\npatterns, and process parameters. We validate the proposed framework in two\nscenarios: full-field temperature prediction for a thin wall and 2D temperature\nfield prediction for cylinder and cubic parts, demonstrating errors below 3%\nand 1%, respectively. Our proposed framework exhibits the flexibility to be\napplied across diverse scenarios with varying process parameters, geometries,\nand deposition patterns.", "published": "2024-01-04", "categories": ["cs.LG", "cs.AI"], "links": "http://arxiv.org/abs/2401.02403v1"}
{"title": "TinyLlama: An Open-Source Small Language Model", "author": "Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, Wei Lu", "abstract": "We present TinyLlama, a compact 1.1B language model pretrained on around 1\ntrillion tokens for approximately 3 epochs. Building on the architecture and\ntokenizer of Llama 2, TinyLlama leverages various advances contributed by the\nopen-source community (e.g., FlashAttention), achieving better computational\nefficiency. Despite its relatively small size, TinyLlama demonstrates\nremarkable performance in a series of downstream tasks. It significantly\noutperforms existing open-source language models with comparable sizes. Our\nmodel checkpoints and code are publicly available on GitHub at\nhttps://github.com/jzhang38/TinyLlama.", "published": "2024-01-04", "categories": ["cs.CL", "cs.AI"], "links": "http://arxiv.org/abs/2401.02385v1"}
{"title": "Survey of 3D Human Body Pose and Shape Estimation Methods for Contemporary Dance Applications", "author": "Darshan Venkatrayappa, Alain Tremeau, Damien Muselet, Philippe Colantoni", "abstract": "3D human body shape and pose estimation from RGB images is a challenging\nproblem with potential applications in augmented/virtual reality, healthcare\nand fitness technology and virtual retail. Recent solutions have focused on\nthree types of inputs: i) single images, ii) multi-view images and iii) videos.\nIn this study, we surveyed and compared 3D body shape and pose estimation\nmethods for contemporary dance and performing arts, with a special focus on\nhuman body pose and dressing, camera viewpoint, illumination conditions and\nbackground conditions. We demonstrated that multi-frame methods, such as PHALP,\nprovide better results than single-frame method for pose estimation when\ndancers are performing contemporary dances.", "published": "2024-01-04", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.02383v1"}
{"title": "A Survey Analyzing Generalization in Deep Reinforcement Learning", "author": "Ezgi Korkmaz", "abstract": "Reinforcement learning research obtained significant success and attention\nwith the utilization of deep neural networks to solve problems in high\ndimensional state or action spaces. While deep reinforcement learning policies\nare currently being deployed in many different fields from medical applications\nto self driving vehicles, there are still ongoing questions the field is trying\nto answer on the generalization capabilities of deep reinforcement learning\npolicies. In this paper, we will outline the fundamental reasons why deep\nreinforcement learning policies encounter overfitting problems that limit their\nrobustness and generalization capabilities. Furthermore, we will formalize and\nunify the diverse solution approaches to increase generalization, and overcome\noverfitting in state-action value functions. We believe our study can provide a\ncompact systematic unified analysis for the current advancements in deep\nreinforcement learning, and help to construct robust deep neural policies with\nimproved generalization abilities.", "published": "2024-01-04", "categories": ["cs.LG", "cs.AI", "stat.ML"], "links": "http://arxiv.org/abs/2401.02349v1"}
{"title": "Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training", "author": "Longtian Qiu, Shan Ning, Xuming He", "abstract": "Image captioning aims at generating descriptive and meaningful textual\ndescriptions of images, enabling a broad range of vision-language applications.\nPrior works have demonstrated that harnessing the power of Contrastive Image\nLanguage Pre-training (CLIP) offers a promising approach to achieving zero-shot\ncaptioning, eliminating the need for expensive caption annotations. However,\nthe widely observed modality gap in the latent space of CLIP harms the\nperformance of zero-shot captioning by breaking the alignment between paired\nimage-text features. To address this issue, we conduct an analysis on the CLIP\nlatent space which leads to two findings. Firstly, we observe that the CLIP's\nvisual feature of image subregions can achieve closer proximity to the paired\ncaption due to the inherent information loss in text descriptions. In addition,\nwe show that the modality gap between a paired image-text can be empirically\nmodeled as a zero-mean Gaussian distribution. Motivated by the findings, we\npropose a novel zero-shot image captioning framework with text-only training to\nreduce the modality gap. In particular, we introduce a subregion feature\naggregation to leverage local region information, which produces a compact\nvisual representation for matching text representation. Moreover, we\nincorporate a noise injection and CLIP reranking strategy to boost captioning\nperformance. We also extend our framework to build a zero-shot VQA pipeline,\ndemonstrating its generality. Through extensive experiments on common\ncaptioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that\nour method achieves remarkable performance improvements. Code is available at\nhttps://github.com/Artanic30/MacCap.", "published": "2024-01-04", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.02347v1"}
{"title": "Path-based Explanation for Knowledge Graph Completion", "author": "Heng Chang, Jiangnan Ye, Alejo Lopez Avila, Jinhua Du, Jia Li", "abstract": "Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph\nCompletion (KGC) by modelling how entities and relations interact in recent\nyears. However, the explanation of the predicted facts has not caught the\nnecessary attention. Proper explanations for the results of GNN-based KGC\nmodels increase model transparency and help researchers develop more reliable\nmodels. Existing practices for explaining KGC tasks rely on\ninstance/subgraph-based approaches, while in some scenarios, paths can provide\nmore user-friendly and interpretable explanations. Nonetheless, the methods for\ngenerating path-based explanations for KGs have not been well-explored. To\naddress this gap, we propose Power-Link, the first path-based KGC explainer\nthat explores GNN-based models. We design a novel simplified graph-powering\ntechnique, which enables the generation of path-based explanations with a fully\nparallelisable and memory-efficient training scheme. We further introduce three\nnew metrics for quantitative evaluation of the explanations, together with a\nqualitative human evaluation. Extensive experiments demonstrate that Power-Link\noutperforms the SOTA baselines in interpretability, efficiency, and\nscalability.", "published": "2024-01-04", "categories": ["cs.LG", "cs.AI", "cs.SI"], "links": "http://arxiv.org/abs/2401.02290v1"}
{"title": "Uncertainty-Aware Deep Attention Recurrent Neural Network for Heterogeneous Time Series Imputation", "author": "Linglong Qian, Zina Ibrahim, Richard Dobson", "abstract": "Missingness is ubiquitous in multivariate time series and poses an obstacle\nto reliable downstream analysis. Although recurrent network imputation achieved\nthe SOTA, existing models do not scale to deep architectures that can\npotentially alleviate issues arising in complex data. Moreover, imputation\ncarries the risk of biased estimations of the ground truth. Yet, confidence in\nthe imputed values is always unmeasured or computed post hoc from model output.\nWe propose DEep Attention Recurrent Imputation (DEARI), which jointly estimates\nmissing values and their associated uncertainty in heterogeneous multivariate\ntime series. By jointly representing feature-wise correlations and temporal\ndynamics, we adopt a self attention mechanism, along with an effective residual\ncomponent, to achieve a deep recurrent neural network with good imputation\nperformance and stable convergence. We also leverage self-supervised metric\nlearning to boost performance by optimizing sample similarity. Finally, we\ntransform DEARI into a Bayesian neural network through a novel Bayesian\nmarginalization strategy to produce stochastic DEARI, which outperforms its\ndeterministic equivalent. Experiments show that DEARI surpasses the SOTA in\ndiverse imputation tasks using real-world datasets, namely air quality control,\nhealthcare and traffic.", "published": "2024-01-04", "categories": ["cs.LG", "cs.AI"], "links": "http://arxiv.org/abs/2401.02258v1"}
{"title": "Policy-regularized Offline Multi-objective Reinforcement Learning", "author": "Qian Lin, Chao Yu, Zongkai Liu, Zifan Wu", "abstract": "In this paper, we aim to utilize only offline trajectory data to train a\npolicy for multi-objective RL. We extend the offline policy-regularized method,\na widely-adopted approach for single-objective offline RL problems, into the\nmulti-objective setting in order to achieve the above goal. However, such\nmethods face a new challenge in offline MORL settings, namely the\npreference-inconsistent demonstration problem. We propose two solutions to this\nproblem: 1) filtering out preference-inconsistent demonstrations via\napproximating behavior preferences, and 2) adopting regularization techniques\nwith high policy expressiveness. Moreover, we integrate the\npreference-conditioned scalarized update method into policy-regularized offline\nRL, in order to simultaneously learn a set of policies using a single policy\nnetwork, thus reducing the computational cost induced by the training of a\nlarge number of individual policies for various preferences. Finally, we\nintroduce Regularization Weight Adaptation to dynamically determine appropriate\nregularization weights for arbitrary target preferences during deployment.\nEmpirical results on various multi-objective datasets demonstrate the\ncapability of our approach in solving offline MORL problems.", "published": "2024-01-04", "categories": ["cs.LG", "cs.AI"], "links": "http://arxiv.org/abs/2401.02244v1"}
{"title": "Joint Multi-Facts Reasoning Network For Complex Temporal Question Answering Over Knowledge Graph", "author": "Rikui Huang, Wei Wei, Xiaoye Qu, Wenfeng Xie, Xianling Mao, Dangyang Chen", "abstract": "Temporal Knowledge Graph (TKG) is an extension of regular knowledge graph by\nattaching the time scope. Existing temporal knowledge graph question answering\n(TKGQA) models solely approach simple questions, owing to the prior assumption\nthat each question only contains a single temporal fact with explicit/implicit\ntemporal constraints. Hence, they perform poorly on questions which own\nmultiple temporal facts. In this paper, we propose \\textbf{\\underline{J}}oint\n\\textbf{\\underline{M}}ulti \\textbf{\\underline{F}}acts\n\\textbf{\\underline{R}}easoning \\textbf{\\underline{N}}etwork (JMFRN), to jointly\nreasoning multiple temporal facts for accurately answering \\emph{complex}\ntemporal questions. Specifically, JMFRN first retrieves question-related\ntemporal facts from TKG for each entity of the given complex question. For\njoint reasoning, we design two different attention (\\ie entity-aware and\ntime-aware) modules, which are suitable for universal settings, to aggregate\nentities and timestamps information of retrieved facts. Moreover, to filter\nincorrect type answers, we introduce an additional answer type discrimination\ntask. Extensive experiments demonstrate our proposed method significantly\noutperforms the state-of-art on the well-known complex temporal question\nbenchmark TimeQuestions.", "published": "2024-01-04", "categories": ["cs.CL", "cs.AI"], "links": "http://arxiv.org/abs/2401.02212v1"}
{"title": "LADRI: LeArning-based Dynamic Risk Indicator in Automated Driving System", "author": "Anil Ranjitbhai Patel, Peter Liggesmeyer", "abstract": "As the horizon of intelligent transportation expands with the evolution of\nAutomated Driving Systems (ADS), ensuring paramount safety becomes more\nimperative than ever. Traditional risk assessment methodologies, primarily\ncrafted for human-driven vehicles, grapple to adequately adapt to the\nmultifaceted, evolving environments of ADS. This paper introduces a framework\nfor real-time Dynamic Risk Assessment (DRA) in ADS, harnessing the potency of\nArtificial Neural Networks (ANNs).\n  Our proposed solution transcends these limitations, drawing upon ANNs, a\ncornerstone of deep learning, to meticulously analyze and categorize risk\ndimensions using real-time On-board Sensor (OBS) data. This learning-centric\napproach not only elevates the ADS's situational awareness but also enriches\nits understanding of immediate operational contexts. By dissecting OBS data,\nthe system is empowered to pinpoint its current risk profile, thereby enhancing\nsafety prospects for onboard passengers and the broader traffic ecosystem.\n  Through this framework, we chart a direction in risk assessment, bridging the\nconventional voids and enhancing the proficiency of ADS. By utilizing ANNs, our\nmethodology offers a perspective, allowing ADS to adeptly navigate and react to\npotential risk factors, ensuring safer and more informed autonomous journeys.", "published": "2024-01-04", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SE", "cs.SY"], "links": "http://arxiv.org/abs/2401.02199v1"}
{"title": "FairGridSearch: A Framework to Compare Fairness-Enhancing Models", "author": "Shih-Chi Ma, Tatiana Ermakova, Benjamin Fabian", "abstract": "Machine learning models are increasingly used in critical decision-making\napplications. However, these models are susceptible to replicating or even\namplifying bias present in real-world data. While there are various bias\nmitigation methods and base estimators in the literature, selecting the optimal\nmodel for a specific application remains challenging.\n  This paper focuses on binary classification and proposes FairGridSearch, a\nnovel framework for comparing fairness-enhancing models. FairGridSearch enables\nexperimentation with different model parameter combinations and recommends the\nbest one. The study applies FairGridSearch to three popular datasets (Adult,\nCOMPAS, and German Credit) and analyzes the impacts of metric selection, base\nestimator choice, and classification threshold on model fairness.\n  The results highlight the significance of selecting appropriate accuracy and\nfairness metrics for model evaluation. Additionally, different base estimators\nand classification threshold values affect the effectiveness of bias mitigation\nmethods and fairness stability respectively, but the effects are not consistent\nacross all datasets. Based on these findings, future research on fairness in\nmachine learning should consider a broader range of factors when building fair\nmodels, going beyond bias mitigation methods alone.", "published": "2024-01-04", "categories": ["cs.LG", "cs.AI", "cs.CY"], "links": "http://arxiv.org/abs/2401.02183v1"}
{"title": "Prompt Decoupling for Text-to-Image Person Re-identification", "author": "Weihao Li, Lei Tan, Pingyang Dai, Yan Zhang", "abstract": "Text-to-image person re-identification (TIReID) aims to retrieve the target\nperson from an image gallery via a textual description query. Recently,\npre-trained vision-language models like CLIP have attracted significant\nattention and have been widely utilized for this task due to their robust\ncapacity for semantic concept learning and rich multi-modal knowledge. However,\nrecent CLIP-based TIReID methods commonly rely on direct fine-tuning of the\nentire network to adapt the CLIP model for the TIReID task. Although these\nmethods show competitive performance on this topic, they are suboptimal as they\nnecessitate simultaneous domain adaptation and task adaptation. To address this\nissue, we attempt to decouple these two processes during the training stage.\nSpecifically, we introduce the prompt tuning strategy to enable domain\nadaptation and propose a two-stage training approach to disentangle domain\nadaptation from task adaptation. In the first stage, we freeze the two encoders\nfrom CLIP and solely focus on optimizing the prompts to alleviate domain gap\nbetween the original training data of CLIP and downstream tasks. In the second\nstage, we maintain the fixed prompts and fine-tune the CLIP model to prioritize\ncapturing fine-grained information, which is more suitable for TIReID task.\nFinally, we evaluate the effectiveness of our method on three widely used\ndatasets. Compared to the directly fine-tuned approach, our method achieves\nsignificant improvements.", "published": "2024-01-04", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.02173v1"}
{"title": "Shayona@SMM4H23: COVID-19 Self diagnosis classification using BERT and LightGBM models", "author": "Rushi Chavda, Darshan Makwana, Vraj Patel, Anupam Shukla", "abstract": "This paper describes approaches and results for shared Task 1 and 4 of\nSMMH4-23 by Team Shayona. Shared Task-1 was binary classification of english\ntweets self-reporting a COVID-19 diagnosis, and Shared Task-4 was Binary\nclassification of English Reddit posts self-reporting a social anxiety disorder\ndiagnosis. Our team has achieved the highest f1-score 0.94 in Task-1 among all\nparticipants. We have leveraged the Transformer model (BERT) in combination\nwith the LightGBM model for both tasks.", "published": "2024-01-04", "categories": ["cs.CL", "cs.AI"], "links": "http://arxiv.org/abs/2401.02158v1"}
{"title": "Disentangle Estimation of Causal Effects from Cross-Silo Data", "author": "Yuxuan Liu, Haozhao Wang, Shuang Wang, Zhiming He, Wenchao Xu, Jialiang Zhu, Fan Yang", "abstract": "Estimating causal effects among different events is of great importance to\ncritical fields such as drug development. Nevertheless, the data features\nassociated with events may be distributed across various silos and remain\nprivate within respective parties, impeding direct information exchange between\nthem. This, in turn, can result in biased estimations of local causal effects,\nwhich rely on the characteristics of only a subset of the covariates. To tackle\nthis challenge, we introduce an innovative disentangle architecture designed to\nfacilitate the seamless cross-silo transmission of model parameters, enriched\nwith causal mechanisms, through a combination of shared and private branches.\nBesides, we introduce global constraints into the equation to effectively\nmitigate bias within the various missing domains, thereby elevating the\naccuracy of our causal effect estimation. Extensive experiments conducted on\nnew semi-synthetic datasets show that our method outperforms state-of-the-art\nbaselines.", "published": "2024-01-04", "categories": ["cs.LG", "cs.AI", "cs.CR", "stat.ME"], "links": "http://arxiv.org/abs/2401.02154v1"}
{"title": "Unit Testing in ASP Revisited: Language and Test-Driven Development Environment", "author": "Giovanni Amendola, Tobias Berei, Giuseppe Mazzotta, Francesco Ricca", "abstract": "Unit testing frameworks are nowadays considered a best practice, included in\nalmost all modern software development processes, to achieve rapid development\nof correct specifications. Knowledge representation and reasoning paradigms\nsuch as Answer Set Programming (ASP), that have been used in industry-level\napplications, are not an exception. Indeed, the first unit testing\nspecification language for ASP was proposed in 2011 as a feature of the ASPIDE\ndevelopment environment. Later, a more portable unit testing language was\nincluded in the LANA annotation language. In this paper we revisit both\nlanguages and tools for unit testing in ASP. We propose a new unit test\nspecification language that allows one to inline tests within ASP programs, and\nwe identify the computational complexity of the tasks associated with checking\nthe various program-correctness assertions. Test-case specifications are\ntransparent to the traditional evaluation, but can be interpreted by a specific\ntesting tool. Thus, we present a novel environment supporting test driven\ndevelopment of ASP programs.", "published": "2024-01-04", "categories": ["cs.SE", "cs.AI"], "links": "http://arxiv.org/abs/2401.02153v1"}
{"title": "Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy and Directions", "author": "Cheng-Te Li, Yu-Che Tsai, Chih-Yao Chen, Jay Chiehen Liao", "abstract": "In this survey, we dive into Tabular Data Learning (TDL) using Graph Neural\nNetworks (GNNs), a domain where deep learning-based approaches have\nincreasingly shown superior performance in both classification and regression\ntasks compared to traditional methods. The survey highlights a critical gap in\ndeep neural TDL methods: the underrepresentation of latent correlations among\ndata instances and feature values. GNNs, with their innate capability to model\nintricate relationships and interactions between diverse elements of tabular\ndata, have garnered significant interest and application across various TDL\ndomains. Our survey provides a systematic review of the methods involved in\ndesigning and implementing GNNs for TDL (GNN4TDL). It encompasses a detailed\ninvestigation into the foundational aspects and an overview of GNN-based TDL\nmethods, offering insights into their evolving landscape. We present a\ncomprehensive taxonomy focused on constructing graph structures and\nrepresentation learning within GNN-based TDL methods. In addition, the survey\nexamines various training plans, emphasizing the integration of auxiliary tasks\nto enhance the effectiveness of instance representations. A critical part of\nour discussion is dedicated to the practical application of GNNs across a\nspectrum of GNN4TDL scenarios, demonstrating their versatility and impact.\nLastly, we discuss the limitations and propose future research directions,\naiming to spur advancements in GNN4TDL. This survey serves as a resource for\nresearchers and practitioners, offering a thorough understanding of GNNs' role\nin revolutionizing TDL and pointing towards future innovations in this\npromising area.", "published": "2024-01-04", "categories": ["cs.LG", "cs.AI", "cs.IR", "cs.SI"], "links": "http://arxiv.org/abs/2401.02143v1"}
{"title": "SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment", "author": "Ziping Ma, Furong Xu, Jian Liu, Ming Yang, Qingpei Guo", "abstract": "Multimodal alignment between language and vision is the fundamental topic in\ncurrent vision-language model research. Contrastive Captioners (CoCa), as a\nrepresentative method, integrates Contrastive Language-Image Pretraining (CLIP)\nand Image Caption (IC) into a unified framework, resulting in impressive\nresults. CLIP imposes a bidirectional constraints on global representation of\nentire images and sentences. Although IC conducts an unidirectional\nimage-to-text generation on local representation, it lacks any constraint on\nlocal text-to-image reconstruction, which limits the ability to understand\nimages at a fine-grained level when aligned with texts. To achieve multimodal\nalignment from both global and local perspectives, this paper proposes\nSymmetrizing Contrastive Captioners (SyCoCa), which introduces bidirectional\ninteractions on images and texts across the global and local representation\nlevels. Specifically, we expand a Text-Guided Masked Image Modeling (TG-MIM)\nhead based on ITC and IC heads. The improved SyCoCa can further leverage\ntextual cues to reconstruct contextual images and visual cues to predict\ntextual contents. When implementing bidirectional local interactions, the local\ncontents of images tend to be cluttered or unrelated to their textual\ndescriptions. Thus, we employ an attentive masking strategy to select effective\nimage patches for interaction. Extensive experiments on five vision-language\ntasks, including image-text retrieval, image-captioning, visual question\nanswering, and zero-shot/finetuned image classification, validate the\neffectiveness of our proposed method.", "published": "2024-01-04", "categories": ["cs.CV", "cs.AI"], "links": "http://arxiv.org/abs/2401.02137v1"}
{"title": "DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models", "author": "Wendi Cui, Jiaxin Zhang, Zhuohang Li, Lopez Damien, Kamalika Das, Bradley Malin, Sricharan Kumar", "abstract": "Evaluating the quality and variability of text generated by Large Language\nModels (LLMs) poses a significant, yet unresolved research challenge.\nTraditional evaluation methods, such as ROUGE and BERTScore, which measure\ntoken similarity, often fail to capture the holistic semantic equivalence. This\nresults in a low correlation with human judgments and intuition, which is\nespecially problematic in high-stakes applications like healthcare and finance\nwhere reliability, safety, and robust decision-making are highly critical. This\nwork proposes DCR, an automated framework for evaluating and improving the\nconsistency of LLM-generated texts using a divide-conquer-reasoning approach.\nUnlike existing LLM-based evaluators that operate at the paragraph level, our\nmethod employs a divide-and-conquer evaluator (DCE) that breaks down the\nparagraph-to-paragraph comparison between two generated responses into\nindividual sentence-to-paragraph comparisons, each evaluated based on\npredefined criteria. To facilitate this approach, we introduce an automatic\nmetric converter (AMC) that translates the output from DCE into an\ninterpretable numeric score. Beyond the consistency evaluation, we further\npresent a reason-assisted improver (RAI) that leverages the analytical reasons\nwith explanations identified by DCE to generate new responses aimed at reducing\nthese inconsistencies. Through comprehensive and systematic empirical analysis,\nwe show that our approach outperforms state-of-the-art methods by a large\nmargin (e.g., +19.3% and +24.3% on the SummEval dataset) in evaluating the\nconsistency of LLM generation across multiple benchmarks in semantic, factual,\nand summarization consistency tasks. Our approach also substantially reduces\nnearly 90% of output inconsistencies, showing promise for effective\nhallucination mitigation.", "published": "2024-01-04", "categories": ["cs.CL", "cs.AI"], "links": "http://arxiv.org/abs/2401.02132v1"}
{"title": "ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach", "author": "Zeynep Hilal Kilimci, Mustafa Yalcin", "abstract": "Anticancer peptides (ACPs) are a class of molecules that have gained\nsignificant attention in the field of cancer research and therapy. ACPs are\nshort chains of amino acids, the building blocks of proteins, and they possess\nthe ability to selectively target and kill cancer cells. One of the key\nadvantages of ACPs is their ability to selectively target cancer cells while\nsparing healthy cells to a greater extent. This selectivity is often attributed\nto differences in the surface properties of cancer cells compared to normal\ncells. That is why ACPs are being investigated as potential candidates for\ncancer therapy. ACPs may be used alone or in combination with other treatment\nmodalities like chemotherapy and radiation therapy. While ACPs hold promise as\na novel approach to cancer treatment, there are challenges to overcome,\nincluding optimizing their stability, improving selectivity, and enhancing\ntheir delivery to cancer cells, continuous increasing in number of peptide\nsequences, developing a reliable and precise prediction model. In this work, we\npropose an efficient transformer-based framework to identify anticancer\npeptides for by performing accurate a reliable and precise prediction model.\nFor this purpose, four different transformer models, namely ESM, ProtBert,\nBioBERT, and SciBERT are employed to detect anticancer peptides from amino acid\nsequences. To demonstrate the contribution of the proposed framework, extensive\nexperiments are carried on widely-used datasets in the literature, two versions\nof AntiCp2, cACP-DeepGram, ACP-740. Experiment results show the usage of\nproposed model enhances classification accuracy when compared to the\nstate-of-the-art studies. The proposed framework, ESM, exhibits 96.45 of\naccuracy for AntiCp2 dataset, 97.66 of accuracy for cACP-DeepGram dataset, and\n88.51 of accuracy for ACP-740 dataset, thence determining new state-of-the-art.", "published": "2024-01-04", "categories": ["q-bio.BM", "cs.AI", "cs.CE", "cs.LG"], "links": "http://arxiv.org/abs/2401.02124v1"}
{"title": "Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation", "author": "Zipeng Fu, Tony Z. Zhao, Chelsea Finn", "abstract": "Imitation learning from human demonstrations has shown impressive performance\nin robotics. However, most results focus on table-top manipulation, lacking the\nmobility and dexterity necessary for generally useful tasks. In this work, we\ndevelop a system for imitating mobile manipulation tasks that are bimanual and\nrequire whole-body control. We first present Mobile ALOHA, a low-cost and\nwhole-body teleoperation system for data collection. It augments the ALOHA\nsystem with a mobile base, and a whole-body teleoperation interface. Using data\ncollected with Mobile ALOHA, we then perform supervised behavior cloning and\nfind that co-training with existing static ALOHA datasets boosts performance on\nmobile manipulation tasks. With 50 demonstrations for each task, co-training\ncan increase success rates by up to 90%, allowing Mobile ALOHA to autonomously\ncomplete complex mobile manipulation tasks such as sauteing and serving a piece\nof shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling\nand entering an elevator, and lightly rinsing a used pan using a kitchen\nfaucet. Project website: https://mobile-aloha.github.io", "published": "2024-01-04", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "links": "http://arxiv.org/abs/2401.02117v1"}
{"title": "k-Winners-Take-All Ensemble Neural Network", "author": "Abien Fred Agarap, Arnulfo P. Azcarraga", "abstract": "Ensembling is one approach that improves the performance of a neural network\nby combining a number of independent neural networks, usually by either\naveraging or summing up their individual outputs. We modify this ensembling\napproach by training the sub-networks concurrently instead of independently.\nThis concurrent training of sub-networks leads them to cooperate with each\nother, and we refer to them as \"cooperative ensemble\". Meanwhile, the\nmixture-of-experts approach improves a neural network performance by dividing\nup a given dataset to its sub-networks. It then uses a gating network that\nassigns a specialization to each of its sub-networks called \"experts\". We\nimprove on these aforementioned ways for combining a group of neural networks\nby using a k-Winners-Take-All (kWTA) activation function, that acts as the\ncombination method for the outputs of each sub-network in the ensemble. We\nrefer to this proposed model as \"kWTA ensemble neural networks\" (kWTA-ENN).\nWith the kWTA activation function, the losing neurons of the sub-networks are\ninhibited while the winning neurons are retained. This results in sub-networks\nhaving some form of specialization but also sharing knowledge with one another.\nWe compare our approach with the cooperative ensemble and mixture-of-experts,\nwhere we used a feed-forward neural network with one hidden layer having 100\nneurons as the sub-network architecture. Our approach yields a better\nperformance compared to the baseline models, reaching the following test\naccuracies on benchmark datasets: 98.34% on MNIST, 88.06% on Fashion-MNIST,\n91.56% on KMNIST, and 95.97% on WDBC.", "published": "2024-01-04", "categories": ["cs.NE", "cs.AI"], "links": "http://arxiv.org/abs/2401.02092v1"}
{"title": "An Example of Evolutionary Computation + Large Language Model Beating Human: Design of Efficient Guided Local Search", "author": "Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, Qingfu Zhang", "abstract": "It is often very tedious for human experts to design efficient algorithms.\nRecently, we have proposed a novel Algorithm Evolution using Large Language\nModel (AEL) framework for automatic algorithm design. AEL combines the power of\na large language model and the paradigm of evolutionary computation to design,\ncombine, and modify algorithms automatically. In this paper, we use AEL to\ndesign the guide algorithm for guided local search (GLS) to solve the\nwell-known traveling salesman problem (TSP). AEL automatically evolves elite\nGLS algorithms in two days, with minimal human effort and no model training.\nExperimental results on 1,000 TSP20-TSP100 instances and TSPLib instances show\nthat AEL-designed GLS outperforms state-of-the-art human-designed GLS with the\nsame iteration budget. It achieves a 0% gap on TSP20 and TSP50 and a 0.032% gap\non TSP100 in 1,000 iterations. Our findings mark the emergence of a new era in\nautomatic algorithm design.", "published": "2024-01-04", "categories": ["cs.NE", "cs.AI"], "links": "http://arxiv.org/abs/2401.02051v1"}
{"title": "Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives", "author": "Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu", "abstract": "The reflection capacity of Large Language Model (LLM) has garnered extensive\nattention. A post-hoc prompting strategy, e.g., reflexion and self-refine,\nrefines LLM's response based on self-evaluated or external feedback. However,\nrecent research indicates without external feedback, LLM's intrinsic reflection\nis unstable. Our investigation unveils that the key bottleneck is the quality\nof the self-evaluated feedback. We find LLMs often exhibit overconfidence or\nhigh randomness when self-evaluate, offering stubborn or inconsistent feedback,\nwhich causes poor reflection. To remedy this, we advocate Self-Contrast: It\nadaptively explores diverse solving perspectives tailored to the request,\ncontrasts the differences, and summarizes these discrepancies into a checklist\nwhich could be used to re-examine and eliminate discrepancies. Our method\nendows LLM with diverse perspectives to alleviate stubborn biases. Moreover,\ntheir discrepancies indicate potential errors or inherent uncertainties that\nLLM often overlooks. Reflecting upon these can catalyze more accurate and\nstable reflection. Experiments conducted on a series of reasoning and\ntranslation tasks with different LLMs serve to underscore the effectiveness and\ngenerality of our strategy.", "published": "2024-01-04", "categories": ["cs.CL", "cs.AI"], "links": "http://arxiv.org/abs/2401.02009v1"}
