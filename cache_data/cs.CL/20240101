{"title": "Graph-Convolutional Autoencoder Ensembles for the Humanities, Illustrated with a Study of the American Slave Trade", "author": "Tom Lippincott", "abstract": "We introduce a graph-aware autoencoder ensemble framework, with associated\nformalisms and tooling, designed to facilitate deep learning for scholarship in\nthe humanities. By composing sub-architectures to produce a model isomorphic to\na humanistic domain we maintain interpretability while providing function\nsignatures for each sub-architectural choice, allowing both traditional and\ncomputational researchers to collaborate without disrupting established\npractices. We illustrate a practical application of our approach to a\nhistorical study of the American post-Atlantic slave trade, and make several\nspecific technical contributions: a novel hybrid graph-convolutional\nautoencoder mechanism, batching policies for common graph topologies, and\nmasking techniques for particular use-cases. The effectiveness of the framework\nfor broadening participation of diverse domains is demonstrated by a growing\nsuite of two dozen studies, both collaborations with humanists and established\ntasks from machine learning literature, spanning a variety of fields and data\nmodalities. We make performance comparisons of several different architectural\nchoices and conclude with an ambitious list of imminent next steps for this\nresearch.", "published": "2024-01-01", "categories": ["cs.LG", "cs.CL"], "links": "http://arxiv.org/abs/2401.00824v1"}
{"title": "A Computational Framework for Behavioral Assessment of LLM Therapists", "author": "Yu Ying Chiu, Ashish Sharma, Inna Wanyin Lin, Tim Althoff", "abstract": "The emergence of ChatGPT and other large language models (LLMs) has greatly\nincreased interest in utilizing LLMs as therapists to support individuals\nstruggling with mental health challenges. However, due to the lack of\nsystematic studies, our understanding of how LLM therapists behave, i.e., ways\nin which they respond to clients, is significantly limited. Understanding their\nbehavior across a wide range of clients and situations is crucial to accurately\nassess their capabilities and limitations in the high-risk setting of mental\nhealth, where undesirable behaviors can lead to severe consequences. In this\npaper, we propose BOLT, a novel computational framework to study the\nconversational behavior of LLMs when employed as therapists. We develop an\nin-context learning method to quantitatively measure the behavior of LLMs based\non 13 different psychotherapy techniques including reflections, questions,\nsolutions, normalizing, and psychoeducation. Subsequently, we compare the\nbehavior of LLM therapists against that of high- and low-quality human therapy,\nand study how their behavior can be modulated to better reflect behaviors\nobserved in high-quality therapy. Our analysis of GPT and Llama-variants\nreveals that these LLMs often resemble behaviors more commonly exhibited in\nlow-quality therapy rather than high-quality therapy, such as offering a higher\ndegree of problem-solving advice when clients share emotions, which is against\ntypical recommendations. At the same time, unlike low-quality therapy, LLMs\nreflect significantly more upon clients' needs and strengths. Our analysis\nframework suggests that despite the ability of LLMs to generate anecdotal\nexamples that appear similar to human therapists, LLM therapists are currently\nnot fully consistent with high-quality care, and thus require additional\nresearch to ensure quality care.", "published": "2024-01-01", "categories": ["cs.CL", "cs.HC"], "links": "http://arxiv.org/abs/2401.00820v1"}
{"title": "If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents", "author": "Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, Chengxiang Zhai", "abstract": "The prominent large language models (LLMs) of today differ from past language\nmodels not only in size, but also in the fact that they are trained on a\ncombination of natural language and formal language (code). As a medium between\nhumans and computers, code translates high-level goals into executable steps,\nfeaturing standard syntax, logical consistency, abstraction, and modularity. In\nthis survey, we present an overview of the various benefits of integrating code\ninto LLMs' training data. Specifically, beyond enhancing LLMs in code\ngeneration, we observe that these unique properties of code help (i) unlock the\nreasoning ability of LLMs, enabling their applications to a range of more\ncomplex natural language tasks; (ii) steer LLMs to produce structured and\nprecise intermediate steps, which can then be connected to external execution\nends through function calls; and (iii) take advantage of code compilation and\nexecution environment, which also provides diverse feedback for model\nimprovement. In addition, we trace how these profound capabilities of LLMs,\nbrought by code, have led to their emergence as intelligent agents (IAs) in\nsituations where the ability to understand instructions, decompose goals, plan\nand execute actions, and refine from feedback are crucial to their success on\ndownstream tasks. Finally, we present several key challenges and future\ndirections of empowering LLMs with code.", "published": "2024-01-01", "categories": ["cs.CL"], "links": "http://arxiv.org/abs/2401.00812v1"}
{"title": "PerSHOP -- A Persian dataset for shopping dialogue systems modeling", "author": "Keyvan Mahmoudi, Heshaam Faili", "abstract": "Nowadays, dialogue systems are used in many fields of industry and research.\nThere are successful instances of these systems, such as Apple Siri, Google\nAssistant, and IBM Watson. Task-oriented dialogue system is a category of\nthese, that are used in specific tasks. They can perform tasks such as booking\nplane tickets or making restaurant reservations. Shopping is one of the most\npopular areas on these systems. The bot replaces the human salesperson and\ninteracts with the customers by speaking. To train the models behind the scenes\nof these systems, annotated data is needed. In this paper, we developed a\ndataset of dialogues in the Persian language through crowd-sourcing. We\nannotated these dialogues to train a model. This dataset contains nearly 22k\nutterances in 15 different domains and 1061 dialogues. This is the largest\nPersian dataset in this field, which is provided freely so that future\nresearchers can use it. Also, we proposed some baseline models for natural\nlanguage understanding (NLU) tasks. These models perform two tasks for NLU:\nintent classification and entity extraction. The F-1 score metric obtained for\nintent classification is around 91% and for entity extraction is around 93%,\nwhich can be a baseline for future research.", "published": "2024-01-01", "categories": ["cs.CL", "cs.HC"], "links": "http://arxiv.org/abs/2401.00811v1"}
{"title": "SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models", "author": "Jinglong Luo, Yehong Zhang, Jiaqi Zhang, Xin Mu, Hui Wang, Yue Yu, Zenglin Xu", "abstract": "With the growing use of large language models hosted on cloud platforms to\noffer inference services, privacy concerns are escalating, especially\nconcerning sensitive data like investment plans and bank account details.\nSecure Multi-Party Computing (SMPC) emerges as a promising solution to protect\nthe privacy of inference data and model parameters. However, the application of\nSMPC in Privacy-Preserving Inference (PPI) for large language models,\nparticularly those based on the Transformer architecture, often leads to\nconsiderable slowdowns or declines in performance. This is largely due to the\nmultitude of nonlinear operations in the Transformer architecture, which are\nnot well-suited to SMPC and are difficult to circumvent or optimize\neffectively. To address this concern, we introduce an advanced optimization\nframework called SecFormer, designed to strike an optimal balance between\nperformance and efficiency in PPI for Transformer models. By implementing\nknowledge distillation techniques, we successfully eliminate the high-cost\nexponential and maximum operations in PPI without sacrificing model\nperformance. Additionally, we have developed a suite of efficient SMPC\nprotocols that utilize segmented polynomials and Goldschmidt's method to handle\nother complex nonlinear functions within PPI, such as GeLU, LayerNorm, and\nSoftmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer\nin performance, showing improvements of $5.6\\%$ and $24.2\\%$ for\nBERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In terms of\nefficiency, SecFormer is 3.4 and 3.2 times faster than Puma, demonstrating its\neffectiveness and speed.", "published": "2024-01-01", "categories": ["cs.LG", "cs.CL", "cs.CR"], "links": "http://arxiv.org/abs/2401.00793v1"}
{"title": "Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models", "author": "Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries, Qian Liu, Niklas Muennighoff", "abstract": "The high cost of full-parameter fine-tuning (FFT) of Large Language Models\n(LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods.\nHowever, it remains unclear which methods provide the best cost-performance\ntrade-off at different model scales. We introduce Astraios, a suite of 28\ninstruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up\nto 16 billion parameters. Through investigations across 5 tasks and 8 different\ndatasets encompassing both code comprehension and code generation tasks, we\nfind that FFT generally leads to the best downstream performance across all\nscales, and PEFT methods differ significantly in their efficacy based on the\nmodel scale. LoRA usually offers the most favorable trade-off between cost and\nperformance. Further investigation into the effects of these methods on both\nmodel robustness and code security reveals that larger models tend to\ndemonstrate reduced robustness and less security. At last, we explore the\nrelationships among updated parameters, cross-entropy loss, and task\nperformance. We find that the tuning effectiveness observed in small models\ngeneralizes well to larger models, and the validation loss in instruction\ntuning can be a reliable indicator of overall downstream performance.", "published": "2024-01-01", "categories": ["cs.CL", "cs.AI", "cs.SE"], "links": "http://arxiv.org/abs/2401.00788v1"}
{"title": "Temporal Validity Change Prediction", "author": "Georg Wenzel, Adam Jatowt", "abstract": "Temporal validity is an important property of text that is useful for many\ndownstream applications, such as recommender systems, conversational AI, or\nstory understanding. Existing benchmarking tasks often require models to\nidentify the temporal validity duration of a single statement. However, in many\ncases, additional contextual information, such as sentences in a story or posts\non a social media profile, can be collected from the available text stream.\nThis contextual information may greatly alter the duration for which a\nstatement is expected to be valid. We propose Temporal Validity Change\nPrediction, a natural language processing task benchmarking the capability of\nmachine learning models to detect contextual statements that induce such\nchange. We create a dataset consisting of temporal target statements sourced\nfrom Twitter and crowdsource sample context statements. We then benchmark a set\nof transformer-based language models on our dataset. Finally, we experiment\nwith temporal validity duration prediction as an auxiliary task to improve the\nperformance of the state-of-the-art model.", "published": "2024-01-01", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "links": "http://arxiv.org/abs/2401.00779v1"}
{"title": "New Job, New Gender? Measuring the Social Bias in Image Generation Models", "author": "Wenxuan Wang, Haonan Bai, Jen-tse Huang, Yuxuan Wan, Youliang Yuan, Haoyi Qiu, Nanyun Peng, Michael R. Lyu", "abstract": "Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel metamorphic testing\nframework that can accurately, automatically and comprehensively trigger social\nbias in image generation models. BiasPainter uses a diverse range of seed\nimages of individuals and prompts the image generation models to edit these\nimages using gender, race, and age-neutral queries. These queries span 62\nprofessions, 39 activities, 57 types of objects, and 70 personality traits. The\nframework then compares the edited images to the original seed images, focusing\non any changes related to gender, race, and age. BiasPainter adopts a testing\noracle that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. To evaluate the\neffectiveness of BiasPainter, we use BiasPainter to test five widely-used\ncommercial image generation software and models, such as stable diffusion and\nMidjourney. Experimental results show that 100\\% of the generated test cases\ncan successfully trigger social bias in image generation models.", "published": "2024-01-01", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "links": "http://arxiv.org/abs/2401.00763v1"}
{"title": "The Earth is Flat? Unveiling Factual Errors in Large Language Models", "author": "Wenxuan Wang, Juluan Shi, Zhaopeng Tu, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu", "abstract": "Large Language Models (LLMs) like ChatGPT are foundational in various\napplications due to their extensive knowledge from pre-training and\nfine-tuning. Despite this, they are prone to generating factual and commonsense\nerrors, raising concerns in critical areas like healthcare, journalism, and\neducation to mislead users. Current methods for evaluating LLMs' veracity are\nlimited by test data leakage or the need for extensive human labor, hindering\nefficient and accurate error detection. To tackle this problem, we introduce a\nnovel, automatic testing framework, FactChecker, aimed at uncovering factual\ninaccuracies in LLMs. This framework involves three main steps: First, it\nconstructs a factual knowledge graph by retrieving fact triplets from a\nlarge-scale knowledge database. Then, leveraging the knowledge graph,\nFactChecker employs a rule-based approach to generates three types of questions\n(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and\nmulti-hop relations, along with correct answers. Lastly, it assesses the LLMs'\nresponses for accuracy using tailored matching strategies for each question\ntype. Our extensive tests on six prominent LLMs, including text-davinci-002,\ntext-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal\nthat FactChecker can trigger factual errors in up to 45\\% of questions in these\nmodels. Moreover, we demonstrate that FactChecker's test cases can improve\nLLMs' factual accuracy through in-context learning and fine-tuning (e.g.,\nllama-2-13b-chat's accuracy increase from 35.3\\% to 68.5\\%). We are making all\ncode, data, and results available for future research endeavors.", "published": "2024-01-01", "categories": ["cs.SE", "cs.AI", "cs.CL"], "links": "http://arxiv.org/abs/2401.00761v1"}
{"title": "A & B == B & A: Triggering Logical Reasoning Failures in Large Language Models", "author": "Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-tse Huang, Pinjia He, Wenxiang Jiao, Michael R. Lyu", "abstract": "Recent advancements in large language models (LLMs) have propelled Artificial\nIntelligence (AI) to new heights, enabling breakthroughs in various tasks such\nas writing assistance, code generation, and machine translation. A significant\ndistinction of advanced LLMs, such as ChatGPT, is their demonstrated ability to\n\"reason.\" However, evaluating the reasoning ability of LLMs remains a challenge\nas most existing evaluations focus on their accuracy on the downstream tasks\nrather than directly assessing their reasoning processes. Efforts have been\nmade to develop benchmarks and metrics to assess reasoning in LLMs, but they\nsuffer from data leakage or limited scope. In this paper, we introduce\nLogicAsker, an automatic approach that comprehensively evaluates and improves\nthe logical reasoning abilities of LLMs under a set of atomic reasoning skills\nbased on propositional and predicate logic. The results provide insights into\nLLMs' reasoning abilities and reveal the logical rules the LLMs did not learn\nwell. We evaluate LogicAsker on six widely deployed LLMs, including GPT-3,\nChatGPT, GPT-4, Bard, Vicuna, and Guanaco. The results show that test cases\nfrom LogicAsker can find logical reasoning failures in different LLMs with a\nrate of 25\\% - 94\\%. In addition, the test cases of LogicAsker can be further\nused to design demonstration examples for in-context learning, which\neffectively improves the logical reasoning ability of LLMs, e.g., 10\\% for\nGPT-4. As far as we know, our work is the first to create prompts based on\ntesting results to improve LLMs' formal reasoning ability effectively. All the\ncode, data, and results will be released for reproduction and future research.", "published": "2024-01-01", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LO"], "links": "http://arxiv.org/abs/2401.00757v1"}
{"title": "Machine Translation Testing via Syntactic Tree Pruning", "author": "Quanjun Zhang, Juan Zhai, Chunrong Fang, Jiawei Liu, Weisong Sun, Haichuan Hu, Qingyu Wang", "abstract": "Machine translation systems have been widely adopted in our daily life,\nmaking life easier and more convenient. Unfortunately, erroneous translations\nmay result in severe consequences, such as financial losses. This requires to\nimprove the accuracy and the reliability of machine translation systems.\nHowever, it is challenging to test machine translation systems because of the\ncomplexity and intractability of the underlying neural models. To tackle these\nchallenges, we propose a novel metamorphic testing approach by syntactic tree\npruning (STP) to validate machine translation systems. Our key insight is that\na pruned sentence should have similar crucial semantics compared with the\noriginal sentence. Specifically, STP (1) proposes a core semantics-preserving\npruning strategy by basic sentence structure and dependency relations on the\nlevel of syntactic tree representation; (2) generates source sentence pairs\nbased on the metamorphic relation; (3) reports suspicious issues whose\ntranslations break the consistency property by a bag-of-words model. We further\nevaluate STP on two state-of-the-art machine translation systems (i.e., Google\nTranslate and Bing Microsoft Translator) with 1,200 source sentences as inputs.\nThe results show that STP can accurately find 5,073 unique erroneous\ntranslations in Google Translate and 5,100 unique erroneous translations in\nBing Microsoft Translator (400% more than state-of-the-art techniques), with\n64.5% and 65.4% precision, respectively. The reported erroneous translations\nvary in types and more than 90% of them cannot be found by state-of-the-art\ntechniques. There are 9,393 erroneous translations unique to STP, which is\n711.9% more than state-of-the-art techniques. Moreover, STP is quite effective\nto detect translation errors for the original sentences with a recall reaching\n74.0%, improving state-of-the-art techniques by 55.1% on average.", "published": "2024-01-01", "categories": ["cs.CL", "cs.SE"], "links": "http://arxiv.org/abs/2401.00751v1"}
{"title": "ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios", "author": "Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, Xuanjing Huang", "abstract": "Existing evaluations of tool learning primarily focus on validating the\nalignment of selected tools for large language models (LLMs) with expected\noutcomes. However, these approaches rely on a limited set of scenarios where\nanswers can be pre-determined, diverging from genuine needs. Furthermore, a\nsole emphasis on outcomes disregards the intricate capabilities essential for\nLLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a\nfine-grained system tailored for the evaluation of the LLMs' tool learning\ncapabilities in authentic scenarios. The system meticulously examines seven\nreal-world scenarios, analyzing five dimensions crucial to LLMs in tool\nlearning: format alignment, intent comprehension, behavior planning, tool\nselection, and answer organization. Additionally, ToolEyes incorporates a tool\nlibrary boasting approximately 600 tools, serving as an intermediary between\nLLMs and the physical world. Evaluations involving ten LLMs across three\ncategories reveal a preference for specific scenarios and limited cognitive\nabilities in tool learning. Intriguingly, expanding the model size even\nexacerbates the hindrance to tool learning. These findings offer instructive\ninsights aimed at advancing the field of tool learning. The data is available\natt https://github.com/Junjie-Ye/ToolEyes.git.", "published": "2024-01-01", "categories": ["cs.CL", "cs.AI"], "links": "http://arxiv.org/abs/2401.00741v1"}
{"title": "Large Language Models aren't all that you need", "author": "Kiran Voderhobli Holla, Chaithanya Kumar, Aryan Singh", "abstract": "This paper describes the architecture and systems built towards solving the\nSemEval 2023 Task 2: MultiCoNER II (Multilingual Complex Named Entity\nRecognition) [1]. We evaluate two approaches (a) a traditional Conditional\nRandom Fields model and (b) a Large Language Model (LLM) fine-tuned with a\ncustomized head and compare the two approaches. The novel ideas explored are:\n1) Decaying auxiliary loss (with residual) - where we train the model on an\nauxiliary task of Coarse-Grained NER and include this task as a part of the\nloss function 2) Triplet token blending - where we explore ways of blending the\nembeddings of neighboring tokens in the final NER layer prior to prediction 3)\nTask-optimal heads - where we explore a variety of custom heads and learning\nrates for the final layer of the LLM. We also explore multiple LLMs including\nGPT-3 and experiment with a variety of dropout and other hyperparameter\nsettings before arriving at our final model which achieves micro & macro f1 of\n0.85/0.84 (on dev) and 0.67/0.61 on the test data . We show that while\npre-trained LLMs, by themselves, bring about a large improvement in scores as\ncompared to traditional models, we also demonstrate that tangible improvements\nto the Macro-F1 score can be made by augmenting the LLM with additional\nfeature/loss/model engineering techniques described above.", "published": "2024-01-01", "categories": ["cs.CL", "cs.AI", "cs.LG"], "links": "http://arxiv.org/abs/2401.00698v1"}
{"title": "Benchmarking Large Language Models on Controllable Generation under Diversified Instructions", "author": "Yihan Chen, Benfeng Xu, Quan Wang, Yi Liu, Zhendong Mao", "abstract": "While large language models (LLMs) have exhibited impressive\ninstruction-following capabilities, it is still unclear whether and to what\nextent they can respond to explicit constraints that might be entailed in\nvarious instructions. As a significant aspect of LLM alignment, it is thus\nimportant to formulate such a specialized set of instructions as well as\ninvestigate the resulting behavior of LLMs. To address this vacancy, we propose\na new benchmark CoDI-Eval to systematically and comprehensively evaluate LLMs'\nresponses to instructions with various constraints. We construct a large\ncollection of constraints-attributed instructions as a test suite focused on\nboth generalization and coverage. Specifically, we advocate an instruction\ndiversification process to synthesize diverse forms of constraint expression\nand also deliberate the candidate task taxonomy with even finer-grained\nsub-categories. Finally, we automate the entire evaluation process to\nfacilitate further developments. Different from existing studies on\ncontrollable text generation, CoDI-Eval extends the scope to the prevalent\ninstruction-following paradigm for the first time. We provide extensive\nevaluations of representative LLMs (e.g., ChatGPT, Vicuna) on CoDI-Eval,\nrevealing their limitations in following instructions with specific constraints\nand there is still a significant gap between open-source and commercial\nclosed-source LLMs. We believe this benchmark will facilitate research into\nimproving the controllability of LLMs' responses to instructions. Our data and\ncode are available at https://github.com/Xt-cyh/CoDI-Eval.", "published": "2024-01-01", "categories": ["cs.CL"], "links": "http://arxiv.org/abs/2401.00690v1"}
{"title": "Large language model for Bible sentiment analysis: Sermon on the Mount", "author": "Mahek Vora, Tom Blau, Vansh Kachhwal, Ashu M. G. Solo, Rohitash Chandra", "abstract": "The revolution of natural language processing via large language models has\nmotivated its use in multidisciplinary areas that include social sciences and\nhumanities and more specifically, comparative religion. Sentiment analysis\nprovides a mechanism to study the emotions expressed in text. Recently,\nsentiment analysis has been used to study and compare translations of the\nBhagavad Gita, which is a fundamental and sacred Hindu text. In this study, we\nuse sentiment analysis for studying selected chapters of the Bible. These\nchapters are known as the Sermon on the Mount. We utilize a pre-trained\nlanguage model for sentiment analysis by reviewing five translations of the\nSermon on the Mount, which include the King James version, the New\nInternational Version, the New Revised Standard Version, the Lamsa Version, and\nthe Basic English Version. We provide a chapter-by-chapter and verse-by-verse\ncomparison using sentiment and semantic analysis and review the major\nsentiments expressed. Our results highlight the varying sentiments across the\nchapters and verses. We found that the vocabulary of the respective\ntranslations is significantly different. We detected different levels of\nhumour, optimism, and empathy in the respective chapters that were used by\nJesus to deliver his message.", "published": "2024-01-01", "categories": ["cs.CL", "cs.AI"], "links": "http://arxiv.org/abs/2401.00689v1"}
{"title": "Digger: Detecting Copyright Content Mis-usage in Large Language Model Training", "author": "Haodong Li, Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu, Guoai Xu, Guosheng Xu, Haoyu Wang", "abstract": "Pre-training, which utilizes extensive and varied datasets, is a critical\nfactor in the success of Large Language Models (LLMs) across numerous\napplications. However, the detailed makeup of these datasets is often not\ndisclosed, leading to concerns about data security and potential misuse. This\nis particularly relevant when copyrighted material, still under legal\nprotection, is used inappropriately, either intentionally or unintentionally,\ninfringing on the rights of the authors.\n  In this paper, we introduce a detailed framework designed to detect and\nassess the presence of content from potentially copyrighted books within the\ntraining datasets of LLMs. This framework also provides a confidence estimation\nfor the likelihood of each content sample's inclusion. To validate our\napproach, we conduct a series of simulated experiments, the results of which\naffirm the framework's effectiveness in identifying and addressing instances of\ncontent misuse in LLM training processes. Furthermore, we investigate the\npresence of recognizable quotes from famous literary works within these\ndatasets. The outcomes of our study have significant implications for ensuring\nthe ethical use of copyrighted materials in the development of LLMs,\nhighlighting the need for more transparent and responsible data management\npractices in this field.", "published": "2024-01-01", "categories": ["cs.CR", "cs.CL", "cs.LG"], "links": "http://arxiv.org/abs/2401.00676v1"}
{"title": "Predicting Anti-microbial Resistance using Large Language Models", "author": "Hyunwoo Yoo, Bahrad Sokhansanj, James R. Brown, Gail Rosen", "abstract": "During times of increasing antibiotic resistance and the spread of infectious\ndiseases like COVID-19, it is important to classify genes related to antibiotic\nresistance. As natural language processing has advanced with transformer-based\nlanguage models, many language models that learn characteristics of nucleotide\nsequences have also emerged. These models show good performance in classifying\nvarious features of nucleotide sequences. When classifying nucleotide\nsequences, not only the sequence itself, but also various background knowledge\nis utilized. In this study, we use not only a nucleotide sequence-based\nlanguage model but also a text language model based on PubMed articles to\nreflect more biological background knowledge in the model. We propose a method\nto fine-tune the nucleotide sequence language model and the text language model\nbased on various databases of antibiotic resistance genes. We also propose an\nLLM-based augmentation technique to supplement the data and an ensemble method\nto effectively combine the two models. We also propose a benchmark for\nevaluating the model. Our method achieved better performance than the\nnucleotide sequence language model in the drug resistance class prediction.", "published": "2024-01-01", "categories": ["cs.CL"], "links": "http://arxiv.org/abs/2401.00642v1"}
