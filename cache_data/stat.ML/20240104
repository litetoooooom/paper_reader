{"title": "Simulation-Based Inference with Quantile Regression", "author": "He Jia", "abstract": "We present Neural Quantile Estimation (NQE), a novel Simulation-Based\nInference (SBI) method based on conditional quantile regression. NQE\nautoregressively learns individual one dimensional quantiles for each posterior\ndimension, conditioned on the data and previous posterior dimensions. Posterior\nsamples are obtained by interpolating the predicted quantiles using monotonic\ncubic Hermite spline, with specific treatment for the tail behavior and\nmulti-modal distributions. We introduce an alternative definition for the\nBayesian credible region using the local Cumulative Density Function (CDF),\noffering substantially faster evaluation than the traditional Highest Posterior\nDensity Region (HPDR). In case of limited simulation budget and/or known model\nmisspecification, a post-processing broadening step can be integrated into NQE\nto ensure the unbiasedness of the posterior estimation with negligible\nadditional computational cost. We demonstrate that the proposed NQE method\nachieves state-of-the-art performance on a variety of benchmark problems.", "published": "2024-01-04", "categories": ["stat.ML", "cs.LG"], "links": "http://arxiv.org/abs/2401.02413v1"}
{"title": "A Survey Analyzing Generalization in Deep Reinforcement Learning", "author": "Ezgi Korkmaz", "abstract": "Reinforcement learning research obtained significant success and attention\nwith the utilization of deep neural networks to solve problems in high\ndimensional state or action spaces. While deep reinforcement learning policies\nare currently being deployed in many different fields from medical applications\nto self driving vehicles, there are still ongoing questions the field is trying\nto answer on the generalization capabilities of deep reinforcement learning\npolicies. In this paper, we will outline the fundamental reasons why deep\nreinforcement learning policies encounter overfitting problems that limit their\nrobustness and generalization capabilities. Furthermore, we will formalize and\nunify the diverse solution approaches to increase generalization, and overcome\noverfitting in state-action value functions. We believe our study can provide a\ncompact systematic unified analysis for the current advancements in deep\nreinforcement learning, and help to construct robust deep neural policies with\nimproved generalization abilities.", "published": "2024-01-04", "categories": ["cs.LG", "cs.AI", "stat.ML"], "links": "http://arxiv.org/abs/2401.02349v1"}
{"title": "A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning", "author": "Parvin Malekzadeh, Konstantinos N. Plataniotis, Zissis Poulos, Zeyu Wang", "abstract": "Distributional Reinforcement Learning (RL) estimates return distribution\nmainly by learning quantile values via minimizing the quantile Huber loss\nfunction, entailing a threshold parameter often selected heuristically or via\nhyperparameter search, which may not generalize well and can be suboptimal.\nThis paper introduces a generalized quantile Huber loss function derived from\nWasserstein distance (WD) calculation between Gaussian distributions, capturing\nnoise in predicted (current) and target (Bellman-updated) quantile values.\nCompared to the classical quantile Huber loss, this innovative loss function\nenhances robustness against outliers. Notably, the classical Huber loss\nfunction can be seen as an approximation of our proposed loss, enabling\nparameter adjustment by approximating the amount of noise in the data during\nthe learning process. Empirical tests on Atari games, a common application in\ndistributional RL, and a recent hedging strategy using distributional RL,\nvalidate the effectiveness of our proposed loss function and its potential for\nparameter adjustments in distributional RL.", "published": "2024-01-04", "categories": ["cs.LG", "stat.ML"], "links": "http://arxiv.org/abs/2401.02325v1"}
{"title": "Robust bilinear factor analysis based on the matrix-variate $t$ distribution", "author": "Xuan Ma, Jianhua Zhao, Changchun Shang, Fen Jiang, Philip L. H. Yu", "abstract": "Factor Analysis based on multivariate $t$ distribution ($t$fa) is a useful\nrobust tool for extracting common factors on heavy-tailed or contaminated data.\nHowever, $t$fa is only applicable to vector data. When $t$fa is applied to\nmatrix data, it is common to first vectorize the matrix observations. This\nintroduces two challenges for $t$fa: (i) the inherent matrix structure of the\ndata is broken, and (ii) robustness may be lost, as vectorized matrix data\ntypically results in a high data dimension, which could easily lead to the\nbreakdown of $t$fa. To address these issues, starting from the intrinsic matrix\nstructure of matrix data, a novel robust factor analysis model, namely bilinear\nfactor analysis built on the matrix-variate $t$ distribution ($t$bfa), is\nproposed in this paper. The novelty is that it is capable to simultaneously\nextract common factors for both row and column variables of interest on\nheavy-tailed or contaminated matrix data. Two efficient algorithms for maximum\nlikelihood estimation of $t$bfa are developed. Closed-form expression for the\nFisher information matrix to calculate the accuracy of parameter estimates are\nderived. Empirical studies are conducted to understand the proposed $t$bfa\nmodel and compare with related competitors. The results demonstrate the\nsuperiority and practicality of $t$bfa. Importantly, $t$bfa exhibits a\nsignificantly higher breakdown point than $t$fa, making it more suitable for\nmatrix data.", "published": "2024-01-04", "categories": ["stat.ML", "cs.LG"], "links": "http://arxiv.org/abs/2401.02203v1"}
{"title": "Energy based diffusion generator for efficient sampling of Boltzmann distributions", "author": "Yan Wang, Ling Guo, Hao Wu, Tao Zhou", "abstract": "We introduce a novel sampler called the energy based diffusion generator for\ngenerating samples from arbitrary target distributions. The sampling model\nemploys a structure similar to a variational autoencoder, utilizing a decoder\nto transform latent variables from a simple distribution into random variables\napproximating the target distribution, and we design an encoder based on the\ndiffusion model. Leveraging the powerful modeling capacity of the diffusion\nmodel for complex distributions, we can obtain an accurate variational estimate\nof the Kullback-Leibler divergence between the distributions of the generated\nsamples and the target. Moreover, we propose a decoder based on generalized\nHamiltonian dynamics to further enhance sampling performance. Through empirical\nevaluation, we demonstrate the effectiveness of our method across various\ncomplex distribution functions, showcasing its superiority compared to existing\nmethods.", "published": "2024-01-04", "categories": ["cs.LG", "stat.CO", "stat.ML"], "links": "http://arxiv.org/abs/2401.02080v1"}
{"title": "U-Trustworthy Models.Reliability, Competence, and Confidence in Decision-Making", "author": "Ritwik Vashistha, Arya Farahi", "abstract": "With growing concerns regarding bias and discrimination in predictive models,\nthe AI community has increasingly focused on assessing AI system\ntrustworthiness. Conventionally, trustworthy AI literature relies on the\nprobabilistic framework and calibration as prerequisites for trustworthiness.\nIn this work, we depart from this viewpoint by proposing a novel trust\nframework inspired by the philosophy literature on trust. We present a precise\nmathematical definition of trustworthiness, termed\n$\\mathcal{U}$-trustworthiness, specifically tailored for a subset of tasks\naimed at maximizing a utility function. We argue that a model's\n$\\mathcal{U}$-trustworthiness is contingent upon its ability to maximize Bayes\nutility within this task subset. Our first set of results challenges the\nprobabilistic framework by demonstrating its potential to favor less\ntrustworthy models and introduce the risk of misleading trustworthiness\nassessments. Within the context of $\\mathcal{U}$-trustworthiness, we prove that\nproperly-ranked models are inherently $\\mathcal{U}$-trustworthy. Furthermore,\nwe advocate for the adoption of the AUC metric as the preferred measure of\ntrustworthiness. By offering both theoretical guarantees and experimental\nvalidation, AUC enables robust evaluation of trustworthiness, thereby enhancing\nmodel selection and hyperparameter tuning to yield more trustworthy outcomes.", "published": "2024-01-04", "categories": ["stat.ML", "cs.LG"], "links": "http://arxiv.org/abs/2401.02062v1"}
{"title": "Neural Collapse for Cross-entropy Class-Imbalanced Learning with Unconstrained ReLU Feature Model", "author": "Hien Dang, Tho Tran, Tan Nguyen, Nhat Ho", "abstract": "The current paradigm of training deep neural networks for classification\ntasks includes minimizing the empirical risk that pushes the training loss\nvalue towards zero, even after the training error has been vanished. In this\nterminal phase of training, it has been observed that the last-layer features\ncollapse to their class-means and these class-means converge to the vertices of\na simplex Equiangular Tight Frame (ETF). This phenomenon is termed as Neural\nCollapse (NC). To theoretically understand this phenomenon, recent works employ\na simplified unconstrained feature model to prove that NC emerges at the global\nsolutions of the training problem. However, when the training dataset is\nclass-imbalanced, some NC properties will no longer be true. For example, the\nclass-means geometry will skew away from the simplex ETF when the loss\nconverges. In this paper, we generalize NC to imbalanced regime for\ncross-entropy loss under the unconstrained ReLU feature model. We prove that,\nwhile the within-class features collapse property still holds in this setting,\nthe class-means will converge to a structure consisting of orthogonal vectors\nwith different lengths. Furthermore, we find that the classifier weights are\naligned to the scaled and centered class-means with scaling factors depend on\nthe number of training samples of each class, which generalizes NC in the\nclass-balanced setting. We empirically prove our results through experiments on\npractical architectures and dataset.", "published": "2024-01-04", "categories": ["cs.LG", "stat.ML"], "links": "http://arxiv.org/abs/2401.02058v1"}
