{"title": "Hierarchical Clustering in $Λ$CDM Cosmologies via Persistence Energy", "author": "Michael Etienne Van Huffel, Leonardo Aldo Alejandro Barberi, Tobias Sagis", "abstract": "In this research, we investigate the structural evolution of the cosmic web,\nemploying advanced methodologies from Topological Data Analysis. Our approach\ninvolves leveraging $Persistence$ $Signals$, an innovative method from recent\nliterature that facilitates the embedding of persistence diagrams into vector\nspaces by re-conceptualizing them as signals in $\\mathbb R^2_+$. Utilizing this\nmethodology, we analyze three quintessential cosmic structures: clusters,\nfilaments, and voids. A central discovery is the correlation between\n$Persistence$ $Energy$ and redshift values, linking persistent homology with\ncosmic evolution and providing insights into the dynamics of cosmic structures.", "published": "2024-01-03", "categories": ["astro-ph.CO", "cs.CG", "math.AT", "stat.ML"], "links": "http://arxiv.org/abs/2401.01988v1"}
{"title": "Beyond Regrets: Geometric Metrics for Bayesian Optimization", "author": "Jungtaek Kim", "abstract": "Bayesian optimization is a principled optimization strategy for a black-box\nobjective function. It shows its effectiveness in a wide variety of real-world\napplications such as scientific discovery and experimental design. In general,\nthe performance of Bayesian optimization is assessed by regret-based metrics\nsuch as instantaneous, simple, and cumulative regrets. These metrics only rely\non function evaluations, so that they do not consider geometric relationships\nbetween query points and global solutions, or query points themselves. Notably,\nthey cannot discriminate if multiple global solutions are successfully found.\nMoreover, they do not evaluate Bayesian optimization's abilities to exploit and\nexplore a search space given. To tackle these issues, we propose four new\ngeometric metrics, i.e., precision, recall, average degree, and average\ndistance. These metrics allow us to compare Bayesian optimization algorithms\nconsidering the geometry of both query points and global optima, or query\npoints. However, they are accompanied by an extra parameter, which needs to be\ncarefully determined. We therefore devise the parameter-free forms of the\nrespective metrics by integrating out the additional parameter. Finally, we\nempirically validate that our proposed metrics can provide more convincing\ninterpretation and understanding of Bayesian optimization algorithms from\ndistinct perspectives, compared to the conventional metrics.", "published": "2024-01-03", "categories": ["cs.LG", "stat.ML"], "links": "http://arxiv.org/abs/2401.01981v1"}
{"title": "On the hardness of learning under symmetries", "author": "Bobak T. Kiani, Thien Le, Hannah Lawrence, Stefanie Jegelka, Melanie Weber", "abstract": "We study the problem of learning equivariant neural networks via gradient\ndescent. The incorporation of known symmetries (\"equivariance\") into neural\nnets has empirically improved the performance of learning pipelines, in domains\nranging from biology to computer vision. However, a rich yet separate line of\nlearning theoretic research has demonstrated that actually learning shallow,\nfully-connected (i.e. non-symmetric) networks has exponential complexity in the\ncorrelational statistical query (CSQ) model, a framework encompassing gradient\ndescent. In this work, we ask: are known problem symmetries sufficient to\nalleviate the fundamental hardness of learning neural nets with gradient\ndescent? We answer this question in the negative. In particular, we give lower\nbounds for shallow graph neural networks, convolutional networks, invariant\npolynomials, and frame-averaged networks for permutation subgroups, which all\nscale either superpolynomially or exponentially in the relevant input\ndimension. Therefore, in spite of the significant inductive bias imparted via\nsymmetry, actually learning the complete classes of functions represented by\nequivariant neural networks via gradient descent remains hard.", "published": "2024-01-03", "categories": ["cs.LG", "cs.DS", "math.ST", "stat.ML", "stat.TH"], "links": "http://arxiv.org/abs/2401.01869v1"}
{"title": "Optimal cross-learning for contextual bandits with unknown context distributions", "author": "Jon Schneider, Julian Zimmert", "abstract": "We consider the problem of designing contextual bandit algorithms in the\n``cross-learning'' setting of Balseiro et al., where the learner observes the\nloss for the action they play in all possible contexts, not just the context of\nthe current round. We specifically consider the setting where losses are chosen\nadversarially and contexts are sampled i.i.d. from an unknown distribution. In\nthis setting, we resolve an open problem of Balseiro et al. by providing an\nefficient algorithm with a nearly tight (up to logarithmic factors) regret\nbound of $\\widetilde{O}(\\sqrt{TK})$, independent of the number of contexts. As\na consequence, we obtain the first nearly tight regret bounds for the problems\nof learning to bid in first-price auctions (under unknown value distributions)\nand sleeping bandits with a stochastic action set.\n  At the core of our algorithm is a novel technique for coordinating the\nexecution of a learning algorithm over multiple epochs in such a way to remove\ncorrelations between estimation of the unknown distribution and the actions\nplayed by the algorithm. This technique may be of independent interest for\nother learning problems involving estimation of an unknown context\ndistribution.", "published": "2024-01-03", "categories": ["cs.LG", "stat.ML"], "links": "http://arxiv.org/abs/2401.01857v1"}
{"title": "Efficient Computation of Confidence Sets Using Classification on Equidistributed Grids", "author": "Lujie Zhou", "abstract": "Economic models produce moment inequalities, which can be used to form tests\nof the true parameters. Confidence sets (CS) of the true parameters are derived\nby inverting these tests. However, they often lack analytical expressions,\nnecessitating a grid search to obtain the CS numerically by retaining the grid\npoints that pass the test. When the statistic is not asymptotically pivotal,\nconstructing the critical value for each grid point in the parameter space adds\nto the computational burden. In this paper, we convert the computational issue\ninto a classification problem by using a support vector machine (SVM)\nclassifier. Its decision function provides a faster and more systematic way of\ndividing the parameter space into two regions: inside vs. outside of the\nconfidence set. We label those points in the CS as 1 and those outside as -1.\nResearchers can train the SVM classifier on a grid of manageable size and use\nit to determine whether points on denser grids are in the CS or not. We\nestablish certain conditions for the grid so that there is a tuning that allows\nus to asymptotically reproduce the test in the CS. This means that in the\nlimit, a point is classified as belonging to the confidence set if and only if\nit is labeled as 1 by the SVM.", "published": "2024-01-03", "categories": ["econ.EM", "stat.ML"], "links": "http://arxiv.org/abs/2401.01804v1"}
{"title": "Deep learning the Hurst parameter of linear fractional processes and assessing its reliability", "author": "Dániel Boros, Bálint Csanády, Iván Ivkovic, Lóránt Nagy, András Lukács, László Márkus", "abstract": "This research explores the reliability of deep learning, specifically Long\nShort-Term Memory (LSTM) networks, for estimating the Hurst parameter in\nfractional stochastic processes. The study focuses on three types of processes:\nfractional Brownian motion (fBm), fractional Ornstein-Uhlenbeck (fOU) process,\nand linear fractional stable motions (lfsm). The work involves a fast\ngeneration of extensive datasets for fBm and fOU to train the LSTM network on a\nlarge volume of data in a feasible time. The study analyses the accuracy of the\nLSTM network's Hurst parameter estimation regarding various performance\nmeasures like RMSE, MAE, MRE, and quantiles of the absolute and relative\nerrors. It finds that LSTM outperforms the traditional statistical methods in\nthe case of fBm and fOU processes; however, it has limited accuracy on lfsm\nprocesses. The research also delves into the implications of training length\nand valuation sequence length on the LSTM's performance. The methodology is\napplied by estimating the Hurst parameter in Li-ion battery degradation data\nand obtaining confidence bounds for the estimation. The study concludes that\nwhile deep learning methods show promise in parameter estimation of fractional\nprocesses, their effectiveness is contingent on the process type and the\nquality of training data.", "published": "2024-01-03", "categories": ["stat.ML", "cs.AI", "cs.LG", "68T07"], "links": "http://arxiv.org/abs/2401.01789v1"}
{"title": "Model Averaging and Double Machine Learning", "author": "Achim Ahrens, Christian B. Hansen, Mark E. Schaffer, Thomas Wiemann", "abstract": "This paper discusses pairing double/debiased machine learning (DDML) with\nstacking, a model averaging method for combining multiple candidate learners,\nto estimate structural parameters. We introduce two new stacking approaches for\nDDML: short-stacking exploits the cross-fitting step of DDML to substantially\nreduce the computational burden and pooled stacking enforces common stacking\nweights over cross-fitting folds. Using calibrated simulation studies and two\napplications estimating gender gaps in citations and wages, we show that DDML\nwith stacking is more robust to partially unknown functional forms than common\nalternative approaches based on single pre-selected learners. We provide Stata\nand R software implementing our proposals.", "published": "2024-01-03", "categories": ["econ.EM", "stat.ML"], "links": "http://arxiv.org/abs/2401.01645v1"}
