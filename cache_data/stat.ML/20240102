{"title": "Point Cloud Classification via Deep Set Linearized Optimal Transport", "author": "Scott Mahan, Caroline Moosmüller, Alexander Cloninger", "abstract": "We introduce Deep Set Linearized Optimal Transport, an algorithm designed for\nthe efficient simultaneous embedding of point clouds into an $L^2-$space. This\nembedding preserves specific low-dimensional structures within the Wasserstein\nspace while constructing a classifier to distinguish between various classes of\npoint clouds. Our approach is motivated by the observation that $L^2-$distances\nbetween optimal transport maps for distinct point clouds, originating from a\nshared fixed reference distribution, provide an approximation of the\nWasserstein-2 distance between these point clouds, under certain assumptions.\nTo learn approximations of these transport maps, we employ input convex neural\nnetworks (ICNNs) and establish that, under specific conditions, Euclidean\ndistances between samples from these ICNNs closely mirror Wasserstein-2\ndistances between the true distributions. Additionally, we train a\ndiscriminator network that attaches weights these samples and creates a\npermutation invariant classifier to differentiate between different classes of\npoint clouds. We showcase the advantages of our algorithm over the standard\ndeep set approach through experiments on a flow cytometry dataset with a\nlimited number of labeled point clouds.", "published": "2024-01-02", "categories": ["cs.LG", "stat.ML"], "links": "http://arxiv.org/abs/2401.01460v1"}
{"title": "Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference", "author": "Md Musfiqur Rahman, Murat Kocaoglu", "abstract": "Pearl's causal hierarchy establishes a clear separation between\nobservational, interventional, and counterfactual questions. Researchers\nproposed sound and complete algorithms to compute identifiable causal queries\nat a given level of the hierarchy using the causal structure and data from the\nlower levels of the hierarchy. However, most of these algorithms assume that we\ncan accurately estimate the probability distribution of the data, which is an\nimpractical assumption for high-dimensional variables such as images. On the\nother hand, modern generative deep learning architectures can be trained to\nlearn how to accurately sample from such high-dimensional distributions.\nEspecially with the recent rise of foundation models for images, it is\ndesirable to leverage pre-trained models to answer causal queries with such\nhigh-dimensional data. To address this, we propose a sequential training\nalgorithm that, given the causal structure and a pre-trained conditional\ngenerative model, can train a deep causal generative model, which utilizes the\npre-trained model and can provably sample from identifiable interventional and\ncounterfactual distributions. Our algorithm, called Modular-DCM, uses\nadversarial training to learn the network weights, and to the best of our\nknowledge, is the first algorithm that can make use of pre-trained models and\nprovably sample from any identifiable causal query in the presence of latent\nconfounders with high-dimensional data. We demonstrate the utility of our\nalgorithm using semi-synthetic and real-world datasets containing images as\nvariables in the causal structure.", "published": "2024-01-02", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ME", "stat.ML"], "links": "http://arxiv.org/abs/2401.01426v1"}
{"title": "Scalable network reconstruction in subquadratic time", "author": "Tiago P. Peixoto", "abstract": "Network reconstruction consists in determining the unobserved pairwise\ncouplings between $N$ nodes given only observational data on the resulting\nbehavior that is conditioned on those couplings -- typically a time-series or\nindependent samples from a graphical model. A major obstacle to the scalability\nof algorithms proposed for this problem is a seemingly unavoidable quadratic\ncomplexity of $O(N^2)$, corresponding to the requirement of each possible\npairwise coupling being contemplated at least once, despite the fact that most\nnetworks of interest are sparse, with a number of non-zero couplings that is\nonly $O(N)$. Here we present a general algorithm applicable to a broad range of\nreconstruction problems that achieves its result in subquadratic time, with a\ndata-dependent complexity loosely upper bounded by $O(N^{3/2}\\log N)$, but with\na more typical log-linear complexity of $O(N\\log^2N)$. Our algorithm relies on\na stochastic second neighbor search that produces the best edge candidates with\nhigh probability, thus bypassing an exhaustive quadratic search. In practice,\nour algorithm achieves a performance that is many orders of magnitude faster\nthan the quadratic baseline, allows for easy parallelization, and thus enables\nthe reconstruction of networks with hundreds of thousands and even millions of\nnodes and edges.", "published": "2024-01-02", "categories": ["cs.DS", "cs.LG", "physics.data-an", "stat.CO", "stat.ML"], "links": "http://arxiv.org/abs/2401.01404v1"}
{"title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models", "author": "Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu", "abstract": "Harnessing the power of human-annotated data through Supervised Fine-Tuning\n(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we\ndelve into the prospect of growing a strong LLM out of a weak one without the\nneed for acquiring additional human-annotated data. We propose a new\nfine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a\nsupervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,\nwhere the LLM refines its capability by playing against instances of itself.\nMore specifically, the LLM generates its own training data from its previous\niterations, refining its policy by discerning these self-generated responses\nfrom those obtained from human-annotated data. Our method progressively\nelevates the LLM from a nascent model to a formidable one, unlocking the full\npotential of human-annotated demonstration data for SFT. Theoretically, we\nprove that the global optimum to the training objective function of our method\nis achieved only when the LLM policy aligns with the target data distribution.\nEmpirically, we evaluate our method on several benchmark datasets including the\nHuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our\nresults show that SPIN can significantly improve the LLM's performance across a\nvariety of benchmarks and even outperform models trained through direct\npreference optimization (DPO) supplemented with extra GPT-4 preference data.\nThis sheds light on the promise of self-play, enabling the achievement of\nhuman-level performance in LLMs without the need for expert opponents.", "published": "2024-01-02", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "links": "http://arxiv.org/abs/2401.01335v1"}
{"title": "Efficient Sparse Least Absolute Deviation Regression with Differential Privacy", "author": "Weidong Liu, Xiaojun Mao, Xiaofei Zhang, Xin Zhang", "abstract": "In recent years, privacy-preserving machine learning algorithms have\nattracted increasing attention because of their important applications in many\nscientific fields. However, in the literature, most privacy-preserving\nalgorithms demand learning objectives to be strongly convex and Lipschitz\nsmooth, which thus cannot cover a wide class of robust loss functions (e.g.,\nquantile/least absolute loss). In this work, we aim to develop a fast\nprivacy-preserving learning solution for a sparse robust regression problem.\nOur learning loss consists of a robust least absolute loss and an $\\ell_1$\nsparse penalty term. To fast solve the non-smooth loss under a given privacy\nbudget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE)\nalgorithm for least absolute deviation regression. Our algorithm achieves a\nfast estimation by reformulating the sparse LAD problem as a penalized least\nsquare estimation problem and adopts a three-stage noise injection to guarantee\nthe $(\\epsilon,\\delta)$-differential privacy. We show that our algorithm can\nachieve better privacy and statistical accuracy trade-off compared with the\nstate-of-the-art privacy-preserving regression algorithms. In the end, we\nconduct experiments to verify the efficiency of our proposed FRAPPE algorithm.", "published": "2024-01-02", "categories": ["stat.ML", "cs.LG", "stat.ME", "62J07"], "links": "http://arxiv.org/abs/2401.01294v1"}
{"title": "Encoding Binary Events from Continuous Time Series in Rooted Trees using Contrastive Learning", "author": "Tobias Engelhardt Rasmussen, Siv Sørensen", "abstract": "Broadband infrastructure owners do not always know how their customers are\nconnected in the local networks, which are structured as rooted trees. A recent\nstudy is able to infer the topology of a local network using discrete time\nseries data from the leaves of the tree (customers). In this study we propose a\ncontrastive approach for learning a binary event encoder from continuous time\nseries data. As a preliminary result, we show that our approach has some\npotential in learning a valuable encoder.", "published": "2024-01-02", "categories": ["cs.LG", "cs.AI", "cs.SI", "stat.ML"], "links": "http://arxiv.org/abs/2401.01242v1"}
{"title": "PAC-Bayes-Chernoff bounds for unbounded losses", "author": "Ioar Casado, Luis A. Ortega, Andrés R. Masegosa, Aritz Pérez", "abstract": "We present a new high-probability PAC-Bayes oracle bound for unbounded\nlosses. This result can be understood as a PAC-Bayes version of the Chernoff\nbound. The proof technique relies on uniformly bounding the tail of certain\nrandom variable based on the Cram\\'er transform of the loss. We highlight two\napplications of our main result. First, we show that our bound solves the open\nproblem of optimizing the free parameter on many PAC-Bayes bounds. Finally, we\nshow that our approach allows working with flexible assumptions on the loss\nfunction, resulting in novel bounds that generalize previous ones and can be\nminimized to obtain Gibbs-like posteriors.", "published": "2024-01-02", "categories": ["stat.ML", "cs.LG"], "links": "http://arxiv.org/abs/2401.01148v1"}
{"title": "PAC-Bayesian Domain Adaptation Bounds for Multi-view learning", "author": "Mehdi Hennequin, Khalid Benabdeslem, Haytham Elghazel", "abstract": "This paper presents a series of new results for domain adaptation in the\nmulti-view learning setting. The incorporation of multiple views in the domain\nadaptation was paid little attention in the previous studies. In this way, we\npropose an analysis of generalization bounds with Pac-Bayesian theory to\nconsolidate the two paradigms, which are currently treated separately. Firstly,\nbuilding on previous work by Germain et al., we adapt the distance between\ndistribution proposed by Germain et al. for domain adaptation with the concept\nof multi-view learning. Thus, we introduce a novel distance that is tailored\nfor the multi-view domain adaptation setting. Then, we give Pac-Bayesian bounds\nfor estimating the introduced divergence. Finally, we compare the different new\nbounds with the previous studies.", "published": "2024-01-02", "categories": ["cs.LG", "stat.ML"], "links": "http://arxiv.org/abs/2401.01048v1"}
{"title": "Sharp Analysis of Power Iteration for Tensor PCA", "author": "Yuchen Wu, Kangjie Zhou", "abstract": "We investigate the power iteration algorithm for the tensor PCA model\nintroduced in Richard and Montanari (2014). Previous work studying the\nproperties of tensor power iteration is either limited to a constant number of\niterations, or requires a non-trivial data-independent initialization. In this\npaper, we move beyond these limitations and analyze the dynamics of randomly\ninitialized tensor power iteration up to polynomially many steps. Our\ncontributions are threefold: First, we establish sharp bounds on the number of\niterations required for power method to converge to the planted signal, for a\nbroad range of the signal-to-noise ratios. Second, our analysis reveals that\nthe actual algorithmic threshold for power iteration is smaller than the one\nconjectured in literature by a polylog(n) factor, where n is the ambient\ndimension. Finally, we propose a simple and effective stopping criterion for\npower iteration, which provably outputs a solution that is highly correlated\nwith the true signal. Extensive numerical experiments verify our theoretical\nresults.", "published": "2024-01-02", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML"], "links": "http://arxiv.org/abs/2401.01047v1"}
{"title": "Inverting estimating equations for causal inference on quantiles", "author": "Chao Cheng, Fan Li", "abstract": "The causal inference literature frequently focuses on estimating the mean of\nthe potential outcome, whereas the quantiles of the potential outcome may carry\nimportant additional information. We propose a universal approach, based on the\ninverse estimating equations, to generalize a wide class of causal inference\nsolutions from estimating the mean of the potential outcome to its quantiles.\nWe assume that an identifying moment function is available to identify the mean\nof the threshold-transformed potential outcome, based on which a convenient\nconstruction of the estimating equation of quantiles of potential outcome is\nproposed. In addition, we also give a general construction of the efficient\ninfluence functions of the mean and quantiles of potential outcomes, and\nidentify their connection. We motivate estimators for the quantile estimands\nwith the efficient influence function, and develop their asymptotic properties\nwhen either parametric models or data-adaptive machine learners are used to\nestimate the nuisance functions. A broad implication of our results is that one\ncan rework the existing result for mean causal estimands to facilitate causal\ninference on quantiles, rather than starting from scratch. Our results are\nillustrated by several examples.", "published": "2024-01-02", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "links": "http://arxiv.org/abs/2401.00987v1"}
