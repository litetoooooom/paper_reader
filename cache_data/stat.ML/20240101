{"title": "Families of costs with zero and nonnegative MTW tensor in optimal transport", "author": "Du Nguyen", "abstract": "We compute explicitly the MTW tensor (or cross curvature) for the optimal\ntransport problem on $\\mathbb{R}^n$ with a cost function of form $\\mathsf{c}(x,\ny) = \\mathsf{u}(x^{\\mathfrak{t}}y)$, where $\\mathsf{u}$ is a scalar function\nwith inverse $\\mathsf{s}$, $x^{\\ft}y$ is a nondegenerate bilinear pairing of\nvectors $x, y$ belonging to an open subset of $\\mathbb{R}^n$. The condition\nthat the MTW-tensor vanishes on null vectors under the Kim-McCann metric is a\nfourth-order nonlinear ODE, which could be reduced to a linear ODE of the form\n$\\mathsf{s}^{(2)} - S\\mathsf{s}^{(1)} + P\\mathsf{s} = 0$ with constant\ncoefficients $P$ and $S$. The resulting inverse functions include {\\it Lambert}\nand {\\it generalized inverse hyperbolic\\slash trigonometric} functions. The\nsquare Euclidean metric and $\\log$-type costs are equivalent to instances of\nthese solutions. The optimal map for the family is also explicit. For cost\nfunctions of a similar form on a hyperboloid model of the hyperbolic space and\nunit sphere, we also express this tensor in terms of algebraic expressions in\nderivatives of $\\mathsf{s}$ using the Gauss-Codazzi equation, obtaining new\nfamilies of strictly regular costs for these manifolds, including new families\nof {\\it power function costs}. We analyze the $\\sinh$-type hyperbolic cost,\nproviding examples of $\\mathsf{c}$-convex functions and divergence.", "published": "2024-01-01", "categories": ["math.AP", "cs.IT", "cs.LG", "math.IT", "stat.ML", "58C05, 49Q22, 53C80, 57Z20, 57Z25, 68T05, 26B25"], "links": "http://arxiv.org/abs/2401.00953v1"}
{"title": "Multi-Lattice Sampling of Quantum Field Theories via Neural Operators", "author": "Bálint Máté, François Fleuret", "abstract": "We consider the problem of sampling discrete field configurations $\\phi$ from\nthe Boltzmann distribution $[d\\phi] Z^{-1} e^{-S[\\phi]}$, where $S$ is the\nlattice-discretization of the continuous Euclidean action $\\mathcal S$ of some\nquantum field theory. Since such densities arise as the approximation of the\nunderlying functional density $[\\mathcal D\\phi(x)] \\mathcal Z^{-1} e^{-\\mathcal\nS[\\phi(x)]}$, we frame the task as an instance of operator learning. In\nparticular, we propose to approximate a time-dependent operator $\\mathcal V_t$\nwhose time integral provides a mapping between the functional distributions of\nthe free theory $[\\mathcal D\\phi(x)] \\mathcal Z_0^{-1} e^{-\\mathcal\nS_{0}[\\phi(x)]}$ and of the target theory $[\\mathcal D\\phi(x)]\\mathcal\nZ^{-1}e^{-\\mathcal S[\\phi(x)]}$. Whenever a particular lattice is chosen, the\noperator $\\mathcal V_t$ can be discretized to a finite dimensional,\ntime-dependent vector field $V_t$ which in turn induces a continuous\nnormalizing flow between finite dimensional distributions over the chosen\nlattice. This flow can then be trained to be a diffeormorphism between the\ndiscretized free and target theories $[d\\phi] Z_0^{-1} e^{-S_{0}[\\phi]}$,\n$[d\\phi] Z^{-1}e^{-S[\\phi]}$. We run experiments on the $\\phi^4$-theory to\nexplore to what extent such operator-based flow architectures generalize to\nlattice sizes they were not trained on and show that pretraining on smaller\nlattices can lead to speedup over training only a target lattice size.", "published": "2024-01-01", "categories": ["cs.LG", "hep-lat", "stat.ML"], "links": "http://arxiv.org/abs/2401.00828v1"}
{"title": "Factor Importance Ranking and Selection using Total Indices", "author": "Chaofan Huang, V. Roshan Joseph", "abstract": "Factor importance measures the impact of each feature on output prediction\naccuracy. Many existing works focus on the model-based importance, but an\nimportant feature in one learning algorithm may hold little significance in\nanother model. Hence, a factor importance measure ought to characterize the\nfeature's predictive potential without relying on a specific prediction\nalgorithm. Such algorithm-agnostic importance is termed as intrinsic importance\nin Williamson et al. (2023), but their estimator again requires model fitting.\nTo bypass the modeling step, we present the equivalence between predictiveness\npotential and total Sobol' indices from global sensitivity analysis, and\nintroduce a novel consistent estimator that can be directly estimated from\nnoisy data. Integrating with forward selection and backward elimination gives\nrise to FIRST, Factor Importance Ranking and Selection using Total (Sobol')\nindices. Extensive simulations are provided to demonstrate the effectiveness of\nFIRST on regression and binary classification problems, and a clear advantage\nover the state-of-the-art methods.", "published": "2024-01-01", "categories": ["stat.ME", "stat.ML"], "links": "http://arxiv.org/abs/2401.00800v1"}
{"title": "Inferring Heterogeneous Treatment Effects of Crashes on Highway Traffic: A Doubly Robust Causal Machine Learning Approach", "author": "Shuang Li, Ziyuan Pu, Zhiyong Cui, Seunghyeon Lee, Xiucheng Guo, Dong Ngoduy", "abstract": "Highway traffic crashes exert a considerable impact on both transportation\nsystems and the economy. In this context, accurate and dependable emergency\nresponses are crucial for effective traffic management. However, the influence\nof crashes on traffic status varies across diverse factors and may be biased\ndue to selection bias. Therefore, there arises a necessity to accurately\nestimate the heterogeneous causal effects of crashes, thereby providing\nessential insights to facilitate individual-level emergency decision-making.\nThis paper proposes a novel causal machine learning framework to estimate the\ncausal effect of different types of crashes on highway speed. The Neyman-Rubin\nCausal Model (RCM) is employed to formulate this problem from a causal\nperspective. The Conditional Shapley Value Index (CSVI) is proposed based on\ncausal graph theory to filter adverse variables, and the Structural Causal\nModel (SCM) is then adopted to define the statistical estimand for causal\neffects. The treatment effects are estimated by Doubly Robust Learning (DRL)\nmethods, which combine doubly robust causal inference with classification and\nregression machine learning models. Experimental results from 4815 crashes on\nHighway Interstate 5 in Washington State reveal the heterogeneous treatment\neffects of crashes at varying distances and durations. The rear-end crashes\ncause more severe congestion and longer durations than other types of crashes,\nand the sideswipe crashes have the longest delayed impact. Additionally, the\nfindings show that rear-end crashes affect traffic greater at night, while\ncrash to objects has the most significant influence during peak hours.\nStatistical hypothesis tests, error metrics based on matched \"counterfactual\noutcomes\", and sensitive analyses are employed for assessment, and the results\nvalidate the accuracy and effectiveness of our method.", "published": "2024-01-01", "categories": ["cs.LG", "stat.ML"], "links": "http://arxiv.org/abs/2401.00781v1"}
{"title": "Unsupervised Outlier Detection using Random Subspace and Subsampling Ensembles of Dirichlet Process Mixtures", "author": "Dongwook Kim, Juyeon Park, Hee Cheol Chung, Seonghyun Jeong", "abstract": "Probabilistic mixture models are acknowledged as a valuable tool for\nunsupervised outlier detection owing to their interpretability and intuitive\ngrounding in statistical principles. Within this framework, Dirichlet process\nmixture models emerge as a compelling alternative to conventional finite\nmixture models for both clustering and outlier detection tasks. However,\ndespite their evident advantages, the widespread adoption of Dirichlet process\nmixture models in unsupervised outlier detection has been hampered by\nchallenges related to computational inefficiency and sensitivity to outliers\nduring the construction of detectors. To tackle these challenges, we propose a\nnovel outlier detection method based on ensembles of Dirichlet process Gaussian\nmixtures. The proposed method is a fully unsupervised algorithm that\ncapitalizes on random subspace and subsampling ensembles, not only ensuring\nefficient computation but also enhancing the robustness of the resulting\noutlier detector. Moreover, the proposed method leverages variational inference\nfor Dirichlet process mixtures to ensure efficient and fast computation.\nEmpirical studies with benchmark datasets demonstrate that our method\noutperforms existing approaches for unsupervised outlier detection.", "published": "2024-01-01", "categories": ["cs.LG", "cs.AI", "stat.ML"], "links": "http://arxiv.org/abs/2401.00773v1"}
{"title": "Stochastic Gradient Descent for Additive Nonparametric Regression", "author": "Xin Chen, Jason M. Klusowski", "abstract": "This paper introduces an iterative algorithm designed to train additive\nmodels with favorable memory storage and computational requirements. The\nalgorithm can be viewed as the functional counterpart of stochastic gradient\ndescent, applied to the coefficients of a truncated basis expansion of the\ncomponent functions. We show that the resulting estimator satisfies an oracle\ninequality that allows for model mispecification. In the well-specified\nsetting, by choosing the learning rate carefully across three distinct stages\nof training, we prove that its risk is minimax optimal in terms of the\ndependence on the dimensionality of the data and the size of the training\nsample.", "published": "2024-01-01", "categories": ["stat.ML", "cs.LG"], "links": "http://arxiv.org/abs/2401.00691v1"}
